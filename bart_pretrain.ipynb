{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Serbernari/NER-summarization/blob/main/bart_pretrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrdctq4-jvyW",
        "outputId": "d6e8902d-a77c-4b1a-fc1f-06af6d6de5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERY USEFULL https://github.com/jessevig/bertviz#encoder-decoder-models-bart-t5-etc"
      ],
      "metadata": {
        "id": "m8suuBcKIoae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wfyD30dFU2P",
        "outputId": "a220f7fd-bfeb-426e-a871-fccf1d8b9e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 14.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 62.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 80.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mS8vX0w1Reb",
        "outputId": "98d0638b-5261-425a-9bbe-3a87d566ae8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.0.0-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonlines) (4.2.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines) (21.4.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v0MwTfPyhE4m"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import jsonlines\n",
        "import statistics\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime\n",
        "import torch\n",
        "from torch.optim import AdamW, lr_scheduler\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from linecache import getline\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import BartTokenizer, BartTokenizerFast, BartForConditionalGeneration, BartConfig\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVs871VXSI9n"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUJSMv2LVUVD"
      },
      "source": [
        "creating target data for MLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Sr8caLxS9MX"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/Diplom/Pretraining/target.txt\", 'w+') as output: #\"</s>\"? won't it get cut? wont tokenizer add it?\n",
        "  for filename in [\"/content/arxiv-dataset/test.txt\",\n",
        "                  \"/content/arxiv-dataset/train.txt\",\n",
        "                  \"/content/arxiv-dataset/val.txt\"]:\n",
        "    with open(filename) as f:\n",
        "        for i, line in enumerate(f):\n",
        "          data = json.loads(line)              \n",
        "          article_text = data['article_text']\n",
        "          article_text = ' '.join(article_text)\n",
        "          article_text+=\"</s>\"\n",
        "          article_text+=\"\\n\"\n",
        "          output.write(article_text)\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RCFXL8_SMfI"
      },
      "outputs": [],
      "source": [
        "def generate_input(example): #чтоб число токенов совпадало - сложносоставные слова это несколько масок.\n",
        "#TODO: random masking of other words\n",
        "  NER_mask_proba = 0.5\n",
        "  tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "  model = AutoModelForTokenClassification.from_pretrained(\"/content/drive/MyDrive/Diplom/NER/outputs/checkpoint-1372-epoch-7\")\n",
        "  nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=0) \n",
        "\n",
        "  ner_results = nlp(example) #need to get rid of this list - unlist thing \n",
        "  output = []\n",
        "    \n",
        "  for i in range(len(ner_results)): # HERE LAYS RANDOM\n",
        "    ents = set(x['word'].replace(\"Ġ\", '') for x in ner_results[i] if x['entity'] != 'LABEL_0' and len(x[\"word\"]) > 3 and random.random() >= NER_mask_proba) #select all words that are entities and not in cathegory \"OTHER\"\n",
        "    words = [\"<mask>\" if x in ents else x for x in example[i].split()]\n",
        "    output.append(\" \".join(words))\n",
        "\n",
        "  return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gM7ROmV6rXu"
      },
      "outputs": [],
      "source": [
        "#generate_input([\"this is experiment example\", \"with multiple strings attached method\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b2XxMpOTEoh",
        "outputId": "3b1b6746-5a93-481b-9b25-79ec086d784f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working on /content/arxiv-dataset/test.txt. Lines in file: 6440\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6440it [07:49, 13.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working on /content/arxiv-dataset/train.txt. Lines in file: 203037\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "203037it [3:24:27, 16.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working on /content/arxiv-dataset/val.txt. Lines in file: 6436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6436it [06:24, 16.73it/s]\n"
          ]
        }
      ],
      "source": [
        "batch = []\n",
        "batch_size = 100000\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Diplom/Pretraining/masked.txt\", 'w+') as output:\n",
        "  for filename in [\"/content/arxiv-dataset/test.txt\",\n",
        "                   \"/content/arxiv-dataset/train.txt\",\n",
        "                   \"/content/arxiv-dataset/val.txt\"]:\n",
        "    file_len = sum(1 for line in open(filename))\n",
        "    print(f\"Working on {filename}. Lines in file: {file_len}\")\n",
        "    with open(filename) as f:\n",
        "        for i, line in tqdm(enumerate(f, 1)):\n",
        "          data = json.loads(line)              \n",
        "          article_text = data['article_text']\n",
        "          article_text = ' '.join(article_text)\n",
        "          batch.append(article_text)\n",
        "          if len(batch) >= batch_size or i == file_len:\n",
        "            masked_batch = generate_input(batch)\n",
        "            for text in masked_batch:\n",
        "              text+=\"\\n\"\n",
        "              output.write(text)\n",
        "            batch.clear()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhswwCMGTEuf",
        "outputId": "621d7c59-6c00-4b31-b80e-28ee226c4093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "for about 20 years the problem of properties of short - term changes of solar activity has been considered extensively . many investigators studied the short - term periodicities of the various indices of solar activity . several periodicities were detected , but the periodicities about 155 days and from the interval of @xmath3 $ ] days ( @xmath4 $ ] years ) are mentioned most often . first of them was discovered by @xcite in the occurence rate of gamma - ray flares detected by the gamma - ray spectrometer aboard the _ solar maximum mission ( smm ) . this periodicity was confirmed for other solar flares data and for the same time period @xcite . it was also found in proton flares during solar cycles 19 and 20 @xcite , but it was not found in the solar flares data during solar cycles 22 @xcite . _    several autors confirmed above results for the daily sunspot area data . @xcite studied the sunspot data from 18741984 . she found the 155-day periodicity in data records from 31 years . this periodicity is always characteristic for one of the solar hemispheres ( the southern hemisphere for cycles 1215 and the northern hemisphere for cycles 1621 ) . moreover , it is only present during epochs of maximum activity ( in episodes of 13 years ) . similarinvestigationswerecarriedoutby + @xcite . they applied the same power spectrum method as lean , but the daily sunspot area data ( cycles 1221 ) were divided into 10 shorter time series . the periodicities were searched for the frequency interval 57115 nhz ( 100200 days ) and for each of 10 time series . the authors showed that the periodicity between 150160 days is statistically significant during all cycles from 16 to 21 . the considered peaks were remained unaltered after removing the 11-year cycle and applying the power spectrum analysis . @xcite used the wavelet technique for the daily sunspot areas between 1874 and 1993 . they determined the epochs of appearance of this periodicity and concluded that it presents around the maximum activity period in cycles 16 to 21 . moreover , the power of this periodicity started growing at cycle 19 , decreased in cycles 20 and 21 and disappered after cycle 21 . similaranalyseswerepresentedby + @xcite , but for sunspot number , solar wind plasma , interplanetary magnetic field and geomagnetic activity index @xmath5 . during 1964 - 2000 the sunspot number wavelet power of periods less than one year shows a cyclic evolution with the phase of the solar cycle.the 154-day period is prominent and its strenth is stronger around the 1982 - 1984 interval in almost all solar wind parameters . the existence of the 156-day periodicity in sunspot data were confirmed by @xcite . they considered the possible relation between the 475-day ( 1.3-year ) and 156-day periodicities . the 475-day ( 1.3-year ) periodicity was also detected in variations of the interplanetary magnetic field , geomagnetic activity helioseismic data and in the solar wind speed @xcite . @xcite concluded that the region of larger wavelet power shifts from 475-day ( 1.3-year ) period to 620-day ( 1.7-year ) period and then back to 475-day ( 1.3-year ) . the periodicities from the interval @xmath6 $ ] days ( @xmath4 $ ] years ) have been considered from 1968 . @xcite mentioned a 16.3-month ( 490-day ) periodicity in the sunspot numbers and in the geomagnetic data . @xcite analysed the occurrence rate of major flares during solar cycles 19 . they found a 18-month ( 540-day ) periodicity in flare rate of the norhern hemisphere . @xcite confirmed this result for the @xmath7 flare data for solar cycles 20 and 21 and found a peak in the power spectra near 510540 days . @xcite found a 17-month ( 510-day ) periodicity of sunspot groups and their areas from 1969 to 1986 . these authors concluded that the length of this period is variable and the reason of this periodicity is still not understood . @xcite and + @xcite obtained statistically significant peaks of power at around 158 days for daily sunspot data from 1923 - 1933 ( cycle 16 ) . in this paper the problem of the existence of this periodicity for sunspot data from cycle 16 is considered . the daily sunspot areas , the mean sunspot areas per carrington rotation , the monthly sunspot numbers and their fluctuations , which are obtained after removing the 11-year cycle are analysed . in section 2 the properties of the power spectrum methods are described . in section 3 a new approach to the problem of aliases in the power spectrum analysis is presented . in section 4 numerical results of the new method of the diagnosis of an echo - effect for sunspot area data are discussed . in section 5 the problem of the existence of the periodicity of about 155 days during the maximum activity period for sunspot data from the whole solar disk and from each solar hemisphere separately is considered . to find periodicities in a given time series the power spectrum analysis is applied . in this paper two methods are used : the fast fourier transformation algorithm with the hamming window function ( fft ) and the blackman - tukey ( bt ) power spectrum method @xcite . the bt method is used for the diagnosis of the reasons of the existence of peaks , which are obtained by the fft method . the bt method consists in the smoothing of a cosine transform of an autocorrelation function using a 3-point weighting average . such an estimator is consistent and unbiased . moreover , the peaks are uncorrelated and their sum is a variance of a considered time series . the main disadvantage of this method is a weak resolution of the periodogram points , particularly for low frequences . for example , if the autocorrelation function is evaluated for @xmath8 , then the distribution points in the time domain are : @xmath9 thus , it is obvious that this method should not be used for detecting low frequency periodicities with a fairly good resolution . however , because of an application of the autocorrelation function , the bt method can be used to verify a reality of peaks which are computed using a method giving the better resolution ( for example the fft method ) . it is valuable to remember that the power spectrum methods should be applied very carefully . the difficulties in the interpretation of significant peaks could be caused by at least four effects : a sampling of a continuos function , an echo - effect , a contribution of long - term periodicities and a random noise . first effect exists because periodicities , which are shorter than the sampling interval , may mix with longer periodicities . in result , this effect can be reduced by an decrease of the sampling interval between observations . the echo - effect occurs when there is a latent harmonic of frequency @xmath10 in the time series , giving a spectral peak at @xmath10 , and also periodic terms of frequency @xmath11 etc . this may be detected by the autocorrelation function for time series with a large variance . time series often contain long - term periodicities , that influence short - term peaks . they could rise periodogram s peaks at lower frequencies . however , it is also easy to notice the influence of the long - term periodicities on short - term peaks in the graphs of the autocorrelation functions . this effect is observed for the time series of solar activity indexes which are limited by the 11-year cycle .    to find statistically significant periodicities it is reasonable to use the autocorrelation function and the power spectrum method with a high resolution . in the case of a stationary time series they give similar results . moreover , for a stationary time series with the mean zero the fourier transform is equivalent to the cosine transform of an autocorrelation function @xcite . thus , after a comparison of a periodogram with an appropriate autocorrelation function one can detect peaks which are in the graph of the first function and do not exist in the graph of the second function . the reasons of their existence could be explained by the long - term periodicities and the echo - effect . below method enables one to detect these effects . ( solid line ) and the 95% confidence level basing on thered noise ( dotted line ) . the periodogram values are presented on the left axis . the lower curve illustrates the autocorrelation function of the same time series ( solid line ) . the dotted lines represent two standard errors of the autocorrelation function . the dashed horizontal line shows the zero level . the autocorrelation values are shown in the right axis . ]     because the statistical tests indicate that the time series is a white noise the confidence level is not marked . ]    . ] the method of the diagnosis of an echo - effect in the power spectrum ( de ) consists in an analysis of a periodogram of a given time series computed using the bt method . the bt method bases on the cosine transform of the autocorrelation function which creates peaks which are in the periodogram , but not in the autocorrelation function . the de method is used for peaks which are computed by the fft method ( with high resolution ) and are statistically significant . the time series of sunspot activity indexes with the spacing interval one rotation or one month contain a markov - type persistence , which means a tendency for the successive values of the time series to remember their antecendent values . thus , i use a confidence level basing on the red noise of markov @xcite for the choice of the significant peaks of the periodogram computed by the fft method . when a time series does not contain the markov - type persistence i apply the fisher test and the kolmogorov - smirnov test at the significance level @xmath12 @xcite to verify a statistically significance of periodograms peaks . the fisher test checks the null hypothesis that the time series is white noise agains the alternative hypothesis that the time series contains an added deterministic periodic component of unspecified frequency . because the fisher test tends to be severe in rejecting peaks as insignificant the kolmogorov - smirnov test is also used . the de method analyses raw estimators of the power spectrum . they are given as follows    @xmath13    for @xmath14 + where @xmath15 for @xmath16 + @xmath17 is the length of the time series @xmath18 and @xmath19 is the mean value . the first term of the estimator @xmath20 is constant . the second term takes two values ( depending on odd or even @xmath21 ) which are not significant because @xmath22 for large m. thus , the third term of ( 1 ) should be analysed . looking for intervals of @xmath23 for which @xmath24 has the same sign and different signs one can find such parts of the function @xmath25 which create the value @xmath20 . let the set of values of the independent variable of the autocorrelation function be called @xmath26 and it can be divided into the sums of disjoint sets : @xmath27 where + @xmath28 + @xmath29 @xmath30 @xmath31 + @xmath32 + @xmath33 @xmath34 @xmath35 @xmath36 @xmath37 @xmath38 @xmath39 @xmath40    well , the set @xmath41 contains all integer values of @xmath23 from the interval of @xmath42 for which the autocorrelation function and the cosinus function with the period @xmath43 $ ] are positive . the index @xmath44 indicates successive parts of the cosinus function for which the cosinuses of successive values of @xmath23 have the same sign . however , sometimes the set @xmath41 can be empty . for example , for @xmath45 and @xmath46 the set @xmath47 should contain all @xmath48 $ ] for which @xmath49 and @xmath50 , but for such values of @xmath23 the values of @xmath51 are negative . thus , the set @xmath47 is empty .    . the periodogram values are presented on the left axis . the lower curve illustrates the autocorrelation function of the same time series . the autocorrelation values are shown in the right axis . ] let us take into consideration all sets \\{@xmath52 } , \\{@xmath53 } and \\{@xmath41 } which are not empty . because numberings and power of these sets depend on the form of the autocorrelation function of the given time series , it is impossible to establish them arbitrary . thus , the sets of appropriate indexes of the sets \\{@xmath52 } , \\{@xmath53 } and \\{@xmath41 } are called @xmath54 , @xmath55 and @xmath56 respectively . for example the set @xmath56 contains all @xmath44 from the set @xmath57 for which the sets @xmath41 are not empty . to separate quantitatively in the estimator @xmath20 the positive contributions which are originated by the cases described by the formula ( 5 ) from the cases which are described by the formula ( 3 ) the following indexes are introduced : @xmath58 @xmath59 @xmath60 @xmath61 where @xmath62 @xmath63 @xmath64 taking for the empty sets \\{@xmath53 } and \\{@xmath41 } the indices @xmath65 and @xmath66 equal zero . the index @xmath65 describes a percentage of the contribution of the case when @xmath25 and @xmath51 are positive to the positive part of the third term of the sum ( 1 ) . the index @xmath66 describes a similar contribution , but for the case when the both @xmath25 and @xmath51 are simultaneously negative . thanks to these one can decide which the positive or the negative values of the autocorrelation function have a larger contribution to the positive values of the estimator @xmath20 . when the difference @xmath67 is positive , the statement the @xmath21-th peak really exists can not be rejected . thus , the following formula should be satisfied : @xmath68    because the @xmath21-th peak could exist as a result of the echo - effect , it is necessary to verify the second condition :    @xmath69\\in c_m.\\ ] ]    . the periodogram values are presented on the left axis . the lower curve illustrates the autocorrelation function of the same time series ( solid line ) . the dotted lines represent two standard errors of the autocorrelation function . the dashed horizontal line shows the zero level . the autocorrelation values are shown in the right axis . ]    to verify the implication ( 8) firstly it is necessary to evaluate the sets @xmath41 for @xmath70 of the values of @xmath23 for which the autocorrelation function and the cosine function with the period @xmath71 $ ] are positive and the sets @xmath72 of values of @xmath23 for which the autocorrelation function and the cosine function with the period @xmath43 $ ] are negative . secondly , a percentage of the contribution of the sum of products of positive values of @xmath25 and @xmath51 to the sum of positive products of the values of @xmath25 and @xmath51 should be evaluated . as a result the indexes @xmath65 for each set @xmath41 where @xmath44 is the index from the set @xmath56 are obtained . thirdly , from all sets @xmath41 such that @xmath70 the set @xmath73 for which the index @xmath65 is the greatest should be chosen .    the implication ( 8) is true when the set @xmath73 includes the considered period @xmath43 $ ] . this means that the greatest contribution of positive values of the autocorrelation function and positive cosines with the period @xmath43 $ ] to the periodogram value @xmath20 is caused by the sum of positive products of @xmath74 for each @xmath75-\\frac{m}{2k},[\\frac{2m}{k}]+\\frac{m}{2k})$ ] .    when the implication ( 8) is false , the peak @xmath20 is mainly created by the sum of positive products of @xmath74 for each @xmath76-\\frac{m}{2k},\\big [ \\frac{2m}{n}\\big ] + \\frac{m}{2k } \\big ) $ ] , where @xmath77 is a multiple or a divisor of @xmath21 . it is necessary to add , that the de method should be applied to the periodograms peaks , which probably exist because of the echo - effect . it enables one to find such parts of the autocorrelation function , which have the significant contribution to the considered peak . the fact , that the conditions ( 7 ) and ( 8) are satisfied , can unambiguously decide about the existence of the considered periodicity in the given time series , but if at least one of them is not satisfied , one can doubt about the existence of the considered periodicity . thus , in such cases the sentence the peak can not be treated as true should be used .    using the de method it is necessary to remember about the power of the set @xmath78 . if @xmath79 is too large , errors of an autocorrelation function estimation appear . they are caused by the finite length of the given time series and as a result additional peaks of the periodogram occur . if @xmath79 is too small , there are less peaks because of a low resolution of the periodogram . in applications @xmath80 is used . in order to evaluate the value @xmath79 the fft method is used . the periodograms computed by the bt and the fft method are compared . the conformity of them enables one to obtain the value @xmath79 .    . the fft periodogram values are presented on the left axis . the lower curve illustrates the bt periodogram of the same time series ( solid line and large black circles ) . the bt periodogram values are shown in the right axis . ] in this paper the sunspot activity data ( august 1923 - october 1933 ) provided by the greenwich photoheliographic results ( gpr ) are analysed . firstly , i consider the monthly sunspot number data . to eliminate the 11-year trend from these data , the consecutively smoothed monthly sunspot number @xmath81 is subtracted from the monthly sunspot number @xmath82 where the consecutive mean @xmath83 is given by @xmath84 the values @xmath83 for @xmath85 and @xmath86 are calculated using additional data from last six months of cycle 15 and first six months of cycle 17 .    because of the north - south asymmetry of various solar indices @xcite , the sunspot activity is considered for each solar hemisphere separately . analogously to the monthly sunspot numbers , the time series of sunspot areas in the northern and southern hemispheres with the spacing interval @xmath87 rotation are denoted . in order to find periodicities , the following time series are used : + @xmath88   + @xmath89    + @xmath90     + in the lower part of figure [ f1 ] the autocorrelation function of the time series for the northern hemisphere @xmath88 is shown . it is easy to notice that the prominent peak falls at 17 rotations interval ( 459 days ) and @xmath25 for @xmath91 $ ] rotations ( [ 81 , 162 ] days ) are significantly negative . the periodogram of the time series @xmath88 ( see the upper curve in figures [ f1 ] ) does not show the significant peaks at @xmath92 rotations ( 135 , 162 days ) , but there is the significant peak at @xmath93 ( 243 days ) . the peaks at @xmath94 are close to the peaks of the autocorrelation function . thus , the result obtained for the periodicity at about @xmath0 days are contradict to the results obtained for the time series of daily sunspot areas @xcite .    for the southern hemisphere ( the lower curve in figure [ f2 ] ) @xmath25 for @xmath95 $ ] rotations ( [ 54 , 189 ] days ) is not positive except @xmath96 ( 135 days ) for which @xmath97 is not statistically significant . the upper curve in figures [ f2 ] presents the periodogram of the time series @xmath89 . this time series does not contain a markov - type persistence . moreover , the kolmogorov - smirnov test and the fisher test do not reject a null hypothesis that the time series is a white noise only . this means that the time series do not contain an added deterministic periodic component of unspecified frequency . the autocorrelation function of the time series @xmath90 ( the lower curve in figure [ f3 ] ) has only one statistically significant peak for @xmath98 months ( 480 days ) and negative values for @xmath99 $ ] months ( [ 90 , 390 ] days ) . however , the periodogram of this time series ( the upper curve in figure [ f3 ] ) has two significant peaks the first at 15.2 and the second at 5.3 months ( 456 , 159 days ) . thus , the periodogram contains the significant peak , although the autocorrelation function has the negative value at @xmath100 months .    to explain these problems two following time series of daily sunspot areas are considered : + @xmath101   + @xmath102     + where @xmath103    the values @xmath104 for @xmath105 and @xmath106 are calculated using additional daily data from the solar cycles 15 and 17 .     and the cosine function for @xmath45 ( the period at about 154 days ) . the horizontal line ( dotted line ) shows the zero level . the vertical dotted lines evaluate the intervals where the sets @xmath107 ( for @xmath108 ) are searched . the percentage values show the index @xmath65 for each @xmath41 for the time series @xmath102 ( in parentheses for the time series @xmath101 ) . in the right bottom corner the values of @xmath65 for the time series @xmath102 , for @xmath109 are written . ] ( the 500-day period ) ]    the comparison of the functions @xmath25 of the time series @xmath101 ( the lower curve in figure [ f4 ] ) and @xmath102 ( the lower curve in figure [ f5 ] ) suggests that the positive values of the function @xmath110 of the time series @xmath101 in the interval of @xmath111 $ ] days could be caused by the 11-year cycle . this effect is not visible in the case of periodograms of the both time series computed using the fft method ( see the upper curves in figures [ f4 ] and [ f5 ] ) or the bt method ( see the lower curve in figure [ f6 ] ) . moreover , the periodogram of the time series @xmath102 has the significant values at @xmath112 days , but the autocorrelation function is negative at these points . @xcite showed that the lomb - scargle periodograms for the both time series ( see @xcite , figures 7 a - c ) have a peak at 158.8 days which stands over the fap level by a significant amount . using the de method the above discrepancies are obvious . to establish the @xmath79 value the periodograms computed by the fft and the bt methods are shown in figure [ f6 ] ( the upper and the lower curve respectively ) . for @xmath46 and for periods less than 166 days there is a good comformity of the both periodograms ( but for periods greater than 166 days the points of the bt periodogram are not linked because the bt periodogram has much worse resolution than the fft periodogram ( no one know how to do it ) ) . for @xmath46 and @xmath113 the value of @xmath21 is 13 ( @xmath71=153 $ ] ) . the inequality ( 7 ) is satisfied because @xmath114 . this means that the value of @xmath115 is mainly created by positive values of the autocorrelation function . the implication ( 8) needs an evaluation of the greatest value of the index @xmath65 where @xmath70 , but the solar data contain the most prominent period for @xmath116 days because of the solar rotation . thus , although @xmath117 for each @xmath118 , all sets @xmath41 ( see ( 5 ) and ( 6 ) ) without the set @xmath119 ( see ( 4 ) ) , which contains @xmath120 $ ] , are considered . this situation is presented in figure [ f7 ] . in this figure two curves @xmath121 and @xmath122 are plotted . the vertical dotted lines evaluate the intervals where the sets @xmath107 ( for @xmath123 ) are searched . for such @xmath41 two numbers are written : in parentheses the value of @xmath65 for the time series @xmath101 and above it the value of @xmath65 for the time series @xmath102 . to make this figure clear the curves are plotted for the set @xmath124 only . ( in the right bottom corner information about the values of @xmath65 for the time series @xmath102 , for @xmath109 are written . ) the implication ( 8) is not true , because @xmath125 for @xmath126 . therefore , @xmath43=153\\notin c_6=[423,500]$ ] . moreover , the autocorrelation function for @xmath127 $ ] is negative and the set @xmath128 is empty . thus , @xmath129 . on the basis of these information one can state , that the periodogram peak at @xmath130 days of the time series @xmath102 exists because of positive @xmath25 , but for @xmath23 from the intervals which do not contain this period . looking at the values of @xmath65 of the time series @xmath101 , one can notice that they decrease when @xmath23 increases until @xmath131 . this indicates , that when @xmath23 increases , the contribution of the 11-year cycle to the peaks of the periodogram decreases . an increase of the value of @xmath65 is for @xmath132 for the both time series , although the contribution of the 11-year cycle for the time series @xmath101 is insignificant . thus , this part of the autocorrelation function ( @xmath133 for the time series @xmath102 ) influences the @xmath21-th peak of the periodogram . this suggests that the periodicity at about 155 days is a harmonic of the periodicity from the interval of @xmath1 $ ] days . ( solid line ) and consecutively smoothed sunspot areas of the one rotation time interval @xmath134 ( dotted line ) . both indexes are presented on the left axis . the lower curve illustrates fluctuations of the sunspot areas @xmath135 . the dotted and dashed horizontal lines represent levels zero and @xmath136 respectively . the fluctuations are shown on the right axis . ] the described reasoning can be carried out for other values of the periodogram . for example , the condition ( 8) is not satisfied for @xmath137 ( 250 , 222 , 200 days ) . moreover , the autocorrelation function at these points is negative . these suggest that there are not a true periodicity in the interval of [ 200 , 250 ] days . it is difficult to decide about the existence of the periodicities for @xmath138 ( 333 days ) and @xmath139 ( 286 days ) on the basis of above analysis . the implication ( 8) is not satisfied for @xmath139 and the condition ( 7 ) is not satisfied for @xmath138 , although the function @xmath25 of the time series @xmath102 is significantly positive for @xmath140 . the conditions ( 7 ) and ( 8) are satisfied for @xmath141 ( figure [ f8 ] ) and @xmath142 . therefore , it is possible to exist the periodicity from the interval of @xmath1 $ ] days . similar results were also obtained by @xcite for daily sunspot numbers and daily sunspot areas . she considered the means of three periodograms of these indexes for data from @xmath143 years and found statistically significant peaks from the interval of @xmath1 $ ] ( see @xcite , figure 2 ) . @xcite studied sunspot areas from 1876 - 1999 and sunspot numbers from 1749 - 2001 with the help of the wavelet transform . they pointed out that the 154 - 158-day period could be the third harmonic of the 1.3-year ( 475-day ) period . moreover , the both periods fluctuate considerably with time , being stronger during stronger sunspot cycles . therefore , the wavelet analysis suggests a common origin of the both periodicities . this conclusion confirms the de method result which indicates that the periodogram peak at @xmath144 days is an alias of the periodicity from the interval of @xmath1 $ ] in order to verify the existence of the periodicity at about 155 days i consider the following time series : + @xmath145     + @xmath146    + @xmath147   + the value @xmath134 is calculated analogously to @xmath83 ( see sect . the values @xmath148 and @xmath149 are evaluated from the formula ( 9 ) . in the upper part of figure [ f9 ] the time series of sunspot areas @xmath150 of the one rotation time interval from the whole solar disk and the time series of consecutively smoothed sunspot areas @xmath151 are showed . in the lower part of figure [ f9 ] the time series of sunspot area fluctuations @xmath145 is presented . on the basis of these data the maximum activity period of cycle 16 is evaluated . it is an interval between two strongest fluctuations e.a . @xmath152 $ ] rotations . the length of the time interval @xmath153 is 54 rotations . if the about @xmath0-day ( 6 solar rotations ) periodicity existed in this time interval and it was characteristic for strong fluctuations from this time interval , 10 local maxima in the set of @xmath154 would be seen . then it should be necessary to find such a value of p for which @xmath155 for @xmath156 and the number of the local maxima of these values is 10 . as it can be seen in the lower part of figure [ f9 ] this is for the case of @xmath157 ( in this figure the dashed horizontal line is the level of @xmath158 ) . figure [ f10 ] presents nine time distances among the successive fluctuation local maxima and the horizontal line represents the 6-rotation periodicity . it is immediately apparent that the dispersion of these points is 10 and it is difficult to find even few points which oscillate around the value of 6 . such an analysis was carried out for smaller and larger @xmath136 and the results were similar . therefore , the fact , that the about @xmath0-day periodicity exists in the time series of sunspot area fluctuations during the maximum activity period is questionable .    . the horizontal line represents the 6-rotation ( 162-day ) period . ]    ]    ]    to verify again the existence of the about @xmath0-day periodicity during the maximum activity period in each solar hemisphere separately , the time series @xmath88 and @xmath89 were also cut down to the maximum activity period ( january 1925december 1930 ) . the comparison of the autocorrelation functions of these time series with the appriopriate autocorrelation functions of the time series @xmath88 and @xmath89 , which are computed for the whole 11-year cycle ( the lower curves of figures [ f1 ] and [ f2 ] ) , indicates that there are not significant differences between them especially for @xmath23=5 and 6 rotations ( 135 and 162 days ) ) . this conclusion is confirmed by the analysis of the time series @xmath146 for the maximum activity period . the autocorrelation function ( the lower curve of figure [ f11 ] ) is negative for the interval of [ 57 , 173 ] days , but the resolution of the periodogram is too low to find the significant peak at @xmath159 days . the autocorrelation function gives the same result as for daily sunspot area fluctuations from the whole solar disk ( @xmath160 ) ( see also the lower curve of figures [ f5 ] ) . in the case of the time series @xmath89 @xmath161 is zero for the fluctuations from the whole solar cycle and it is almost zero ( @xmath162 ) for the fluctuations from the maximum activity period . the value @xmath163 is negative . similarly to the case of the northern hemisphere the autocorrelation function and the periodogram of southern hemisphere daily sunspot area fluctuations from the maximum activity period @xmath147 are computed ( see figure [ f12 ] ) . the autocorrelation function has the statistically significant positive peak in the interval of [ 155 , 165 ] days , but the periodogram has too low resolution to decide about the possible periodicities . the correlative analysis indicates that there are positive fluctuations with time distances about @xmath0 days in the maximum activity period . the results of the analyses of the time series of sunspot area fluctuations from the maximum activity period are contradict with the conclusions of @xcite . she uses the power spectrum analysis only . the periodogram of daily sunspot fluctuations contains peaks , which could be harmonics or subharmonics of the true periodicities . they could be treated as real periodicities . this effect is not visible for sunspot data of the one rotation time interval , but averaging could lose true periodicities . this is observed for data from the southern hemisphere . there is the about @xmath0-day peak in the autocorrelation function of daily fluctuations , but the correlation for data of the one rotation interval is almost zero or negative at the points @xmath164 and 6 rotations . thus , it is reasonable to research both time series together using the correlative and the power spectrum analyses . the following results are obtained :    1 . a new method of the detection of statistically significant peaks of the periodograms enables one to identify aliases in the periodogram . 2 .   two effects cause the existence of the peak of the periodogram of the time series of sunspot area fluctuations at about @xmath0 days : the first is caused by the 27-day periodicity , which probably creates the 162-day periodicity ( it is a subharmonic frequency of the 27-day periodicity ) and the second is caused by statistically significant positive values of the autocorrelation function from the intervals of @xmath165 $ ] and @xmath166 $ ] days . the existence of the periodicity of about @xmath0 days of the time series of sunspot area fluctuations and sunspot area fluctuations from the northern hemisphere during the maximum activity period is questionable . the autocorrelation analysis of the time series of sunspot area fluctuations from the southern hemisphere indicates that the periodicity of about 155 days exists during the maximum activity period . i appreciate valuable comments from professor j. jakimiec .</s>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/drive/MyDrive/Diplom/Pretraining/target.txt\") as infile:\n",
        "    for line in infile:\n",
        "        print(line)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8eVtP_KwGqP"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTejjJkLwF9U"
      },
      "outputs": [],
      "source": [
        "tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx5S2xW5wGDS"
      },
      "outputs": [],
      "source": [
        "masked_file = \"/content/drive/MyDrive/Diplom/Pretraining/masked.txt\"\n",
        "target_file = \"/content/drive/MyDrive/Diplom/Pretraining/target.txt\"\n",
        "tokenized_file = \"/content/drive/MyDrive/Diplom/Pretraining/tokenized_items.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwWJFSta2yVq"
      },
      "outputs": [],
      "source": [
        "with open(target_file) as f:\n",
        "      for len, _ in enumerate(f):\n",
        "          pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1zRmpyLwGGn",
        "outputId": "dbe1c4f5-36ea-49fd-d930-a44525b642c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 215912/215912 [5:20:46<00:00, 11.22it/s]\n"
          ]
        }
      ],
      "source": [
        "with jsonlines.open(tokenized_file, mode='w') as writer:\n",
        "  for idx in tqdm(range(len)):\n",
        "      item = {}\n",
        "      masked_data = getline(masked_file, idx)\n",
        "      target_data = getline(target_file, idx)\n",
        "\n",
        "      tokenized = tokenizer(masked_data, truncation=True, padding=True)\n",
        "      item['input_ids'], item['attention_mask'] = tokenized['input_ids'], tokenized['attention_mask'] \n",
        "      item['labels'] = tokenizer(target_data, truncation=True, padding=True)['input_ids']\n",
        "\n",
        "      writer.write(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0loGPq7kxXdT"
      },
      "source": [
        "# Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjWghHhthTIn"
      },
      "outputs": [],
      "source": [
        "class Arxiv_dataset(Dataset):\n",
        "    #articles might be too long and geting truncated?\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.data = []\n",
        "        with jsonlines.open(filename) as reader:\n",
        "            for obj in tqdm(reader):\n",
        "              self.data.append(obj)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx): #проверь, перепутать мог\n",
        "      \n",
        "      item = self.data[idx]     \n",
        "\n",
        "      item['input_ids'] = torch.tensor(item['input_ids'])\n",
        "      item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
        "      item['labels'] = torch.tensor(item['labels'])\n",
        "\n",
        "      return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TftRHbRshTLx",
        "outputId": "c32699ef-f47d-4865-ac98-97be5cb72bfe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "215912it [01:25, 2535.54it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = Arxiv_dataset(\"/content/drive/MyDrive/Diplom/Pretraining/tokenized_items.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcgQgv4vQ32t",
        "outputId": "b890aa10-4d30-45b3-e997-568241e66b2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "215912"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khl9wU0YYHim"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96xsPUVBvRRm"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyTTeww6va0r"
      },
      "outputs": [],
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8HV5IC8vnQ-",
        "outputId": "35699a8e-cb8c-4744-bfa3-d481a49c59fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgm7WTRQP1x3"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILfzoyRrUens"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdy9c0M3RlwW"
      },
      "outputs": [],
      "source": [
        "#total 215912 steps per epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbRW1eUcYHlr",
        "outputId": "66293a2a-2ee7-4d9f-8292-21cdb2e24f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-05-11 14:05:56.236256 || Epoch #0, step 0 loss:1.2685335874557495\n",
            "2022-05-11 14:07:01.247989 || Epoch #0, step 100 loss:0.9708780646324158\n",
            "2022-05-11 14:08:05.924340 || Epoch #0, step 200 loss:2.1814215183258057\n",
            "2022-05-11 14:09:10.599435 || Epoch #0, step 300 loss:0.9246693849563599\n",
            "2022-05-11 14:10:15.642054 || Epoch #0, step 400 loss:0.6126791834831238\n",
            "2022-05-11 14:11:20.749378 || Epoch #0, step 500 loss:0.733305037021637\n",
            "2022-05-11 14:12:26.280023 || Epoch #0, step 600 loss:0.4132881462574005\n",
            "2022-05-11 14:13:31.675842 || Epoch #0, step 700 loss:0.7890387773513794\n",
            "2022-05-11 14:14:37.210273 || Epoch #0, step 800 loss:0.6002416014671326\n",
            "2022-05-11 14:15:42.326324 || Epoch #0, step 900 loss:0.7281654477119446\n",
            "2022-05-11 14:16:47.637583 || Epoch #0, step 1000 loss:0.5212292075157166\n",
            "2022-05-11 14:17:52.529452 || Epoch #0, step 1100 loss:0.6972353458404541\n",
            "2022-05-11 14:18:58.060078 || Epoch #0, step 1200 loss:0.9712578654289246\n",
            "2022-05-11 14:20:03.112727 || Epoch #0, step 1300 loss:1.0805423259735107\n",
            "2022-05-11 14:21:07.004726 || Epoch #0, step 1400 loss:0.720300555229187\n",
            "2022-05-11 14:22:11.604139 || Epoch #0, step 1500 loss:0.8371878862380981\n",
            "2022-05-11 14:23:17.105656 || Epoch #0, step 1600 loss:0.6899908781051636\n",
            "2022-05-11 14:24:22.249423 || Epoch #0, step 1700 loss:0.7361578345298767\n",
            "2022-05-11 14:25:26.338776 || Epoch #0, step 1800 loss:0.8323920369148254\n",
            "2022-05-11 14:26:31.331369 || Epoch #0, step 1900 loss:0.5750956535339355\n",
            "2022-05-11 14:27:35.777692 || Epoch #0, step 2000 loss:0.722806453704834\n",
            "2022-05-11 14:28:40.711975 || Epoch #0, step 2100 loss:0.6114744544029236\n",
            "2022-05-11 14:29:46.212360 || Epoch #0, step 2200 loss:0.56365966796875\n",
            "2022-05-11 14:30:51.114905 || Epoch #0, step 2300 loss:0.5627461075782776\n",
            "2022-05-11 14:31:55.614609 || Epoch #0, step 2400 loss:0.3741828501224518\n",
            "2022-05-11 14:33:00.764903 || Epoch #0, step 2500 loss:0.6338233947753906\n",
            "2022-05-11 14:34:05.318674 || Epoch #0, step 2600 loss:0.45133447647094727\n",
            "2022-05-11 14:35:08.847623 || Epoch #0, step 2700 loss:0.43080347776412964\n",
            "2022-05-11 14:36:13.881907 || Epoch #0, step 2800 loss:0.4989027976989746\n",
            "2022-05-11 14:37:18.931305 || Epoch #0, step 2900 loss:0.7312092185020447\n",
            "2022-05-11 14:38:24.143954 || Epoch #0, step 3000 loss:0.4498167634010315\n",
            "2022-05-11 14:39:28.464543 || Epoch #0, step 3100 loss:0.3620363175868988\n",
            "2022-05-11 14:40:32.953299 || Epoch #0, step 3200 loss:0.379077285528183\n",
            "2022-05-11 14:41:37.788314 || Epoch #0, step 3300 loss:0.26363271474838257\n",
            "2022-05-11 14:42:42.805862 || Epoch #0, step 3400 loss:0.29511213302612305\n",
            "2022-05-11 14:43:51.317735 || Epoch #0, step 3500 loss:0.4416015148162842\n",
            "2022-05-11 14:44:56.383326 || Epoch #0, step 3600 loss:0.32636892795562744\n",
            "2022-05-11 14:46:01.615007 || Epoch #0, step 3700 loss:0.26966243982315063\n",
            "2022-05-11 14:47:06.803050 || Epoch #0, step 3800 loss:0.41206464171409607\n",
            "2022-05-11 14:48:11.884389 || Epoch #0, step 3900 loss:0.4211375415325165\n",
            "2022-05-11 14:49:16.321427 || Epoch #0, step 4000 loss:0.45603612065315247\n",
            "2022-05-11 14:50:21.809245 || Epoch #0, step 4100 loss:0.27332210540771484\n",
            "2022-05-11 14:51:26.017738 || Epoch #0, step 4200 loss:0.1606256067752838\n",
            "2022-05-11 14:52:30.993773 || Epoch #0, step 4300 loss:0.19919171929359436\n",
            "2022-05-11 14:53:36.169596 || Epoch #0, step 4400 loss:0.2046227753162384\n",
            "2022-05-11 14:54:40.734859 || Epoch #0, step 4500 loss:0.09416123479604721\n",
            "2022-05-11 14:55:45.556133 || Epoch #0, step 4600 loss:0.3461664617061615\n",
            "2022-05-11 14:56:49.915940 || Epoch #0, step 4700 loss:0.3757340908050537\n",
            "2022-05-11 14:57:53.976700 || Epoch #0, step 4800 loss:0.3177056908607483\n",
            "2022-05-11 14:58:58.974234 || Epoch #0, step 4900 loss:0.39742985367774963\n",
            "2022-05-11 15:00:03.816630 || Epoch #0, step 5000 loss:0.39281368255615234\n",
            "2022-05-11 15:01:09.162472 || Epoch #0, step 5100 loss:0.4162396490573883\n",
            "2022-05-11 15:02:14.464523 || Epoch #0, step 5200 loss:0.5057092905044556\n",
            "2022-05-11 15:03:19.814011 || Epoch #0, step 5300 loss:0.3385860025882721\n",
            "2022-05-11 15:04:24.538929 || Epoch #0, step 5400 loss:0.17812508344650269\n",
            "2022-05-11 15:05:30.014699 || Epoch #0, step 5500 loss:0.3931390047073364\n",
            "2022-05-11 15:06:34.251824 || Epoch #0, step 5600 loss:0.309741348028183\n",
            "2022-05-11 15:07:38.446100 || Epoch #0, step 5700 loss:0.26787981390953064\n",
            "2022-05-11 15:08:43.472601 || Epoch #0, step 5800 loss:0.3821720778942108\n",
            "2022-05-11 15:09:48.176089 || Epoch #0, step 5900 loss:0.38645756244659424\n",
            "2022-05-11 15:10:52.965732 || Epoch #0, step 6000 loss:0.5040059089660645\n",
            "2022-05-11 15:11:58.472110 || Epoch #0, step 6100 loss:0.6880409717559814\n",
            "2022-05-11 15:13:03.691775 || Epoch #0, step 6200 loss:0.4532100558280945\n",
            "2022-05-11 15:14:08.983180 || Epoch #0, step 6300 loss:0.5137406587600708\n",
            "2022-05-11 15:15:14.463401 || Epoch #0, step 6400 loss:0.3888944387435913\n",
            "2022-05-11 15:16:19.467826 || Epoch #0, step 6500 loss:0.6225959062576294\n",
            "2022-05-11 15:17:24.956857 || Epoch #0, step 6600 loss:0.43210628628730774\n",
            "2022-05-11 15:18:29.383079 || Epoch #0, step 6700 loss:0.2314968854188919\n",
            "2022-05-11 15:19:34.490828 || Epoch #0, step 6800 loss:0.337849885225296\n",
            "2022-05-11 15:20:39.294055 || Epoch #0, step 6900 loss:0.39307740330696106\n",
            "2022-05-11 15:21:48.241403 || Epoch #0, step 7000 loss:0.2329142987728119\n",
            "2022-05-11 15:22:52.828026 || Epoch #0, step 7100 loss:0.4104355573654175\n",
            "2022-05-11 15:23:57.630797 || Epoch #0, step 7200 loss:0.2969038486480713\n",
            "2022-05-11 15:25:02.712618 || Epoch #0, step 7300 loss:0.19529977440834045\n",
            "2022-05-11 15:26:08.054181 || Epoch #0, step 7400 loss:0.40221771597862244\n",
            "2022-05-11 15:27:11.377145 || Epoch #0, step 7500 loss:0.1840980350971222\n",
            "2022-05-11 15:28:15.103432 || Epoch #0, step 7600 loss:0.259860098361969\n",
            "2022-05-11 15:29:20.034058 || Epoch #0, step 7700 loss:0.10189589112997055\n",
            "2022-05-11 15:30:24.863751 || Epoch #0, step 7800 loss:0.18827198445796967\n",
            "2022-05-11 15:31:30.378398 || Epoch #0, step 7900 loss:0.32646963000297546\n",
            "2022-05-11 15:32:35.876748 || Epoch #0, step 8000 loss:0.163126140832901\n",
            "2022-05-11 15:33:41.388649 || Epoch #0, step 8100 loss:0.19102002680301666\n",
            "2022-05-11 15:34:46.503821 || Epoch #0, step 8200 loss:0.4679866135120392\n",
            "2022-05-11 15:35:51.769668 || Epoch #0, step 8300 loss:0.39481791853904724\n",
            "2022-05-11 15:36:55.438767 || Epoch #0, step 8400 loss:0.15971870720386505\n",
            "2022-05-11 15:38:00.926363 || Epoch #0, step 8500 loss:0.22922973334789276\n",
            "2022-05-11 15:39:05.168117 || Epoch #0, step 8600 loss:0.18444229662418365\n",
            "2022-05-11 15:40:10.155766 || Epoch #0, step 8700 loss:0.2894517779350281\n",
            "2022-05-11 15:41:15.307797 || Epoch #0, step 8800 loss:0.2862023711204529\n",
            "2022-05-11 15:42:19.635500 || Epoch #0, step 8900 loss:0.3408846855163574\n",
            "2022-05-11 15:43:25.123541 || Epoch #0, step 9000 loss:0.2052304446697235\n",
            "2022-05-11 15:44:30.120392 || Epoch #0, step 9100 loss:0.1787370890378952\n",
            "2022-05-11 15:45:34.776856 || Epoch #0, step 9200 loss:0.31443917751312256\n",
            "2022-05-11 15:46:38.234335 || Epoch #0, step 9300 loss:0.318377822637558\n",
            "2022-05-11 15:47:43.358359 || Epoch #0, step 9400 loss:0.19290731847286224\n",
            "2022-05-11 15:48:48.855184 || Epoch #0, step 9500 loss:0.36423322558403015\n",
            "2022-05-11 15:49:53.939738 || Epoch #0, step 9600 loss:0.21446576714515686\n",
            "2022-05-11 15:50:59.155491 || Epoch #0, step 9700 loss:0.247310072183609\n",
            "2022-05-11 15:52:03.603129 || Epoch #0, step 9800 loss:0.3042951226234436\n",
            "2022-05-11 15:53:08.620006 || Epoch #0, step 9900 loss:0.371980220079422\n",
            "### Scheduler step ### \n",
            "\n",
            "2022-05-11 15:54:12.863890 || Epoch #0, step 10000 loss:0.24534232914447784\n",
            "2022-05-11 15:55:18.362831 || Epoch #0, step 10100 loss:0.19774310290813446\n",
            "2022-05-11 15:56:23.126532 || Epoch #0, step 10200 loss:0.24435710906982422\n",
            "2022-05-11 15:57:28.624035 || Epoch #0, step 10300 loss:0.29235395789146423\n",
            "2022-05-11 15:58:33.906501 || Epoch #0, step 10400 loss:0.08816847950220108\n",
            "2022-05-11 15:59:42.115591 || Epoch #0, step 10500 loss:0.144457146525383\n",
            "2022-05-11 16:00:46.251585 || Epoch #0, step 10600 loss:0.1436462253332138\n",
            "2022-05-11 16:01:51.251863 || Epoch #0, step 10700 loss:0.268555223941803\n",
            "2022-05-11 16:02:56.285944 || Epoch #0, step 10800 loss:0.2671835124492645\n",
            "2022-05-11 16:04:01.628882 || Epoch #0, step 10900 loss:0.29547277092933655\n",
            "2022-05-11 16:05:06.804978 || Epoch #0, step 11000 loss:0.2041366696357727\n",
            "2022-05-11 16:06:10.580213 || Epoch #0, step 11100 loss:0.23790711164474487\n",
            "2022-05-11 16:07:15.504630 || Epoch #0, step 11200 loss:0.3095417618751526\n",
            "2022-05-11 16:08:20.158009 || Epoch #0, step 11300 loss:0.3580605983734131\n",
            "2022-05-11 16:09:25.651486 || Epoch #0, step 11400 loss:0.21012946963310242\n",
            "2022-05-11 16:10:30.955058 || Epoch #0, step 11500 loss:0.46610814332962036\n",
            "2022-05-11 16:11:36.107377 || Epoch #0, step 11600 loss:0.10643947124481201\n",
            "2022-05-11 16:12:40.633469 || Epoch #0, step 11700 loss:0.32009410858154297\n",
            "2022-05-11 16:13:46.112661 || Epoch #0, step 11800 loss:0.16176815330982208\n",
            "2022-05-11 16:14:51.130882 || Epoch #0, step 11900 loss:0.15466701984405518\n",
            "2022-05-11 16:15:56.341435 || Epoch #0, step 12000 loss:0.2828591465950012\n",
            "2022-05-11 16:17:01.840739 || Epoch #0, step 12100 loss:0.292044997215271\n",
            "2022-05-11 16:18:07.340558 || Epoch #0, step 12200 loss:0.11929619312286377\n",
            "2022-05-11 16:19:12.851892 || Epoch #0, step 12300 loss:0.33443158864974976\n",
            "2022-05-11 16:20:18.180397 || Epoch #0, step 12400 loss:0.2581299841403961\n",
            "2022-05-11 16:21:22.752998 || Epoch #0, step 12500 loss:0.20747265219688416\n",
            "2022-05-11 16:22:27.884085 || Epoch #0, step 12600 loss:0.21909622848033905\n",
            "2022-05-11 16:23:33.180403 || Epoch #0, step 12700 loss:0.32390299439430237\n",
            "2022-05-11 16:24:38.674918 || Epoch #0, step 12800 loss:0.17063598334789276\n",
            "2022-05-11 16:25:44.167083 || Epoch #0, step 12900 loss:0.2731461524963379\n",
            "2022-05-11 16:26:48.391377 || Epoch #0, step 13000 loss:0.09627269953489304\n",
            "2022-05-11 16:27:53.502496 || Epoch #0, step 13100 loss:0.5307533144950867\n",
            "2022-05-11 16:28:59.010741 || Epoch #0, step 13200 loss:0.408229798078537\n",
            "2022-05-11 16:30:03.589049 || Epoch #0, step 13300 loss:0.22712133824825287\n",
            "2022-05-11 16:31:09.081466 || Epoch #0, step 13400 loss:0.15074488520622253\n",
            "2022-05-11 16:32:13.453877 || Epoch #0, step 13500 loss:0.27897951006889343\n",
            "2022-05-11 16:33:18.451541 || Epoch #0, step 13600 loss:0.3286040127277374\n",
            "2022-05-11 16:34:22.596482 || Epoch #0, step 13700 loss:0.18199191987514496\n",
            "2022-05-11 16:35:27.776988 || Epoch #0, step 13800 loss:0.20595955848693848\n",
            "2022-05-11 16:36:32.713805 || Epoch #0, step 13900 loss:0.2608891725540161\n",
            "2022-05-11 16:37:42.036491 || Epoch #0, step 14000 loss:0.2591518461704254\n",
            "2022-05-11 16:38:47.546878 || Epoch #0, step 14100 loss:0.21464334428310394\n",
            "2022-05-11 16:39:52.616566 || Epoch #0, step 14200 loss:0.21532544493675232\n",
            "2022-05-11 16:40:57.254937 || Epoch #0, step 14300 loss:0.3162468373775482\n",
            "2022-05-11 16:42:01.956276 || Epoch #0, step 14400 loss:0.09171699732542038\n",
            "2022-05-11 16:43:06.968140 || Epoch #0, step 14500 loss:0.19863276183605194\n",
            "2022-05-11 16:44:11.875373 || Epoch #0, step 14600 loss:0.32896068692207336\n",
            "2022-05-11 16:45:16.378709 || Epoch #0, step 14700 loss:0.30304062366485596\n",
            "2022-05-11 16:46:21.177713 || Epoch #0, step 14800 loss:0.12550188601016998\n",
            "2022-05-11 16:47:26.155508 || Epoch #0, step 14900 loss:0.30008482933044434\n",
            "2022-05-11 16:48:31.165464 || Epoch #0, step 15000 loss:0.3558128774166107\n",
            "2022-05-11 16:49:35.630308 || Epoch #0, step 15100 loss:0.3033931255340576\n",
            "2022-05-11 16:50:40.693305 || Epoch #0, step 15200 loss:0.2866881787776947\n",
            "2022-05-11 16:51:45.666026 || Epoch #0, step 15300 loss:0.40074217319488525\n",
            "2022-05-11 16:52:50.806145 || Epoch #0, step 15400 loss:0.15902872383594513\n",
            "2022-05-11 16:53:55.771579 || Epoch #0, step 15500 loss:0.14340683817863464\n",
            "2022-05-11 16:55:00.285761 || Epoch #0, step 15600 loss:0.2818347215652466\n",
            "2022-05-11 16:56:04.689251 || Epoch #0, step 15700 loss:0.1885792762041092\n",
            "2022-05-11 16:57:09.657702 || Epoch #0, step 15800 loss:0.11542702466249466\n",
            "2022-05-11 16:58:13.545298 || Epoch #0, step 15900 loss:0.18411970138549805\n",
            "2022-05-11 16:59:18.352687 || Epoch #0, step 16000 loss:0.24279174208641052\n",
            "2022-05-11 17:00:23.619776 || Epoch #0, step 16100 loss:0.13810917735099792\n",
            "2022-05-11 17:01:28.677520 || Epoch #0, step 16200 loss:0.2521871328353882\n",
            "2022-05-11 17:02:33.241125 || Epoch #0, step 16300 loss:0.1786036193370819\n",
            "2022-05-11 17:03:38.263974 || Epoch #0, step 16400 loss:0.44847193360328674\n",
            "2022-05-11 17:04:43.581711 || Epoch #0, step 16500 loss:0.20971564948558807\n",
            "2022-05-11 17:05:49.072743 || Epoch #0, step 16600 loss:0.1622765064239502\n",
            "2022-05-11 17:06:53.699259 || Epoch #0, step 16700 loss:0.1355457752943039\n",
            "2022-05-11 17:07:58.314872 || Epoch #0, step 16800 loss:0.054249994456768036\n",
            "2022-05-11 17:09:03.212907 || Epoch #0, step 16900 loss:0.14443637430667877\n",
            "2022-05-11 17:10:08.181041 || Epoch #0, step 17000 loss:0.22233669459819794\n",
            "2022-05-11 17:11:13.508082 || Epoch #0, step 17100 loss:0.28051838278770447\n",
            "2022-05-11 17:12:18.450272 || Epoch #0, step 17200 loss:0.13864019513130188\n",
            "2022-05-11 17:13:23.946901 || Epoch #0, step 17300 loss:0.19972187280654907\n",
            "2022-05-11 17:14:29.434011 || Epoch #0, step 17400 loss:0.16930769383907318\n",
            "2022-05-11 17:15:38.595930 || Epoch #0, step 17500 loss:0.21139474213123322\n",
            "2022-05-11 17:16:43.605316 || Epoch #0, step 17600 loss:0.12189298123121262\n",
            "2022-05-11 17:17:48.587469 || Epoch #0, step 17700 loss:0.29944726824760437\n",
            "2022-05-11 17:18:54.080474 || Epoch #0, step 17800 loss:0.3069995939731598\n",
            "2022-05-11 17:19:58.645142 || Epoch #0, step 17900 loss:0.3063890337944031\n",
            "2022-05-11 17:21:04.129833 || Epoch #0, step 18000 loss:0.16567541658878326\n",
            "2022-05-11 17:22:09.628165 || Epoch #0, step 18100 loss:0.25476792454719543\n",
            "2022-05-11 17:23:15.029624 || Epoch #0, step 18200 loss:0.24654993414878845\n",
            "2022-05-11 17:24:20.302099 || Epoch #0, step 18300 loss:0.2214260846376419\n",
            "2022-05-11 17:25:25.155857 || Epoch #0, step 18400 loss:0.31207385659217834\n",
            "2022-05-11 17:26:30.238893 || Epoch #0, step 18500 loss:0.24994993209838867\n",
            "2022-05-11 17:27:34.979410 || Epoch #0, step 18600 loss:0.2779821753501892\n",
            "2022-05-11 17:28:38.908845 || Epoch #0, step 18700 loss:0.05319322273135185\n",
            "2022-05-11 17:29:44.266589 || Epoch #0, step 18800 loss:0.17199671268463135\n",
            "2022-05-11 17:30:49.337372 || Epoch #0, step 18900 loss:0.18580226600170135\n",
            "2022-05-11 17:31:54.843881 || Epoch #0, step 19000 loss:0.2626139223575592\n",
            "2022-05-11 17:32:59.270353 || Epoch #0, step 19100 loss:0.12066946923732758\n",
            "2022-05-11 17:34:04.760733 || Epoch #0, step 19200 loss:0.14661185443401337\n",
            "2022-05-11 17:35:09.558254 || Epoch #0, step 19300 loss:0.31704428791999817\n",
            "2022-05-11 17:36:14.184819 || Epoch #0, step 19400 loss:0.14322681725025177\n",
            "2022-05-11 17:37:19.140198 || Epoch #0, step 19500 loss:0.26845061779022217\n",
            "2022-05-11 17:38:24.623268 || Epoch #0, step 19600 loss:0.2822588086128235\n",
            "2022-05-11 17:39:29.439497 || Epoch #0, step 19700 loss:0.2670517861843109\n",
            "2022-05-11 17:40:33.293924 || Epoch #0, step 19800 loss:0.2575122117996216\n",
            "2022-05-11 17:41:38.256132 || Epoch #0, step 19900 loss:0.24979044497013092\n",
            "### Scheduler step ### \n",
            "\n",
            "2022-05-11 17:42:42.571399 || Epoch #0, step 20000 loss:0.17372813820838928\n",
            "2022-05-11 17:43:48.018510 || Epoch #0, step 20100 loss:0.19500850141048431\n",
            "2022-05-11 17:44:53.152788 || Epoch #0, step 20200 loss:0.14810623228549957\n",
            "2022-05-11 17:45:58.640381 || Epoch #0, step 20300 loss:0.22123335301876068\n",
            "2022-05-11 17:47:02.747052 || Epoch #0, step 20400 loss:0.16064338386058807\n",
            "2022-05-11 17:48:07.010131 || Epoch #0, step 20500 loss:0.18235765397548676\n",
            "2022-05-11 17:49:11.430107 || Epoch #0, step 20600 loss:0.18100275099277496\n",
            "2022-05-11 17:50:15.984107 || Epoch #0, step 20700 loss:0.1652320921421051\n",
            "2022-05-11 17:51:21.332560 || Epoch #0, step 20800 loss:0.3200256824493408\n",
            "2022-05-11 17:52:25.836731 || Epoch #0, step 20900 loss:0.130313903093338\n",
            "2022-05-11 17:53:34.179221 || Epoch #0, step 21000 loss:0.19444400072097778\n",
            "2022-05-11 17:54:39.368774 || Epoch #0, step 21100 loss:0.19973517954349518\n",
            "2022-05-11 17:55:43.700163 || Epoch #0, step 21200 loss:0.20561495423316956\n",
            "2022-05-11 17:56:49.192292 || Epoch #0, step 21300 loss:0.15334583818912506\n",
            "2022-05-11 17:57:54.484474 || Epoch #0, step 21400 loss:0.1162371039390564\n",
            "2022-05-11 17:58:58.851953 || Epoch #0, step 21500 loss:0.13407839834690094\n",
            "2022-05-11 18:00:04.291363 || Epoch #0, step 21600 loss:0.25464296340942383\n",
            "2022-05-11 18:01:08.864507 || Epoch #0, step 21700 loss:0.3400781750679016\n",
            "2022-05-11 18:02:14.025314 || Epoch #0, step 21800 loss:0.26657378673553467\n",
            "2022-05-11 18:03:19.072825 || Epoch #0, step 21900 loss:0.32522761821746826\n",
            "2022-05-11 18:04:24.336127 || Epoch #0, step 22000 loss:0.28966161608695984\n",
            "2022-05-11 18:05:29.718092 || Epoch #0, step 22100 loss:0.22533035278320312\n",
            "2022-05-11 18:06:34.856190 || Epoch #0, step 22200 loss:0.22983089089393616\n",
            "2022-05-11 18:07:39.791995 || Epoch #0, step 22300 loss:0.3076651394367218\n",
            "2022-05-11 18:08:45.278492 || Epoch #0, step 22400 loss:0.16377376019954681\n",
            "2022-05-11 18:09:49.808974 || Epoch #0, step 22500 loss:0.15267059206962585\n",
            "2022-05-11 18:10:54.925071 || Epoch #0, step 22600 loss:0.18275699019432068\n",
            "2022-05-11 18:12:00.410849 || Epoch #0, step 22700 loss:0.20038962364196777\n",
            "2022-05-11 18:13:05.912414 || Epoch #0, step 22800 loss:0.11490147560834885\n",
            "2022-05-11 18:14:11.392117 || Epoch #0, step 22900 loss:0.1917339712381363\n",
            "2022-05-11 18:15:15.855185 || Epoch #0, step 23000 loss:0.14738741517066956\n",
            "2022-05-11 18:16:20.393651 || Epoch #0, step 23100 loss:0.1877787560224533\n",
            "2022-05-11 18:17:25.404196 || Epoch #0, step 23200 loss:0.14367738366127014\n",
            "2022-05-11 18:18:30.016120 || Epoch #0, step 23300 loss:0.17413146793842316\n",
            "2022-05-11 18:19:34.681169 || Epoch #0, step 23400 loss:0.13998880982398987\n",
            "2022-05-11 18:20:39.419435 || Epoch #0, step 23500 loss:0.17960819602012634\n",
            "2022-05-11 18:21:44.943174 || Epoch #0, step 23600 loss:0.27261489629745483\n",
            "2022-05-11 18:22:49.752714 || Epoch #0, step 23700 loss:0.16498009860515594\n",
            "2022-05-11 18:23:54.757447 || Epoch #0, step 23800 loss:0.15928786993026733\n",
            "2022-05-11 18:24:59.677141 || Epoch #0, step 23900 loss:0.39004793763160706\n",
            "2022-05-11 18:26:04.675730 || Epoch #0, step 24000 loss:0.22867107391357422\n",
            "2022-05-11 18:27:09.185342 || Epoch #0, step 24100 loss:0.09446953982114792\n",
            "2022-05-11 18:28:13.923245 || Epoch #0, step 24200 loss:0.1828269213438034\n",
            "2022-05-11 18:29:18.601147 || Epoch #0, step 24300 loss:0.0914943590760231\n",
            "2022-05-11 18:30:23.657238 || Epoch #0, step 24400 loss:0.16833560168743134\n",
            "2022-05-11 18:31:32.357049 || Epoch #0, step 24500 loss:0.24523763358592987\n",
            "2022-05-11 18:32:37.361746 || Epoch #0, step 24600 loss:0.18086937069892883\n",
            "2022-05-11 18:33:42.113470 || Epoch #0, step 24700 loss:0.2910635769367218\n",
            "2022-05-11 18:34:47.106497 || Epoch #0, step 24800 loss:0.21175417304039001\n",
            "2022-05-11 18:35:50.941740 || Epoch #0, step 24900 loss:0.26918742060661316\n",
            "2022-05-11 18:36:55.120460 || Epoch #0, step 25000 loss:0.2316790223121643\n",
            "2022-05-11 18:37:59.821721 || Epoch #0, step 25100 loss:0.1780136227607727\n",
            "2022-05-11 18:39:05.096083 || Epoch #0, step 25200 loss:0.14787310361862183\n",
            "2022-05-11 18:40:10.084429 || Epoch #0, step 25300 loss:0.22067376971244812\n",
            "2022-05-11 18:41:15.229825 || Epoch #0, step 25400 loss:0.14947575330734253\n",
            "2022-05-11 18:42:20.605288 || Epoch #0, step 25500 loss:0.16116741299629211\n",
            "2022-05-11 18:43:25.209876 || Epoch #0, step 25600 loss:0.1677742302417755\n",
            "2022-05-11 18:44:29.981896 || Epoch #0, step 25700 loss:0.07487138360738754\n",
            "2022-05-11 18:45:34.952238 || Epoch #0, step 25800 loss:0.25310224294662476\n",
            "2022-05-11 18:46:40.248603 || Epoch #0, step 25900 loss:0.14936797320842743\n",
            "2022-05-11 18:47:45.448031 || Epoch #0, step 26000 loss:0.15693198144435883\n",
            "2022-05-11 18:48:50.222058 || Epoch #0, step 26100 loss:0.18435360491275787\n",
            "2022-05-11 18:49:54.132174 || Epoch #0, step 26200 loss:0.18076685070991516\n",
            "2022-05-11 18:50:59.323330 || Epoch #0, step 26300 loss:0.5840103626251221\n",
            "2022-05-11 18:52:04.639500 || Epoch #0, step 26400 loss:0.14713002741336823\n",
            "2022-05-11 18:53:09.703604 || Epoch #0, step 26500 loss:0.24768821895122528\n",
            "2022-05-11 18:54:14.468952 || Epoch #0, step 26600 loss:0.15960656106472015\n",
            "2022-05-11 18:55:19.521641 || Epoch #0, step 26700 loss:0.1684046983718872\n",
            "2022-05-11 18:56:24.874838 || Epoch #0, step 26800 loss:0.2539181709289551\n",
            "2022-05-11 18:57:30.373515 || Epoch #0, step 26900 loss:0.1038503348827362\n",
            "2022-05-11 18:58:35.396650 || Epoch #0, step 27000 loss:0.1820015013217926\n",
            "2022-05-11 18:59:40.896370 || Epoch #0, step 27100 loss:0.10349700599908829\n",
            "2022-05-11 19:00:45.206977 || Epoch #0, step 27200 loss:0.1793367564678192\n",
            "2022-05-11 19:01:50.667666 || Epoch #0, step 27300 loss:0.11438822001218796\n",
            "2022-05-11 19:02:55.366988 || Epoch #0, step 27400 loss:0.23778611421585083\n",
            "2022-05-11 19:04:00.510065 || Epoch #0, step 27500 loss:0.10718279331922531\n",
            "2022-05-11 19:05:05.491218 || Epoch #0, step 27600 loss:0.1720118671655655\n",
            "2022-05-11 19:06:10.974782 || Epoch #0, step 27700 loss:0.18558956682682037\n",
            "2022-05-11 19:07:16.177003 || Epoch #0, step 27800 loss:0.15430259704589844\n",
            "2022-05-11 19:08:21.673824 || Epoch #0, step 27900 loss:0.13399198651313782\n",
            "2022-05-11 19:09:31.261940 || Epoch #0, step 28000 loss:0.42301467061042786\n",
            "2022-05-11 19:10:34.985603 || Epoch #0, step 28100 loss:0.15880869328975677\n",
            "2022-05-11 19:11:40.470765 || Epoch #0, step 28200 loss:0.25975489616394043\n",
            "2022-05-11 19:12:45.063257 || Epoch #0, step 28300 loss:0.12288074940443039\n",
            "2022-05-11 19:13:50.145777 || Epoch #0, step 28400 loss:0.2202523797750473\n",
            "2022-05-11 19:14:54.703014 || Epoch #0, step 28500 loss:0.1504918932914734\n",
            "2022-05-11 19:15:59.216881 || Epoch #0, step 28600 loss:0.19342242181301117\n",
            "2022-05-11 19:17:04.072904 || Epoch #0, step 28700 loss:0.2558821737766266\n",
            "2022-05-11 19:18:09.045068 || Epoch #0, step 28800 loss:0.21212318539619446\n",
            "2022-05-11 19:19:13.021290 || Epoch #0, step 28900 loss:0.09722846746444702\n",
            "2022-05-11 19:20:16.955324 || Epoch #0, step 29000 loss:0.08961746841669083\n",
            "2022-05-11 19:21:21.942397 || Epoch #0, step 29100 loss:0.05409015342593193\n",
            "2022-05-11 19:22:27.440662 || Epoch #0, step 29200 loss:0.2278432697057724\n",
            "2022-05-11 19:23:32.113251 || Epoch #0, step 29300 loss:0.15198016166687012\n",
            "2022-05-11 19:24:36.478599 || Epoch #0, step 29400 loss:0.12043967843055725\n",
            "2022-05-11 19:25:41.991918 || Epoch #0, step 29500 loss:0.1781824827194214\n",
            "2022-05-11 19:26:46.992882 || Epoch #0, step 29600 loss:0.12962615489959717\n",
            "2022-05-11 19:27:52.205254 || Epoch #0, step 29700 loss:0.0798037126660347\n",
            "2022-05-11 19:28:55.790442 || Epoch #0, step 29800 loss:0.2149462252855301\n",
            "2022-05-11 19:30:01.279431 || Epoch #0, step 29900 loss:0.08368710428476334\n",
            "### Scheduler step ### \n",
            "\n",
            "2022-05-11 19:31:06.391602 || Epoch #0, step 30000 loss:0.15737488865852356\n",
            "2022-05-11 19:32:11.456016 || Epoch #0, step 30100 loss:0.324415922164917\n",
            "2022-05-11 19:33:16.710576 || Epoch #0, step 30200 loss:0.17920099198818207\n",
            "2022-05-11 19:34:21.842726 || Epoch #0, step 30300 loss:0.1600608378648758\n",
            "2022-05-11 19:35:27.341032 || Epoch #0, step 30400 loss:0.19142092764377594\n",
            "2022-05-11 19:36:31.621926 || Epoch #0, step 30500 loss:0.17420455813407898\n",
            "2022-05-11 19:37:37.121353 || Epoch #0, step 30600 loss:0.205419659614563\n",
            "2022-05-11 19:38:41.915816 || Epoch #0, step 30700 loss:0.2102462351322174\n",
            "2022-05-11 19:39:47.406930 || Epoch #0, step 30800 loss:0.18938545882701874\n",
            "2022-05-11 19:40:52.470100 || Epoch #0, step 30900 loss:0.19413703680038452\n",
            "2022-05-11 19:41:57.725005 || Epoch #0, step 31000 loss:0.18605738878250122\n",
            "2022-05-11 19:43:02.268070 || Epoch #0, step 31100 loss:0.3017677366733551\n",
            "2022-05-11 19:44:07.186972 || Epoch #0, step 31200 loss:0.21257749199867249\n",
            "2022-05-11 19:45:12.124802 || Epoch #0, step 31300 loss:0.12143997102975845\n",
            "2022-05-11 19:46:17.486722 || Epoch #0, step 31400 loss:0.24516437947750092\n",
            "2022-05-11 19:47:25.723409 || Epoch #0, step 31500 loss:0.19814319908618927\n",
            "2022-05-11 19:48:31.068464 || Epoch #0, step 31600 loss:0.22038669884204865\n",
            "2022-05-11 19:49:36.088025 || Epoch #0, step 31700 loss:0.1547240912914276\n",
            "2022-05-11 19:50:40.844618 || Epoch #0, step 31800 loss:0.3149467408657074\n",
            "2022-05-11 19:51:45.652471 || Epoch #0, step 31900 loss:0.20188845694065094\n",
            "2022-05-11 19:52:49.708104 || Epoch #0, step 32000 loss:0.16230593621730804\n",
            "2022-05-11 19:53:55.061865 || Epoch #0, step 32100 loss:0.18687129020690918\n",
            "2022-05-11 19:55:00.081097 || Epoch #0, step 32200 loss:0.21282434463500977\n",
            "2022-05-11 19:56:05.047593 || Epoch #0, step 32300 loss:0.2597602605819702\n",
            "2022-05-11 19:57:09.527244 || Epoch #0, step 32400 loss:0.1262926161289215\n",
            "2022-05-11 19:58:14.854241 || Epoch #0, step 32500 loss:0.07059987634420395\n",
            "2022-05-11 19:59:19.646877 || Epoch #0, step 32600 loss:0.13832338154315948\n",
            "2022-05-11 20:00:25.093711 || Epoch #0, step 32700 loss:0.20166617631912231\n",
            "2022-05-11 20:01:30.048339 || Epoch #0, step 32800 loss:0.15372741222381592\n",
            "2022-05-11 20:02:35.064693 || Epoch #0, step 32900 loss:0.10235493630170822\n",
            "2022-05-11 20:03:40.584436 || Epoch #0, step 33000 loss:0.1708255261182785\n",
            "2022-05-11 20:04:45.445490 || Epoch #0, step 33100 loss:0.1672399789094925\n",
            "2022-05-11 20:05:50.972056 || Epoch #0, step 33200 loss:0.2056676149368286\n",
            "2022-05-11 20:06:56.495204 || Epoch #0, step 33300 loss:0.16777096688747406\n",
            "2022-05-11 20:08:01.601923 || Epoch #0, step 33400 loss:0.14668406546115875\n",
            "2022-05-11 20:09:06.918607 || Epoch #0, step 33500 loss:0.26176390051841736\n",
            "2022-05-11 20:10:12.103796 || Epoch #0, step 33600 loss:0.21802733838558197\n",
            "2022-05-11 20:11:17.359371 || Epoch #0, step 33700 loss:0.408914178609848\n",
            "2022-05-11 20:12:20.887053 || Epoch #0, step 33800 loss:0.11769825965166092\n",
            "2022-05-11 20:13:25.676152 || Epoch #0, step 33900 loss:0.14842456579208374\n",
            "2022-05-11 20:14:30.979685 || Epoch #0, step 34000 loss:0.14374110102653503\n",
            "2022-05-11 20:15:35.738067 || Epoch #0, step 34100 loss:0.21346205472946167\n",
            "2022-05-11 20:16:40.604575 || Epoch #0, step 34200 loss:0.14763125777244568\n",
            "2022-05-11 20:17:45.332080 || Epoch #0, step 34300 loss:0.24522404372692108\n",
            "2022-05-11 20:18:49.827096 || Epoch #0, step 34400 loss:0.08915301412343979\n",
            "2022-05-11 20:19:55.153214 || Epoch #0, step 34500 loss:0.24904495477676392\n",
            "2022-05-11 20:20:59.888118 || Epoch #0, step 34600 loss:0.19649404287338257\n",
            "2022-05-11 20:22:04.774069 || Epoch #0, step 34700 loss:0.25569087266921997\n",
            "2022-05-11 20:23:09.556650 || Epoch #0, step 34800 loss:0.21271705627441406\n",
            "2022-05-11 20:24:14.529315 || Epoch #0, step 34900 loss:0.18374215066432953\n",
            "2022-05-11 20:25:23.580928 || Epoch #0, step 35000 loss:0.0624319352209568\n",
            "2022-05-11 20:26:28.942856 || Epoch #0, step 35100 loss:0.3001481294631958\n",
            "2022-05-11 20:27:34.020895 || Epoch #0, step 35200 loss:0.1511159986257553\n",
            "2022-05-11 20:28:39.381594 || Epoch #0, step 35300 loss:0.16929875314235687\n",
            "2022-05-11 20:29:44.318481 || Epoch #0, step 35400 loss:0.32478606700897217\n",
            "2022-05-11 20:30:49.242246 || Epoch #0, step 35500 loss:0.10553159564733505\n",
            "2022-05-11 20:31:54.784565 || Epoch #0, step 35600 loss:0.1507207155227661\n",
            "2022-05-11 20:33:00.201557 || Epoch #0, step 35700 loss:0.32614752650260925\n",
            "2022-05-11 20:34:05.734002 || Epoch #0, step 35800 loss:0.21587204933166504\n",
            "2022-05-11 20:35:10.683847 || Epoch #0, step 35900 loss:0.12042075395584106\n",
            "2022-05-11 20:36:15.419914 || Epoch #0, step 36000 loss:0.2199581414461136\n",
            "2022-05-11 20:37:20.785125 || Epoch #0, step 36100 loss:0.2883566915988922\n",
            "2022-05-11 20:38:25.888415 || Epoch #0, step 36200 loss:0.08588124811649323\n",
            "2022-05-11 20:39:31.113319 || Epoch #0, step 36300 loss:0.12464863806962967\n",
            "2022-05-11 20:40:36.130303 || Epoch #0, step 36400 loss:0.2791304290294647\n",
            "2022-05-11 20:41:41.387360 || Epoch #0, step 36500 loss:0.08162318915128708\n",
            "2022-05-11 20:42:46.473865 || Epoch #0, step 36600 loss:0.17132455110549927\n",
            "2022-05-11 20:43:51.747037 || Epoch #0, step 36700 loss:0.09297121316194534\n",
            "2022-05-11 20:44:57.275677 || Epoch #0, step 36800 loss:0.2491244673728943\n",
            "2022-05-11 20:46:01.295876 || Epoch #0, step 36900 loss:0.06695683300495148\n",
            "2022-05-11 20:47:05.641360 || Epoch #0, step 37000 loss:0.19994816184043884\n",
            "2022-05-11 20:48:11.178913 || Epoch #0, step 37100 loss:0.2133636176586151\n",
            "2022-05-11 20:49:16.360403 || Epoch #0, step 37200 loss:0.1531820297241211\n",
            "2022-05-11 20:50:21.649317 || Epoch #0, step 37300 loss:0.201888307929039\n",
            "2022-05-11 20:51:27.183952 || Epoch #0, step 37400 loss:0.14257849752902985\n",
            "2022-05-11 20:52:32.733580 || Epoch #0, step 37500 loss:0.14585177600383759\n",
            "2022-05-11 20:53:37.899484 || Epoch #0, step 37600 loss:0.12347977608442307\n",
            "2022-05-11 20:54:42.624909 || Epoch #0, step 37700 loss:0.2071705013513565\n",
            "2022-05-11 20:55:47.535299 || Epoch #0, step 37800 loss:0.14022938907146454\n",
            "2022-05-11 20:56:52.593042 || Epoch #0, step 37900 loss:0.13155144453048706\n",
            "2022-05-11 20:57:56.799461 || Epoch #0, step 38000 loss:0.26349174976348877\n",
            "2022-05-11 20:59:01.826237 || Epoch #0, step 38100 loss:0.25295519828796387\n",
            "2022-05-11 21:00:06.606951 || Epoch #0, step 38200 loss:0.13586314022541046\n",
            "2022-05-11 21:01:11.806725 || Epoch #0, step 38300 loss:0.15710842609405518\n",
            "2022-05-11 21:02:16.366222 || Epoch #0, step 38400 loss:0.3326410949230194\n",
            "2022-05-11 21:03:25.693265 || Epoch #0, step 38500 loss:0.14626024663448334\n",
            "2022-05-11 21:04:31.262600 || Epoch #0, step 38600 loss:0.048612892627716064\n",
            "2022-05-11 21:05:36.714288 || Epoch #0, step 38700 loss:0.12404893338680267\n",
            "2022-05-11 21:06:41.507871 || Epoch #0, step 38800 loss:0.24795691668987274\n",
            "2022-05-11 21:07:46.525856 || Epoch #0, step 38900 loss:0.20255571603775024\n",
            "2022-05-11 21:08:51.563281 || Epoch #0, step 39000 loss:0.20012956857681274\n",
            "2022-05-11 21:09:56.603756 || Epoch #0, step 39100 loss:0.11430513858795166\n",
            "2022-05-11 21:11:00.768952 || Epoch #0, step 39200 loss:0.14365775883197784\n",
            "2022-05-11 21:12:06.295320 || Epoch #0, step 39300 loss:0.2245672345161438\n",
            "2022-05-11 21:13:11.453153 || Epoch #0, step 39400 loss:0.050364114344120026\n",
            "2022-05-11 21:14:15.769461 || Epoch #0, step 39500 loss:0.03456916660070419\n",
            "2022-05-11 21:15:20.431648 || Epoch #0, step 39600 loss:0.1580227166414261\n",
            "2022-05-11 21:16:25.097300 || Epoch #0, step 39700 loss:0.20491135120391846\n",
            "2022-05-11 21:17:29.713321 || Epoch #0, step 39800 loss:0.03455096110701561\n",
            "2022-05-11 21:18:34.698483 || Epoch #0, step 39900 loss:0.13561086356639862\n",
            "### Scheduler step ### \n",
            "\n",
            "2022-05-11 21:19:39.041615 || Epoch #0, step 40000 loss:0.10916882008314133\n",
            "2022-05-11 21:20:43.707719 || Epoch #0, step 40100 loss:0.1681661158800125\n",
            "2022-05-11 21:21:48.058674 || Epoch #0, step 40200 loss:0.23515836894512177\n",
            "2022-05-11 21:22:53.406615 || Epoch #0, step 40300 loss:0.2236303836107254\n",
            "2022-05-11 21:23:58.756801 || Epoch #0, step 40400 loss:0.1683596819639206\n",
            "2022-05-11 21:25:04.286544 || Epoch #0, step 40500 loss:0.1398247331380844\n",
            "2022-05-11 21:26:09.819292 || Epoch #0, step 40600 loss:0.14043498039245605\n",
            "2022-05-11 21:27:15.368657 || Epoch #0, step 40700 loss:0.1210845559835434\n",
            "2022-05-11 21:28:20.392730 || Epoch #0, step 40800 loss:0.15182413160800934\n",
            "2022-05-11 21:29:25.344555 || Epoch #0, step 40900 loss:0.1293715238571167\n",
            "2022-05-11 21:30:30.828337 || Epoch #0, step 41000 loss:0.053486425429582596\n",
            "2022-05-11 21:31:35.945796 || Epoch #0, step 41100 loss:0.05595836415886879\n",
            "2022-05-11 21:32:41.323687 || Epoch #0, step 41200 loss:0.15316084027290344\n",
            "2022-05-11 21:33:46.459800 || Epoch #0, step 41300 loss:0.14292511343955994\n",
            "2022-05-11 21:34:51.969739 || Epoch #0, step 41400 loss:0.10559777915477753\n"
          ]
        }
      ],
      "source": [
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = lr_scheduler.StepLR(optim, step_size=1, gamma=0.5)\n",
        "loss_history = []\n",
        "i = 0\n",
        "for epoch in range(1):\n",
        "    for batch in dataloader:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device) \n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if len(loss_history) % 100 == 0: #logging\n",
        "          print(f\"{datetime.now()} || Epoch #{epoch}, step {len(loss_history) - (dataset.__len__() * epoch)} loss:{float(loss)}\")\n",
        "\n",
        "        loss_history.append(float(loss))\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        if len(loss_history) % 10000 == 0 and i>0: #LR scheduling\n",
        "          print(\"### Scheduler step ### \\n\")\n",
        "          scheduler.step()\n",
        "        \n",
        "        if len(loss_history) % 3500 == 0:\n",
        "          Path(f\"/content/drive/MyDrive/Diplom/Pretraining/save_{i}\").mkdir(exist_ok=True) #saving\n",
        "          model.save_pretrained(f\"/content/drive/MyDrive/Diplom/Pretraining/save_{i}\")\n",
        "          i+=1\n",
        "\n",
        "model.eval()\n",
        "model.save_pretrained(\"/content/drive/MyDrive/Diplom/Pretraining\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx-azeVZvEDN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "a4b377f9-041c-497e-86f6-c225c2bfbd28"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-79621fadbe14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Diplom/Pretraining\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/Diplom/Pretraining\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE2iYF2boEVB"
      },
      "source": [
        "# Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daNES9eLSMZP",
        "outputId": "3c5ab44b-5cff-44cf-cfff-6d71fe41f0b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Diplom/Summarization/arxiv-dataset.zip\n",
            "replace __MACOSX/._arxiv-dataset? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip  /content/drive/MyDrive/Diplom/Summarization/arxiv-dataset.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drr0i2_FgFTk"
      },
      "source": [
        "exctractive summary, training and dataset similar to pretraining (pre0tokenization?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtbReUxDoNxM"
      },
      "source": [
        "https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ycD1HCa7YHrX"
      },
      "outputs": [],
      "source": [
        "class Arxiv_summarization_dataset(Dataset): #maximum sequence length for this model (18038 > 1024)!\n",
        "    #articles might be too long and geting truncated?\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        self.tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\") #переименовать чтоб не путать?\n",
        "\n",
        "    def __len__(self): #keep all the file in the ram\n",
        "      with open(self.filename) as f:\n",
        "            for i, _ in enumerate(f, 1):\n",
        "                pass\n",
        "      return i\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx): #Add exctractive summary prior. generate dataset separetely?\n",
        "      try:\n",
        "        item = {}\n",
        "        data = json.loads(getline(self.filename, idx))\n",
        "        article_text = data['article_text']\n",
        "        article_text = ' '.join(article_text)\n",
        "\n",
        "        abstract_text = ' '.join(data['abstract_text']).replace(\"<S>\", '').replace(\"</S>\", '')\n",
        "\n",
        "        tokenized = self.tokenizer(article_text, truncation=True, padding=True)\n",
        "        item['input_ids'], item['attention_mask'] = torch.tensor(tokenized['input_ids']), torch.tensor(tokenized['attention_mask'])\n",
        "        item['labels'] = torch.tensor(self.tokenizer(abstract_text, truncation=True, padding=True)['input_ids'])\n",
        "      except:\n",
        "        idx += 1\n",
        "        self.__getitem__(idx)\n",
        "\n",
        "      return item"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Arxiv_lazy_dataset(Dataset): \n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        self.tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\") #переименовать чтоб не путать?\n",
        "\n",
        "    def __len__(self): #keep all the file in the ram\n",
        "      with open(self.filename) as f:\n",
        "            for i, _ in enumerate(f, 1):\n",
        "                pass\n",
        "      return i\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx): #Add exctractive summary prior. generate dataset separetely?\n",
        "      item = {}\n",
        "      try:\n",
        "        item = json.loads(getline(self.filename, idx))\n",
        "        item['input_ids'] = torch.tensor(item['input_ids'])\n",
        "        item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
        "        item['labels'] = torch.tensor(item['labels'])\n",
        "      except:\n",
        "        print(f\"Error on line {idx}\")\n",
        "        idx += 1\n",
        "        self.__getitem__(idx)\n",
        "      return item"
      ],
      "metadata": {
        "id": "s_wCRjQuZxEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zk7R5DEqYHuR"
      },
      "outputs": [],
      "source": [
        "summarization_dataset = Arxiv_summarization_dataset(\"/content/arxiv-dataset/train.txt\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = Arxiv_summarization_test_dataset(\"/content/arxiv-dataset/val.txt\") "
      ],
      "metadata": {
        "id": "9KSr29uLxzMH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iPb6YWbBYHxK"
      },
      "outputs": [],
      "source": [
        "summarization_dataloader = DataLoader(summarization_dataset, batch_size=1, shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "6V5cL7T5x9qA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CnaIDaUdYH6Q"
      },
      "outputs": [],
      "source": [
        "summarization_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\") #(\"/content/drive/MyDrive/Diplom/Summarization/save_5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2hUdVW9XxFch"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htg5nkCIxCFJ",
        "outputId": "713345a2-4834-4bde-969c-6959f24b4b8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "device = torch.device('cuda')\n",
        "summarization_model.to(device)\n",
        "summarization_model.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model():\n",
        "  summarization_model.eval()  \n",
        "  res = []\n",
        "  lib_rouge_list = []\n",
        "  bleu_list = []\n",
        "  rouge_list = []\n",
        "\n",
        "  for batch in val_dataloader:\n",
        "    #try:\n",
        "      entities = [i[0] for i in batch['entities']] #because fuckind dataloader\n",
        "\n",
        "      generated_tokens = summarization_model.generate(batch['input_ids'].to(device), max_length=150)\n",
        "     \n",
        "      decoded_preds = val_dataset.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "      \n",
        "      bleu = ner_bleu_metric(entities, decoded_preds)\n",
        "      bleu_list.append(bleu)  \n",
        "      \n",
        "      rouge = ner_rouge_metric(entities, decoded_preds)\n",
        "      rouge_list.append(rouge)   \n",
        "     \n",
        "      lib_rouge_list.append(rouge_lib.get_scores(decoded_preds[0], batch['abstract_text'][0])) #rouge.get_scores(hypothesis, reference)\n",
        "     \n",
        "\n",
        "      # res.append(decoded_preds)\n",
        "      # baseline_res.append(base_decoded_preds)\n",
        "\n",
        "      if len(rouge_list) == 100:\n",
        "        break\n",
        "    # except Exception as e:\n",
        "    #   print(e)\n",
        "    #   pass\n",
        "\n",
        "  summarization_model.train()  \n",
        " \n",
        "  print(f\"ner_bleu: model {statistics.mean(bleu_list):.2f} | ner_rouge: model {statistics.mean(rouge_list):.2f}\")\n",
        "  rouge_1_f = []\n",
        "  rouge_1_p = []\n",
        "  rouge_1_r = []\n",
        "\n",
        "  rouge_2_f = []\n",
        "  rouge_2_p = []\n",
        "  rouge_2_r = []\n",
        "\n",
        "  rouge_l_f = []\n",
        "  rouge_l_p = []\n",
        "  rouge_l_r = []\n",
        "\n",
        "  for measurement in lib_rouge_list:\n",
        "    rouge_1_f.append(measurement[0]['rouge-1']['f'])\n",
        "    rouge_1_p.append(measurement[0]['rouge-1']['p'])\n",
        "    rouge_1_r.append(measurement[0]['rouge-1']['r'])\n",
        "\n",
        "    rouge_2_f.append(measurement[0]['rouge-2']['f'])\n",
        "    rouge_2_p.append(measurement[0]['rouge-2']['p'])\n",
        "    rouge_2_r.append(measurement[0]['rouge-2']['r'])\n",
        "\n",
        "    rouge_l_f.append(measurement[0]['rouge-l']['f'])\n",
        "    rouge_l_p.append(measurement[0]['rouge-l']['p'])\n",
        "    rouge_l_r.append(measurement[0]['rouge-l']['r'])\n",
        "\n",
        "  print(f\"\"\"base_lib_rouge_list:   \n",
        "          rouge_1_f {statistics.mean(rouge_1_f):.2f} \n",
        "          rouge_1_p {statistics.mean(rouge_1_p):.2f} \n",
        "          rouge_1_r {statistics.mean(rouge_1_r):.2f} \\n\n",
        "          rouge_2_f {statistics.mean(rouge_2_f):.2f} \n",
        "          rouge_2_p {statistics.mean(rouge_2_p):.2f} \n",
        "          rouge_2_r {statistics.mean(rouge_2_r):.2f} \\n\n",
        "          rouge_l_f {statistics.mean(rouge_l_f):.2f} \n",
        "          rouge_l_p {statistics.mean(rouge_l_p):.2f} \n",
        "          rouge_l_r {statistics.mean(rouge_l_r):.2f} \\n        \n",
        "          \"\"\")\n"
      ],
      "metadata": {
        "id": "5E3BU4OXxmgY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(summarization_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTFnaJZHwugK",
        "outputId": "84933721-0523-4d2e-ba6e-e11bdbf4cc25"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "203037"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim = AdamW(summarization_model.parameters(), lr=2e-5)\n",
        "scheduler = lr_scheduler.StepLR(optim, step_size=1, gamma=0.5)\n",
        "loss_history = []\n",
        "i = 0\n",
        "for epoch in range(1):\n",
        "    for batch in summarization_dataloader:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device) \n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = summarization_model(input_ids, labels=labels, attention_mask=attention_mask)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if len(loss_history) % 100 == 0: #logging\n",
        "          print(f\"{datetime.now()} || Epoch #{epoch}, step {len(loss_history) - (summarization_dataset.__len__() * epoch)} loss:{float(loss)}\")\n",
        "          #eval_model()\n",
        "\n",
        "        loss_history.append(float(loss))\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        if len(loss_history) % 5000 == 0: #LR scheduling\n",
        "          eval_model()\n",
        "          print(\"### Scheduler step ### \\n\")\n",
        "          scheduler.step()\n",
        "          i+=1\n",
        "        \n",
        "        # if len(loss_history) % 3500 == 0:\n",
        "        #   Path(f\"/content/drive/MyDrive/Diplom/Summarization/save_{i}\").mkdir(exist_ok=True) #saving\n",
        "        #   summarization_model.save_pretrained(f\"/content/drive/MyDrive/Diplom/Summarization/save_{i}\")\n",
        "        #   i+=1\n",
        "\n",
        "summarization_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GhrxlG9jinQY",
        "outputId": "3fa64ffe-70a3-4a6a-bd07-bc54d65b9441"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-05-15 20:21:10.015756 || Epoch #0, step 0 loss:3.1951050758361816\n",
            "2022-05-15 20:22:00.246981 || Epoch #0, step 100 loss:1.7605764865875244\n",
            "2022-05-15 20:22:50.293280 || Epoch #0, step 200 loss:3.234666109085083\n",
            "2022-05-15 20:23:40.961491 || Epoch #0, step 300 loss:2.7892918586730957\n",
            "2022-05-15 20:24:31.323323 || Epoch #0, step 400 loss:2.853419303894043\n",
            "2022-05-15 20:25:21.500745 || Epoch #0, step 500 loss:2.669105291366577\n",
            "2022-05-15 20:26:12.799948 || Epoch #0, step 600 loss:2.8407390117645264\n",
            "2022-05-15 20:27:01.716198 || Epoch #0, step 700 loss:2.6398677825927734\n",
            "2022-05-15 20:27:51.644781 || Epoch #0, step 800 loss:2.829951047897339\n",
            "2022-05-15 20:28:42.507695 || Epoch #0, step 900 loss:2.0366458892822266\n",
            "2022-05-15 20:29:34.100300 || Epoch #0, step 1000 loss:2.992586374282837\n",
            "2022-05-15 20:30:25.026221 || Epoch #0, step 1100 loss:1.411232829093933\n",
            "2022-05-15 20:31:15.152608 || Epoch #0, step 1200 loss:2.896451473236084\n",
            "2022-05-15 20:32:05.539356 || Epoch #0, step 1300 loss:2.6894710063934326\n",
            "2022-05-15 20:32:56.755372 || Epoch #0, step 1400 loss:1.8805557489395142\n",
            "2022-05-15 20:33:48.264543 || Epoch #0, step 1500 loss:2.4056193828582764\n",
            "2022-05-15 20:34:38.671287 || Epoch #0, step 1600 loss:3.2682301998138428\n",
            "2022-05-15 20:35:29.997526 || Epoch #0, step 1700 loss:2.981163263320923\n",
            "2022-05-15 20:36:20.189602 || Epoch #0, step 1800 loss:3.7651331424713135\n",
            "2022-05-15 20:37:09.494449 || Epoch #0, step 1900 loss:2.1410298347473145\n",
            "2022-05-15 20:37:59.480163 || Epoch #0, step 2000 loss:2.4060328006744385\n",
            "2022-05-15 20:38:48.981983 || Epoch #0, step 2100 loss:3.4032609462738037\n",
            "2022-05-15 20:39:39.051698 || Epoch #0, step 2200 loss:2.3889641761779785\n",
            "2022-05-15 20:40:28.934626 || Epoch #0, step 2300 loss:3.3417000770568848\n",
            "2022-05-15 20:41:18.983398 || Epoch #0, step 2400 loss:2.771402359008789\n",
            "2022-05-15 20:42:09.933061 || Epoch #0, step 2500 loss:2.8594679832458496\n",
            "2022-05-15 20:42:59.626150 || Epoch #0, step 2600 loss:2.2566394805908203\n",
            "2022-05-15 20:43:49.438348 || Epoch #0, step 2700 loss:2.56042218208313\n",
            "2022-05-15 20:44:40.136770 || Epoch #0, step 2800 loss:2.0810916423797607\n",
            "2022-05-15 20:45:31.415301 || Epoch #0, step 2900 loss:3.1261911392211914\n",
            "2022-05-15 20:46:22.558934 || Epoch #0, step 3000 loss:2.7106964588165283\n",
            "2022-05-15 20:47:12.991698 || Epoch #0, step 3100 loss:2.6404612064361572\n",
            "2022-05-15 20:48:03.582662 || Epoch #0, step 3200 loss:2.622396230697632\n",
            "2022-05-15 20:48:53.784213 || Epoch #0, step 3300 loss:2.5599935054779053\n",
            "2022-05-15 20:49:43.644319 || Epoch #0, step 3400 loss:2.211743116378784\n",
            "2022-05-15 20:50:33.811031 || Epoch #0, step 3500 loss:2.780663251876831\n",
            "2022-05-15 20:51:23.057441 || Epoch #0, step 3600 loss:3.1372458934783936\n",
            "2022-05-15 20:52:12.444185 || Epoch #0, step 3700 loss:2.3050060272216797\n",
            "2022-05-15 20:53:02.133221 || Epoch #0, step 3800 loss:2.9364023208618164\n",
            "2022-05-15 20:53:52.383853 || Epoch #0, step 3900 loss:2.7245428562164307\n",
            "2022-05-15 20:54:42.821280 || Epoch #0, step 4000 loss:2.6240804195404053\n",
            "2022-05-15 20:55:32.458332 || Epoch #0, step 4100 loss:2.303863525390625\n",
            "2022-05-15 20:56:22.466601 || Epoch #0, step 4200 loss:2.3240742683410645\n",
            "2022-05-15 20:57:13.042857 || Epoch #0, step 4300 loss:2.8689699172973633\n",
            "2022-05-15 20:58:03.355010 || Epoch #0, step 4400 loss:2.7470924854278564\n",
            "2022-05-15 20:58:52.556526 || Epoch #0, step 4500 loss:2.7013251781463623\n",
            "2022-05-15 20:59:42.926552 || Epoch #0, step 4600 loss:2.6898086071014404\n",
            "2022-05-15 21:00:33.006729 || Epoch #0, step 4700 loss:2.906219244003296\n",
            "2022-05-15 21:01:23.859463 || Epoch #0, step 4800 loss:2.47469425201416\n",
            "2022-05-15 21:02:13.725178 || Epoch #0, step 4900 loss:2.784508466720581\n",
            "ner_bleu: model 0.74 | ner_rouge: model 0.24\n",
            "base_lib_rouge_list:   \n",
            "          rouge_1_f 0.33 \n",
            "          rouge_1_p 0.51 \n",
            "          rouge_1_r 0.26 \n",
            "\n",
            "          rouge_2_f 0.12 \n",
            "          rouge_2_p 0.20 \n",
            "          rouge_2_r 0.09 \n",
            "\n",
            "          rouge_l_f 0.30 \n",
            "          rouge_l_p 0.46 \n",
            "          rouge_l_r 0.24 \n",
            "        \n",
            "          \n",
            "### Scheduler step ### \n",
            "\n",
            "2022-05-15 21:06:05.581006 || Epoch #0, step 5000 loss:3.3557562828063965\n",
            "2022-05-15 21:06:56.052040 || Epoch #0, step 5100 loss:2.2541682720184326\n",
            "2022-05-15 21:07:45.383406 || Epoch #0, step 5200 loss:2.0923264026641846\n",
            "2022-05-15 21:08:35.459371 || Epoch #0, step 5300 loss:3.0332653522491455\n",
            "2022-05-15 21:09:25.835237 || Epoch #0, step 5400 loss:2.6410038471221924\n",
            "2022-05-15 21:10:15.561274 || Epoch #0, step 5500 loss:2.6416947841644287\n",
            "2022-05-15 21:11:05.991547 || Epoch #0, step 5600 loss:2.1983730792999268\n",
            "2022-05-15 21:11:55.533297 || Epoch #0, step 5700 loss:2.5257346630096436\n",
            "2022-05-15 21:12:45.301648 || Epoch #0, step 5800 loss:3.418902635574341\n",
            "2022-05-15 21:13:35.933045 || Epoch #0, step 5900 loss:2.0455451011657715\n",
            "2022-05-15 21:14:25.757930 || Epoch #0, step 6000 loss:2.3654353618621826\n",
            "2022-05-15 21:15:15.408267 || Epoch #0, step 6100 loss:2.851400136947632\n",
            "2022-05-15 21:16:05.213739 || Epoch #0, step 6200 loss:3.2162680625915527\n",
            "2022-05-15 21:16:55.146066 || Epoch #0, step 6300 loss:3.063601016998291\n",
            "2022-05-15 21:17:46.652616 || Epoch #0, step 6400 loss:3.225539207458496\n",
            "2022-05-15 21:18:37.588739 || Epoch #0, step 6500 loss:2.735419988632202\n",
            "2022-05-15 21:19:28.717780 || Epoch #0, step 6600 loss:2.310213088989258\n",
            "2022-05-15 21:20:19.915252 || Epoch #0, step 6700 loss:2.5730533599853516\n",
            "2022-05-15 21:21:09.240933 || Epoch #0, step 6800 loss:2.1986186504364014\n",
            "2022-05-15 21:21:59.444250 || Epoch #0, step 6900 loss:3.050215482711792\n",
            "2022-05-15 21:22:49.006936 || Epoch #0, step 7000 loss:2.9234983921051025\n",
            "2022-05-15 21:23:38.596513 || Epoch #0, step 7100 loss:3.2210915088653564\n",
            "2022-05-15 21:24:28.771630 || Epoch #0, step 7200 loss:1.834335446357727\n",
            "2022-05-15 21:25:19.120815 || Epoch #0, step 7300 loss:1.3975964784622192\n",
            "2022-05-15 21:26:10.030685 || Epoch #0, step 7400 loss:2.9037420749664307\n",
            "2022-05-15 21:27:00.452268 || Epoch #0, step 7500 loss:2.9370548725128174\n",
            "2022-05-15 21:27:51.380722 || Epoch #0, step 7600 loss:2.8724849224090576\n",
            "2022-05-15 21:28:40.933913 || Epoch #0, step 7700 loss:3.614408254623413\n",
            "2022-05-15 21:29:32.616197 || Epoch #0, step 7800 loss:2.6997058391571045\n",
            "2022-05-15 21:30:23.294497 || Epoch #0, step 7900 loss:2.2847700119018555\n",
            "2022-05-15 21:31:13.190302 || Epoch #0, step 8000 loss:2.401757001876831\n",
            "2022-05-15 21:32:03.414091 || Epoch #0, step 8100 loss:2.9026129245758057\n",
            "2022-05-15 21:32:53.570852 || Epoch #0, step 8200 loss:1.5439000129699707\n",
            "2022-05-15 21:33:43.395914 || Epoch #0, step 8300 loss:3.3133978843688965\n",
            "2022-05-15 21:34:33.470689 || Epoch #0, step 8400 loss:3.2964885234832764\n",
            "2022-05-15 21:35:22.712861 || Epoch #0, step 8500 loss:2.2247989177703857\n",
            "2022-05-15 21:36:13.238467 || Epoch #0, step 8600 loss:1.3502594232559204\n",
            "2022-05-15 21:37:03.520430 || Epoch #0, step 8700 loss:3.001958131790161\n",
            "2022-05-15 21:37:53.571420 || Epoch #0, step 8800 loss:2.760697364807129\n",
            "2022-05-15 21:38:44.164604 || Epoch #0, step 8900 loss:2.942606210708618\n",
            "2022-05-15 21:39:33.684116 || Epoch #0, step 9000 loss:3.7458672523498535\n",
            "2022-05-15 21:40:23.469745 || Epoch #0, step 9100 loss:3.194531202316284\n",
            "2022-05-15 21:41:13.982516 || Epoch #0, step 9200 loss:2.7763378620147705\n",
            "2022-05-15 21:42:05.588446 || Epoch #0, step 9300 loss:1.767061471939087\n",
            "2022-05-15 21:42:55.423512 || Epoch #0, step 9400 loss:2.79949688911438\n",
            "2022-05-15 21:43:46.456274 || Epoch #0, step 9500 loss:1.982978105545044\n",
            "2022-05-15 21:44:36.211037 || Epoch #0, step 9600 loss:2.3404672145843506\n",
            "2022-05-15 21:45:26.792580 || Epoch #0, step 9700 loss:2.513923168182373\n",
            "2022-05-15 21:46:17.628518 || Epoch #0, step 9800 loss:2.9191501140594482\n",
            "2022-05-15 21:47:07.543950 || Epoch #0, step 9900 loss:2.7250781059265137\n",
            "ner_bleu: model 0.70 | ner_rouge: model 0.26\n",
            "base_lib_rouge_list:   \n",
            "          rouge_1_f 0.34 \n",
            "          rouge_1_p 0.48 \n",
            "          rouge_1_r 0.28 \n",
            "\n",
            "          rouge_2_f 0.12 \n",
            "          rouge_2_p 0.18 \n",
            "          rouge_2_r 0.09 \n",
            "\n",
            "          rouge_l_f 0.29 \n",
            "          rouge_l_p 0.42 \n",
            "          rouge_l_r 0.24 \n",
            "        \n",
            "          \n",
            "### Scheduler step ### \n",
            "\n",
            "2022-05-15 21:51:21.537101 || Epoch #0, step 10000 loss:3.7945330142974854\n",
            "2022-05-15 21:52:11.808451 || Epoch #0, step 10100 loss:2.444460153579712\n",
            "2022-05-15 21:53:01.658160 || Epoch #0, step 10200 loss:2.475642204284668\n",
            "2022-05-15 21:53:52.387294 || Epoch #0, step 10300 loss:2.6420998573303223\n",
            "2022-05-15 21:54:42.433699 || Epoch #0, step 10400 loss:2.6840059757232666\n",
            "2022-05-15 21:55:32.037645 || Epoch #0, step 10500 loss:2.659290313720703\n",
            "2022-05-15 21:56:22.201076 || Epoch #0, step 10600 loss:2.94853138923645\n",
            "2022-05-15 21:57:11.933200 || Epoch #0, step 10700 loss:2.7862188816070557\n",
            "2022-05-15 21:58:02.406291 || Epoch #0, step 10800 loss:2.3473498821258545\n",
            "2022-05-15 21:58:51.786428 || Epoch #0, step 10900 loss:2.5205960273742676\n",
            "2022-05-15 21:59:41.296347 || Epoch #0, step 11000 loss:2.7099404335021973\n",
            "2022-05-15 22:00:30.407047 || Epoch #0, step 11100 loss:2.7499096393585205\n",
            "2022-05-15 22:01:20.135889 || Epoch #0, step 11200 loss:2.8092756271362305\n",
            "2022-05-15 22:02:10.114758 || Epoch #0, step 11300 loss:2.4610915184020996\n",
            "2022-05-15 22:03:00.155274 || Epoch #0, step 11400 loss:3.416438341140747\n",
            "2022-05-15 22:03:50.799969 || Epoch #0, step 11500 loss:2.689666986465454\n",
            "2022-05-15 22:04:40.307994 || Epoch #0, step 11600 loss:3.4061708450317383\n",
            "2022-05-15 22:05:30.597335 || Epoch #0, step 11700 loss:3.615993022918701\n",
            "2022-05-15 22:06:20.642966 || Epoch #0, step 11800 loss:3.5039989948272705\n",
            "2022-05-15 22:07:11.355071 || Epoch #0, step 11900 loss:2.246546506881714\n",
            "2022-05-15 22:08:00.796209 || Epoch #0, step 12000 loss:2.636960744857788\n",
            "2022-05-15 22:08:51.301530 || Epoch #0, step 12100 loss:2.6784896850585938\n",
            "2022-05-15 22:09:41.429242 || Epoch #0, step 12200 loss:2.3322348594665527\n",
            "2022-05-15 22:10:30.899854 || Epoch #0, step 12300 loss:2.2056775093078613\n",
            "2022-05-15 22:11:21.199914 || Epoch #0, step 12400 loss:2.838367223739624\n",
            "2022-05-15 22:12:10.856948 || Epoch #0, step 12500 loss:2.7978694438934326\n",
            "2022-05-15 22:13:00.533293 || Epoch #0, step 12600 loss:2.942086696624756\n",
            "2022-05-15 22:13:49.367787 || Epoch #0, step 12700 loss:3.3049426078796387\n",
            "2022-05-15 22:14:40.328231 || Epoch #0, step 12800 loss:1.688127875328064\n",
            "2022-05-15 22:15:30.826634 || Epoch #0, step 12900 loss:2.7000551223754883\n",
            "2022-05-15 22:16:20.601370 || Epoch #0, step 13000 loss:2.960172414779663\n",
            "2022-05-15 22:17:11.300499 || Epoch #0, step 13100 loss:2.901158571243286\n",
            "2022-05-15 22:18:00.958528 || Epoch #0, step 13200 loss:2.596740484237671\n",
            "2022-05-15 22:18:51.547143 || Epoch #0, step 13300 loss:2.4554214477539062\n",
            "2022-05-15 22:19:42.216683 || Epoch #0, step 13400 loss:3.1100080013275146\n",
            "2022-05-15 22:20:32.327750 || Epoch #0, step 13500 loss:2.3956985473632812\n",
            "2022-05-15 22:21:22.188423 || Epoch #0, step 13600 loss:2.760613203048706\n",
            "2022-05-15 22:22:12.250252 || Epoch #0, step 13700 loss:1.9191373586654663\n",
            "2022-05-15 22:23:01.638784 || Epoch #0, step 13800 loss:3.0460855960845947\n",
            "2022-05-15 22:23:51.434503 || Epoch #0, step 13900 loss:2.562203884124756\n",
            "2022-05-15 22:24:42.758291 || Epoch #0, step 14000 loss:1.724997878074646\n",
            "2022-05-15 22:25:32.695562 || Epoch #0, step 14100 loss:2.068680763244629\n",
            "2022-05-15 22:26:22.399582 || Epoch #0, step 14200 loss:2.7923686504364014\n",
            "2022-05-15 22:27:12.461285 || Epoch #0, step 14300 loss:3.2403950691223145\n",
            "2022-05-15 22:28:03.624247 || Epoch #0, step 14400 loss:3.391160011291504\n",
            "2022-05-15 22:28:54.540926 || Epoch #0, step 14500 loss:2.9529263973236084\n",
            "2022-05-15 22:29:45.364526 || Epoch #0, step 14600 loss:2.6121506690979004\n",
            "2022-05-15 22:30:35.290487 || Epoch #0, step 14700 loss:2.2792482376098633\n",
            "2022-05-15 22:31:24.664247 || Epoch #0, step 14800 loss:2.2819063663482666\n",
            "2022-05-15 22:32:16.091510 || Epoch #0, step 14900 loss:2.604318141937256\n",
            "ner_bleu: model 0.70 | ner_rouge: model 0.24\n",
            "base_lib_rouge_list:   \n",
            "          rouge_1_f 0.33 \n",
            "          rouge_1_p 0.49 \n",
            "          rouge_1_r 0.26 \n",
            "\n",
            "          rouge_2_f 0.11 \n",
            "          rouge_2_p 0.18 \n",
            "          rouge_2_r 0.08 \n",
            "\n",
            "          rouge_l_f 0.29 \n",
            "          rouge_l_p 0.43 \n",
            "          rouge_l_r 0.23 \n",
            "        \n",
            "          \n",
            "### Scheduler step ### \n",
            "\n",
            "2022-05-15 22:36:14.536578 || Epoch #0, step 15000 loss:2.8353164196014404\n",
            "2022-05-15 22:37:04.118360 || Epoch #0, step 15100 loss:2.8542659282684326\n",
            "2022-05-15 22:37:54.510241 || Epoch #0, step 15200 loss:2.1747286319732666\n",
            "2022-05-15 22:38:44.706809 || Epoch #0, step 15300 loss:3.0022242069244385\n",
            "2022-05-15 22:39:35.752957 || Epoch #0, step 15400 loss:3.217367172241211\n",
            "2022-05-15 22:40:25.279310 || Epoch #0, step 15500 loss:3.3132805824279785\n",
            "2022-05-15 22:41:14.793469 || Epoch #0, step 15600 loss:1.9049144983291626\n",
            "2022-05-15 22:42:05.344898 || Epoch #0, step 15700 loss:1.9025893211364746\n",
            "2022-05-15 22:42:55.478381 || Epoch #0, step 15800 loss:2.9625186920166016\n",
            "2022-05-15 22:43:45.705527 || Epoch #0, step 15900 loss:2.0877671241760254\n",
            "2022-05-15 22:44:35.490436 || Epoch #0, step 16000 loss:2.039844512939453\n",
            "2022-05-15 22:45:26.128818 || Epoch #0, step 16100 loss:3.0190281867980957\n",
            "2022-05-15 22:46:16.648795 || Epoch #0, step 16200 loss:3.0016369819641113\n",
            "2022-05-15 22:47:06.819056 || Epoch #0, step 16300 loss:1.883729100227356\n",
            "2022-05-15 22:47:56.572400 || Epoch #0, step 16400 loss:1.924147129058838\n",
            "2022-05-15 22:48:46.603582 || Epoch #0, step 16500 loss:2.3822238445281982\n",
            "2022-05-15 22:49:36.351533 || Epoch #0, step 16600 loss:2.039489507675171\n",
            "2022-05-15 22:50:26.664779 || Epoch #0, step 16700 loss:2.876772403717041\n",
            "2022-05-15 22:51:16.567810 || Epoch #0, step 16800 loss:1.5859676599502563\n",
            "2022-05-15 22:52:06.342440 || Epoch #0, step 16900 loss:3.480262041091919\n",
            "2022-05-15 22:52:57.424477 || Epoch #0, step 17000 loss:1.8497487306594849\n",
            "2022-05-15 22:53:46.685642 || Epoch #0, step 17100 loss:2.3679990768432617\n",
            "2022-05-15 22:54:37.106086 || Epoch #0, step 17200 loss:1.9148175716400146\n",
            "2022-05-15 22:55:27.660213 || Epoch #0, step 17300 loss:2.1087019443511963\n",
            "2022-05-15 22:56:17.267254 || Epoch #0, step 17400 loss:2.411747455596924\n",
            "2022-05-15 22:57:08.299114 || Epoch #0, step 17500 loss:2.0926871299743652\n",
            "2022-05-15 22:57:58.757155 || Epoch #0, step 17600 loss:2.4423232078552246\n",
            "2022-05-15 22:58:48.982021 || Epoch #0, step 17700 loss:0.9442949295043945\n",
            "2022-05-15 22:59:39.936843 || Epoch #0, step 17800 loss:3.2725629806518555\n",
            "2022-05-15 23:00:31.406304 || Epoch #0, step 17900 loss:2.6834189891815186\n",
            "2022-05-15 23:01:23.277055 || Epoch #0, step 18000 loss:2.883474826812744\n",
            "2022-05-15 23:02:12.906822 || Epoch #0, step 18100 loss:2.887195110321045\n",
            "2022-05-15 23:03:03.767368 || Epoch #0, step 18200 loss:2.863102912902832\n",
            "2022-05-15 23:03:53.698677 || Epoch #0, step 18300 loss:2.322593927383423\n",
            "2022-05-15 23:04:43.935176 || Epoch #0, step 18400 loss:2.6205289363861084\n",
            "2022-05-15 23:05:34.207769 || Epoch #0, step 18500 loss:3.0104196071624756\n",
            "2022-05-15 23:06:23.744554 || Epoch #0, step 18600 loss:2.4196226596832275\n",
            "2022-05-15 23:07:14.069272 || Epoch #0, step 18700 loss:2.133464813232422\n",
            "2022-05-15 23:08:04.921347 || Epoch #0, step 18800 loss:2.29199481010437\n",
            "2022-05-15 23:08:54.463587 || Epoch #0, step 18900 loss:2.4855916500091553\n",
            "2022-05-15 23:09:44.536141 || Epoch #0, step 19000 loss:1.6560689210891724\n",
            "2022-05-15 23:10:35.281036 || Epoch #0, step 19100 loss:2.694612979888916\n",
            "2022-05-15 23:11:24.882223 || Epoch #0, step 19200 loss:2.5160293579101562\n",
            "2022-05-15 23:12:14.506236 || Epoch #0, step 19300 loss:2.1814615726470947\n",
            "2022-05-15 23:13:05.204932 || Epoch #0, step 19400 loss:2.8423211574554443\n",
            "2022-05-15 23:13:54.522370 || Epoch #0, step 19500 loss:3.046273708343506\n",
            "2022-05-15 23:14:46.040914 || Epoch #0, step 19600 loss:2.9777326583862305\n",
            "2022-05-15 23:15:35.565072 || Epoch #0, step 19700 loss:2.444035053253174\n",
            "2022-05-15 23:16:25.648651 || Epoch #0, step 19800 loss:2.287808418273926\n",
            "2022-05-15 23:17:15.908719 || Epoch #0, step 19900 loss:3.084839105606079\n",
            "ner_bleu: model 0.73 | ner_rouge: model 0.24\n",
            "base_lib_rouge_list:   \n",
            "          rouge_1_f 0.33 \n",
            "          rouge_1_p 0.50 \n",
            "          rouge_1_r 0.26 \n",
            "\n",
            "          rouge_2_f 0.12 \n",
            "          rouge_2_p 0.19 \n",
            "          rouge_2_r 0.09 \n",
            "\n",
            "          rouge_l_f 0.29 \n",
            "          rouge_l_p 0.44 \n",
            "          rouge_l_r 0.23 \n",
            "        \n",
            "          \n",
            "### Scheduler step ### \n",
            "\n",
            "2022-05-15 23:21:19.433804 || Epoch #0, step 20000 loss:2.7491276264190674\n",
            "2022-05-15 23:22:10.598346 || Epoch #0, step 20100 loss:2.79487681388855\n",
            "2022-05-15 23:23:00.905232 || Epoch #0, step 20200 loss:2.6073734760284424\n",
            "2022-05-15 23:23:51.539454 || Epoch #0, step 20300 loss:2.180109739303589\n",
            "2022-05-15 23:24:42.266288 || Epoch #0, step 20400 loss:1.5810776948928833\n",
            "2022-05-15 23:25:31.986915 || Epoch #0, step 20500 loss:2.299135684967041\n",
            "2022-05-15 23:26:22.681856 || Epoch #0, step 20600 loss:2.298391580581665\n",
            "2022-05-15 23:27:12.029477 || Epoch #0, step 20700 loss:3.2321088314056396\n",
            "2022-05-15 23:28:01.470073 || Epoch #0, step 20800 loss:2.4340767860412598\n",
            "2022-05-15 23:28:51.385303 || Epoch #0, step 20900 loss:1.9866405725479126\n",
            "2022-05-15 23:29:40.936984 || Epoch #0, step 21000 loss:4.4459733963012695\n",
            "2022-05-15 23:30:30.656061 || Epoch #0, step 21100 loss:2.0010316371917725\n",
            "2022-05-15 23:31:19.927695 || Epoch #0, step 21200 loss:2.544414520263672\n",
            "2022-05-15 23:32:09.164116 || Epoch #0, step 21300 loss:2.470233678817749\n",
            "2022-05-15 23:32:59.301251 || Epoch #0, step 21400 loss:2.6361422538757324\n",
            "2022-05-15 23:33:49.290143 || Epoch #0, step 21500 loss:2.563537359237671\n",
            "2022-05-15 23:34:39.532287 || Epoch #0, step 21600 loss:2.1492178440093994\n",
            "2022-05-15 23:35:29.793469 || Epoch #0, step 21700 loss:2.3393492698669434\n",
            "2022-05-15 23:36:20.025945 || Epoch #0, step 21800 loss:2.4659926891326904\n",
            "2022-05-15 23:37:09.614798 || Epoch #0, step 21900 loss:2.2728705406188965\n",
            "2022-05-15 23:37:59.610543 || Epoch #0, step 22000 loss:2.138552665710449\n",
            "2022-05-15 23:38:49.455744 || Epoch #0, step 22100 loss:1.7714670896530151\n",
            "2022-05-15 23:39:39.889870 || Epoch #0, step 22200 loss:1.974455714225769\n",
            "2022-05-15 23:40:29.849638 || Epoch #0, step 22300 loss:2.7294583320617676\n",
            "2022-05-15 23:41:20.075694 || Epoch #0, step 22400 loss:3.2197577953338623\n",
            "2022-05-15 23:42:10.447230 || Epoch #0, step 22500 loss:2.7552239894866943\n",
            "2022-05-15 23:43:00.444737 || Epoch #0, step 22600 loss:2.2840585708618164\n",
            "2022-05-15 23:43:50.321462 || Epoch #0, step 22700 loss:2.8426895141601562\n",
            "2022-05-15 23:44:40.674741 || Epoch #0, step 22800 loss:2.405790328979492\n",
            "2022-05-15 23:45:31.177664 || Epoch #0, step 22900 loss:1.5911818742752075\n",
            "2022-05-15 23:46:21.186452 || Epoch #0, step 23000 loss:2.710019111633301\n",
            "2022-05-15 23:47:11.417742 || Epoch #0, step 23100 loss:2.56498646736145\n",
            "2022-05-15 23:48:01.694288 || Epoch #0, step 23200 loss:2.051265239715576\n",
            "2022-05-15 23:48:51.423435 || Epoch #0, step 23300 loss:2.847870111465454\n",
            "2022-05-15 23:49:42.019951 || Epoch #0, step 23400 loss:2.0772907733917236\n",
            "2022-05-15 23:50:32.258379 || Epoch #0, step 23500 loss:3.0213589668273926\n",
            "2022-05-15 23:51:21.341978 || Epoch #0, step 23600 loss:2.4523816108703613\n",
            "2022-05-15 23:52:11.213658 || Epoch #0, step 23700 loss:2.0693106651306152\n",
            "2022-05-15 23:53:01.518040 || Epoch #0, step 23800 loss:2.3132128715515137\n",
            "2022-05-15 23:53:51.728495 || Epoch #0, step 23900 loss:3.256476402282715\n",
            "2022-05-15 23:54:42.379513 || Epoch #0, step 24000 loss:2.842175006866455\n",
            "2022-05-15 23:55:33.698585 || Epoch #0, step 24100 loss:2.6626195907592773\n",
            "2022-05-15 23:56:24.757229 || Epoch #0, step 24200 loss:1.499380350112915\n",
            "2022-05-15 23:57:14.425458 || Epoch #0, step 24300 loss:2.8901185989379883\n",
            "2022-05-15 23:58:04.286613 || Epoch #0, step 24400 loss:2.31280255317688\n",
            "2022-05-15 23:58:55.083489 || Epoch #0, step 24500 loss:2.258242130279541\n",
            "2022-05-15 23:59:45.506086 || Epoch #0, step 24600 loss:2.8527557849884033\n",
            "2022-05-16 00:00:34.346829 || Epoch #0, step 24700 loss:3.666489601135254\n",
            "2022-05-16 00:01:25.673862 || Epoch #0, step 24800 loss:3.0520737171173096\n",
            "2022-05-16 00:02:15.893654 || Epoch #0, step 24900 loss:2.2482805252075195\n",
            "ner_bleu: model 0.75 | ner_rouge: model 0.26\n",
            "base_lib_rouge_list:   \n",
            "          rouge_1_f 0.35 \n",
            "          rouge_1_p 0.49 \n",
            "          rouge_1_r 0.29 \n",
            "\n",
            "          rouge_2_f 0.12 \n",
            "          rouge_2_p 0.19 \n",
            "          rouge_2_r 0.10 \n",
            "\n",
            "          rouge_l_f 0.31 \n",
            "          rouge_l_p 0.43 \n",
            "          rouge_l_r 0.25 \n",
            "        \n",
            "          \n",
            "### Scheduler step ### \n",
            "\n",
            "2022-05-16 00:06:28.205457 || Epoch #0, step 25000 loss:2.2665300369262695\n",
            "2022-05-16 00:07:17.936349 || Epoch #0, step 25100 loss:3.0729925632476807\n",
            "2022-05-16 00:08:07.949443 || Epoch #0, step 25200 loss:1.7181693315505981\n",
            "2022-05-16 00:08:57.976287 || Epoch #0, step 25300 loss:1.7208585739135742\n",
            "2022-05-16 00:09:48.846393 || Epoch #0, step 25400 loss:2.368121862411499\n",
            "2022-05-16 00:10:40.091266 || Epoch #0, step 25500 loss:1.691264033317566\n",
            "2022-05-16 00:11:29.907380 || Epoch #0, step 25600 loss:2.747795820236206\n",
            "2022-05-16 00:12:20.513313 || Epoch #0, step 25700 loss:2.336097240447998\n",
            "2022-05-16 00:13:11.158815 || Epoch #0, step 25800 loss:1.379562497138977\n",
            "2022-05-16 00:14:01.660770 || Epoch #0, step 25900 loss:2.127777576446533\n",
            "2022-05-16 00:14:51.876279 || Epoch #0, step 26000 loss:3.194537401199341\n",
            "2022-05-16 00:15:42.382324 || Epoch #0, step 26100 loss:2.1377344131469727\n",
            "2022-05-16 00:16:33.250474 || Epoch #0, step 26200 loss:2.355227470397949\n",
            "2022-05-16 00:17:22.870661 || Epoch #0, step 26300 loss:2.350957155227661\n",
            "2022-05-16 00:18:13.433165 || Epoch #0, step 26400 loss:2.7681543827056885\n",
            "2022-05-16 00:19:04.014894 || Epoch #0, step 26500 loss:1.6005079746246338\n",
            "2022-05-16 00:19:54.029900 || Epoch #0, step 26600 loss:2.088564395904541\n",
            "2022-05-16 00:20:44.812116 || Epoch #0, step 26700 loss:2.9082539081573486\n",
            "2022-05-16 00:21:35.724816 || Epoch #0, step 26800 loss:2.214982032775879\n",
            "2022-05-16 00:22:25.526157 || Epoch #0, step 26900 loss:1.265951156616211\n",
            "2022-05-16 00:23:13.841055 || Epoch #0, step 27000 loss:2.7962591648101807\n",
            "2022-05-16 00:24:04.353025 || Epoch #0, step 27100 loss:3.433891534805298\n",
            "2022-05-16 00:24:54.349593 || Epoch #0, step 27200 loss:2.5653693675994873\n",
            "2022-05-16 00:25:44.079453 || Epoch #0, step 27300 loss:2.1146531105041504\n",
            "2022-05-16 00:26:33.085917 || Epoch #0, step 27400 loss:1.686204433441162\n",
            "2022-05-16 00:27:22.592572 || Epoch #0, step 27500 loss:2.3153326511383057\n",
            "2022-05-16 00:28:12.298945 || Epoch #0, step 27600 loss:2.2744503021240234\n",
            "2022-05-16 00:29:02.372198 || Epoch #0, step 27700 loss:2.164395570755005\n",
            "2022-05-16 00:29:51.522961 || Epoch #0, step 27800 loss:2.507533073425293\n",
            "2022-05-16 00:30:42.267325 || Epoch #0, step 27900 loss:2.3317840099334717\n",
            "2022-05-16 00:31:32.813947 || Epoch #0, step 28000 loss:1.9460062980651855\n",
            "2022-05-16 00:32:23.246793 || Epoch #0, step 28100 loss:2.074575662612915\n",
            "2022-05-16 00:33:14.220153 || Epoch #0, step 28200 loss:3.061272144317627\n",
            "2022-05-16 00:34:04.499448 || Epoch #0, step 28300 loss:1.8385108709335327\n",
            "2022-05-16 00:34:54.270716 || Epoch #0, step 28400 loss:2.429555654525757\n",
            "2022-05-16 00:35:45.347189 || Epoch #0, step 28500 loss:2.1182644367218018\n",
            "2022-05-16 00:36:35.447971 || Epoch #0, step 28600 loss:3.2789981365203857\n",
            "2022-05-16 00:37:26.336722 || Epoch #0, step 28700 loss:2.0084784030914307\n",
            "2022-05-16 00:38:17.062470 || Epoch #0, step 28800 loss:2.9869728088378906\n",
            "2022-05-16 00:39:07.067397 || Epoch #0, step 28900 loss:1.9981802701950073\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-11a4bacf7c86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{datetime.now()} || Epoch #{epoch}, step {len(loss_history) - (summarization_dataset.__len__() * epoch)} loss:{float(loss)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m           \u001b[0;31m#eval_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0f6226cd4c0d>\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#keep all the file in the ram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf_zh3UbYH9I",
        "outputId": "d434f97e-f0af-442f-fe53-45db295265c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "summarization_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.eval()\n",
        "print(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8goibOkQS2sV",
        "outputId": "701a794c-9d05-4d3c-91da-0a5c6017fd55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aehVIyKXnIkk"
      },
      "source": [
        "# Custom metric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_model = BartForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/Diplom/Summarization/save_9\")"
      ],
      "metadata": {
        "id": "OZCfO11hCDgp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LB3VVo1yYIIo"
      },
      "outputs": [],
      "source": [
        "base_model = summarization_model#BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "jg4mvR9UB3Ui"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_model.to(device)\n",
        "base_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNIUHSBZCGvU",
        "outputId": "967e0f86-3591-43ef-8032-8a0062e5729f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_model.eval()\n",
        "base_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mskCUnleC5Dz",
        "outputId": "3c96e0e1-d1fd-4038-ecee-f2fe484e5212"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "L1RyqlPWYILb"
      },
      "outputs": [],
      "source": [
        "#считать кол-во сущностей в исходном тексте (уникальных?) и их долю в саммари. это bleu cliped precision/ ROUGE - доля сущностей в саммари, которые ест ьв тексте"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"/content/drive/MyDrive/Diplom/NER/outputs/checkpoint-1372-epoch-7\")\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=ner_tokenizer, device=-1) #!!!!!!!!!! 0 ggpu"
      ],
      "metadata": {
        "id": "B7sbZWMx1pnS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_rouge_metric(target_entities, generated_text):\n",
        "  #считать кол-во сущностей в исходном тексте (уникальных?) и сколько из них вошло в саммари: ents_summary / ents_text это bleu cliped precision\n",
        "  ner_results = nlp(generated_text)#\n",
        "  predicted_entities = set()\n",
        "\n",
        "  for ent in ner_results[0]:\n",
        "    if ent['entity'] != 'LABEL_0' and len(ent['word']) > 2:\n",
        "      predicted_entities.add(ent[\"word\"].replace(\"Ġ\", ''))\n",
        "  \n",
        "  correct_ents = 0\n",
        "  for ent in predicted_entities:\n",
        "    if ent in target_entities:\n",
        "      correct_ents += 1\n",
        "  \n",
        "  if len(predicted_entities) == 0 and len(target_entities) == 0:\n",
        "      return 1\n",
        "  else:\n",
        "    return correct_ents / len(target_entities)\n",
        "  "
      ],
      "metadata": {
        "id": "plqlgJoJbSi7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_bleu_metric(target_entities, generated_text):\n",
        "  #ROUGE - доля сущностей в саммари, которые ест ьв тексте rouge cliped recall\n",
        "  ner_results = nlp(generated_text) #\n",
        "  predicted_entities = set()\n",
        "\n",
        "  for ent in ner_results[0]:\n",
        "    if ent['entity'] != 'LABEL_0' and len(ent['word']) > 2:\n",
        "      predicted_entities.add(ent[\"word\"].replace(\"Ġ\", ''))\n",
        "  \n",
        "  correct_ents = 0\n",
        "  for ent in predicted_entities:\n",
        "    if ent in target_entities:\n",
        "      correct_ents += 1\n",
        "  \n",
        "  if len(predicted_entities) == 0 and len(target_entities) == 0:\n",
        "    return 1\n",
        "  if len(predicted_entities) == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return correct_ents / len(predicted_entities)"
      ],
      "metadata": {
        "id": "XsAsAVEOjfhY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460"
      ],
      "metadata": {
        "id": "sed2fUlGdVkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYCumktNlNdQ",
        "outputId": "c2786e2b-e81e-441a-e057-e5cf33fbb5c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge"
      ],
      "metadata": {
        "id": "FDCoF3Anc2xr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_lib = Rouge()"
      ],
      "metadata": {
        "id": "euEUSG0IdHbI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_rouge_metric([\"quantum\", \"star\"], [\"the research about quantum was calculated by score\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1glFk-cAsBtE",
        "outputId": "b13150fb-232e-43ed-eda0-c3c8337a5acd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_bleu_metric([\"quantum\", \"star\"], [\"the research about quantum was calculated by score\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x24Rh3PHsw_q",
        "outputId": "78df31b1-b55f-41a5-9fc0-e62c005d5f96"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_lib.get_scores(\"the research about quantum was calculated by score\", \"quantum star\")[0] #пресижн по ВСЕМ словам а не по только сущностям."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArXaqxuzc-8y",
        "outputId": "0e26781a-0a29-4af1-8db9-1e314da02953"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'f': 0.19999999680000002, 'p': 0.125, 'r': 0.5},\n",
              " 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
              " 'rouge-l': {'f': 0.19999999680000002, 'p': 0.125, 'r': 0.5}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Arxiv_summarization_test_dataset(Dataset): #maximum sequence length for this model (18038 > 1024)!\n",
        "    #articles might be too long and geting truncated?\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        self.tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\") #переименовать чтоб не путать?\n",
        "\n",
        "        ner_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "        model = AutoModelForTokenClassification.from_pretrained(\"/content/drive/MyDrive/Diplom/NER/outputs/checkpoint-1372-epoch-7\")\n",
        "        self.nlp = nlp #!!!!!!!\n",
        "\n",
        "    def __len__(self): #keep all the file in the ram\n",
        "      with open(self.filename) as f:\n",
        "            for i, _ in enumerate(f, 1):\n",
        "                pass\n",
        "      return i\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx): #Add exctractive summary prior. generate dataset separetely? TRY EXCEPT HERE\n",
        "      try:\n",
        "        item = {}\n",
        "        data = json.loads(getline(self.filename, idx))\n",
        "        article_text = data['article_text']\n",
        "        article_text = ' '.join(article_text)\n",
        "\n",
        "        abstract_text = ' '.join(data['abstract_text']).replace(\"<S>\", '').replace(\"</S>\", '')\n",
        "\n",
        "        tokenized = self.tokenizer(article_text, truncation=True, padding=True)\n",
        "        item['input_ids'], item['attention_mask'] = torch.tensor(tokenized['input_ids']), torch.tensor(tokenized['attention_mask'])\n",
        "        item['labels'] = torch.tensor(self.tokenizer(abstract_text, truncation=True, padding=True)['input_ids'])\n",
        "        \n",
        "        item['abstract_text'] = abstract_text\n",
        "        #item['article_text'] = article_text\n",
        "\n",
        "        ner_results = self.nlp(article_text) #Use here article or from target summary? try both abstract_text article_text\n",
        "        entities = list()\n",
        "        for ent in ner_results:\n",
        "          if ent['entity'] != 'LABEL_0' and len(ent['word']) > 2:\n",
        "            entities.append(ent[\"word\"].replace(\"Ġ\", ''))\n",
        "\n",
        "        item[\"entities\"] = list(set(entities)) #pythorch не поддерживает set\n",
        "      except:\n",
        "        #print(f\"Error on line {idx}\")\n",
        "        idx += 1\n",
        "        self.__getitem__(idx)\n",
        "\n",
        "      return item"
      ],
      "metadata": {
        "id": "HHTGxC-FfgEs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_test_dataset = Arxiv_summarization_test_dataset(\"/content/arxiv-dataset/test.txt\") "
      ],
      "metadata": {
        "id": "ftwp7QIjnsUC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(summarization_test_dataset, batch_size=1, shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "63jKvR2OnzM3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(summarization_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9IoMExX8pIa",
        "outputId": "0f8320d4-4ddf-43c0-9f1c-e81364f8b7ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6440"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPNvhWNCYIC6",
        "outputId": "3ff333e8-e954-4614-c9e7-edf583c7c5ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ner_bleu: model 0.96 | base 0.80 || ner_rouge: model 0.57 | base 0.34\n",
            "ner_bleu: model 0.50 | base 0.57 || ner_rouge: model 0.19 | base 0.17\n",
            "ner_bleu: model 1.00 | base 1.00 || ner_rouge: model 0.22 | base 0.23\n",
            "ner_bleu: model 0.94 | base 0.83 || ner_rouge: model 0.57 | base 0.40\n",
            "ner_bleu: model 0.67 | base 0.32 || ner_rouge: model 0.10 | base 0.10\n",
            "ner_bleu: model 0.88 | base 0.85 || ner_rouge: model 0.60 | base 0.60\n",
            "ner_bleu: model 0.68 | base 1.00 || ner_rouge: model 0.31 | base 0.36\n",
            "ner_bleu: model 1.00 | base 0.81 || ner_rouge: model 0.27 | base 0.39\n",
            "ner_bleu: model 0.70 | base 0.85 || ner_rouge: model 0.39 | base 0.49\n",
            "ner_bleu: model 0.48 | base 0.72 || ner_rouge: model 0.12 | base 0.26\n",
            "ner_bleu: model 1.00 | base 1.00 || ner_rouge: model 0.24 | base 0.21\n",
            "ner_bleu: model 0.45 | base 0.34 || ner_rouge: model 0.09 | base 0.19\n",
            "ner_bleu: model 0.81 | base 0.80 || ner_rouge: model 0.45 | base 0.36\n",
            "ner_bleu: model 0.58 | base 0.63 || ner_rouge: model 0.18 | base 0.19\n",
            "ner_bleu: model 1.00 | base 1.00 || ner_rouge: model 0.24 | base 0.35\n",
            "ner_bleu: model 0.86 | base 0.83 || ner_rouge: model 0.26 | base 0.26\n",
            "ner_bleu: model 0.89 | base 0.91 || ner_rouge: model 0.49 | base 0.38\n",
            "ner_bleu: model 0.81 | base 0.55 || ner_rouge: model 0.20 | base 0.18\n",
            "ner_bleu: model 0.54 | base 0.57 || ner_rouge: model 0.31 | base 0.24\n",
            "ner_bleu: model 1.00 | base 1.00 || ner_rouge: model 0.21 | base 0.17\n",
            "ner_bleu: model 0.83 | base 0.96 || ner_rouge: model 0.43 | base 0.43\n",
            "ner_bleu: model 1.00 | base 0.87 || ner_rouge: model 0.28 | base 0.30\n",
            "ner_bleu: model 0.85 | base 1.00 || ner_rouge: model 0.48 | base 0.30\n",
            "ner_bleu: model 0.59 | base 0.86 || ner_rouge: model 0.28 | base 0.17\n",
            "ner_bleu: model 0.90 | base 0.91 || ner_rouge: model 0.24 | base 0.26\n",
            "ner_bleu: model 0.76 | base 0.93 || ner_rouge: model 0.37 | base 0.46\n",
            "ner_bleu: model 0.76 | base 0.94 || ner_rouge: model 0.23 | base 0.23\n",
            "ner_bleu: model 0.62 | base 0.69 || ner_rouge: model 0.21 | base 0.13\n",
            "ner_bleu: model 0.92 | base 0.83 || ner_rouge: model 0.29 | base 0.26\n",
            "ner_bleu: model 0.79 | base 0.75 || ner_rouge: model 0.38 | base 0.38\n",
            "ner_bleu: model 0.60 | base 0.91 || ner_rouge: model 0.22 | base 0.42\n",
            "ner_bleu: model 0.93 | base 0.55 || ner_rouge: model 0.33 | base 0.25\n",
            "ner_bleu: model 0.61 | base 0.59 || ner_rouge: model 0.19 | base 0.18\n",
            "ner_bleu: model 1.00 | base 0.82 || ner_rouge: model 0.43 | base 0.23\n",
            "ner_bleu: model 0.59 | base 0.93 || ner_rouge: model 0.27 | base 0.21\n",
            "ner_bleu: model 0.89 | base 0.83 || ner_rouge: model 0.14 | base 0.26\n",
            "ner_bleu: model 0.38 | base 0.52 || ner_rouge: model 0.18 | base 0.20\n",
            "ner_bleu: model 0.85 | base 1.00 || ner_rouge: model 0.16 | base 0.16\n",
            "ner_bleu: model 0.57 | base 0.55 || ner_rouge: model 0.20 | base 0.20\n",
            "ner_bleu: model 0.59 | base 0.71 || ner_rouge: model 0.22 | base 0.24\n",
            "ner_bleu: model 0.55 | base 0.50 || ner_rouge: model 0.20 | base 0.20\n",
            "ner_bleu: model 0.63 | base 0.70 || ner_rouge: model 0.26 | base 0.32\n",
            "ner_bleu: model 0.96 | base 0.62 || ner_rouge: model 0.51 | base 0.29\n",
            "ner_bleu: model 1.00 | base 0.54 || ner_rouge: model 0.17 | base 0.21\n",
            "ner_bleu: model 0.73 | base 0.82 || ner_rouge: model 0.28 | base 0.42\n",
            "ner_bleu: model 0.47 | base 0.45 || ner_rouge: model 0.24 | base 0.19\n",
            "ner_bleu: model 0.87 | base 0.81 || ner_rouge: model 0.30 | base 0.20\n",
            "ner_bleu: model 0.73 | base 0.81 || ner_rouge: model 0.19 | base 0.33\n",
            "ner_bleu: model 0.67 | base 0.54 || ner_rouge: model 0.22 | base 0.19\n",
            "ner_bleu: model 0.89 | base 0.57 || ner_rouge: model 0.45 | base 0.15\n",
            "ner_bleu: model 0.40 | base 0.40 || ner_rouge: model 0.04 | base 0.04\n",
            "ner_bleu: model 0.44 | base 0.50 || ner_rouge: model 0.18 | base 0.20\n",
            "ner_bleu: model 0.64 | base 0.59 || ner_rouge: model 0.19 | base 0.18\n",
            "ner_bleu: model 0.83 | base 0.85 || ner_rouge: model 0.15 | base 0.43\n",
            "ner_bleu: model 0.46 | base 0.47 || ner_rouge: model 0.36 | base 0.28\n",
            "ner_bleu: model 0.94 | base 1.00 || ner_rouge: model 0.22 | base 0.22\n",
            "ner_bleu: model 0.39 | base 1.00 || ner_rouge: model 0.17 | base 0.17\n",
            "ner_bleu: model 0.88 | base 0.89 || ner_rouge: model 0.37 | base 0.41\n",
            "ner_bleu: model 0.70 | base 0.80 || ner_rouge: model 0.31 | base 0.18\n",
            "ner_bleu: model 0.50 | base 0.43 || ner_rouge: model 0.07 | base 0.07\n",
            "ner_bleu: model 0.65 | base 0.89 || ner_rouge: model 0.20 | base 0.20\n",
            "ner_bleu: model 0.86 | base 0.89 || ner_rouge: model 0.33 | base 0.31\n",
            "ner_bleu: model 0.97 | base 0.86 || ner_rouge: model 0.35 | base 0.36\n",
            "ner_bleu: model 0.57 | base 0.72 || ner_rouge: model 0.13 | base 0.33\n",
            "ner_bleu: model 0.94 | base 0.91 || ner_rouge: model 0.49 | base 0.46\n",
            "ner_bleu: model 0.30 | base 0.92 || ner_rouge: model 0.08 | base 0.16\n",
            "ner_bleu: model 0.80 | base 0.80 || ner_rouge: model 0.16 | base 0.16\n"
          ]
        }
      ],
      "source": [
        "res = []\n",
        "baseline_res = []\n",
        "\n",
        "lib_rouge_list_NER = []\n",
        "base_lib_rouge_list_NER = []\n",
        "\n",
        "lib_rouge_list = []\n",
        "base_lib_rouge_list = []\n",
        "\n",
        "bleu_list = []\n",
        "base_bleu_list = []\n",
        "rouge_list = []\n",
        "base_rouge_list = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "  try:\n",
        "    entities = [i[0] for i in batch['entities']] #because fuckind dataloader\n",
        "    #print(type(batch['entities'][0][0]))\n",
        "    #print(entities)\n",
        "    \n",
        "    summarization_model.to('cuda')\n",
        "    base_model.to('cuda')\n",
        "\n",
        "    generated_tokens = summarization_model.generate(batch['input_ids'].to(device), max_length=150)\n",
        "    #generated_tokens = generated_tokens.cpu().numpy()\n",
        "    base_generated_tokens = base_model.generate(batch['input_ids'].to(device), max_length=150)\n",
        "    #base_generated_tokens = base_generated_tokens.cpu().numpy()\n",
        "\n",
        "    decoded_preds = summarization_test_dataset.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "    base_decoded_preds = summarization_test_dataset.tokenizer.batch_decode( base_generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    bleu = ner_bleu_metric(entities, decoded_preds)\n",
        "    bleu_list.append(bleu)  \n",
        "    base_bleu = ner_bleu_metric(entities, base_decoded_preds)\n",
        "    base_bleu_list.append(base_bleu) \n",
        "\n",
        "    rouge = ner_rouge_metric(entities, decoded_preds)\n",
        "    rouge_list.append(rouge)\n",
        "    base_rouge = ner_rouge_metric(entities, base_decoded_preds)\n",
        "    base_rouge_list.append(base_rouge)\n",
        "\n",
        "    print(f\"ner_bleu: model {bleu:.2f} | base {base_bleu:.2f} || ner_rouge: model {rouge:.2f} | base {base_rouge:.2f}\")\n",
        "\n",
        "    lib_rouge_list_NER.append(rouge_lib.get_scores(decoded_preds[0], ' '.join(entities))) #rouge.get_scores(hypothesis, reference)\n",
        "    base_lib_rouge_list_NER.append(rouge_lib.get_scores(base_decoded_preds[0], ' '.join(entities))) #rouge.get_scores(hypothesis, reference)\n",
        "\n",
        "    lib_rouge_list.append(rouge_lib.get_scores(decoded_preds[0], batch['abstract_text'][0])) #rouge.get_scores(hypothesis, reference)\n",
        "    base_lib_rouge_list.append(rouge_lib.get_scores(base_decoded_preds[0], batch['abstract_text'][0])) #rouge.get_scores(hypothesis, reference)\n",
        "\n",
        "\n",
        "    res.append(decoded_preds)\n",
        "    baseline_res.append(base_decoded_preds)\n",
        "\n",
        "    if len(res) == 100:\n",
        "     break\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    pass\n",
        "  \n",
        "print(\"******************************TOTAL*************************************\") \n",
        "print(f\"ner_bleu: model {statistics.mean(bleu_list):.2f} | base {statistics.mean(base_bleu_list):.2f} || ner_rouge: model {statistics.mean(rouge_list):.2f} | base {statistics.mean(base_rouge_list):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(base_lib_rouge_list_NER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB0ADVU5q_OY",
        "outputId": "d82228ad-ff52-46af-a83b-bf3f53f6413a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6438"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"******************************TOTAL*************************************\") \n",
        "print(f\"ner_bleu: model {statistics.mean(bleu_list):.2f} | base {statistics.mean(base_bleu_list):.2f} || ner_rouge: model {statistics.mean(rouge_list):.2f} | base {statistics.mean(base_rouge_list):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amTw0QCg8k-O",
        "outputId": "aa0ff938-39f3-416a-a2be-b559a6220c59"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******************************TOTAL*************************************\n",
            "ner_bleu: model 0.93 | base 0.86 || ner_rouge: model 0.39 | base 0.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lib_rouge_list_NER \n",
        "base_lib_rouge_list_NER\n",
        "\n",
        "lib_rouge_list\n",
        "base_lib_rouge_list"
      ],
      "metadata": {
        "id": "AwrjFKbxlsMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_1_f = []\n",
        "rouge_1_p = []\n",
        "rouge_1_r = []\n",
        "\n",
        "rouge_2_f = []\n",
        "rouge_2_p = []\n",
        "rouge_2_r = []\n",
        "\n",
        "rouge_l_f = []\n",
        "rouge_l_p = []\n",
        "rouge_l_r = []\n",
        "\n",
        "for measurement in base_lib_rouge_list:\n",
        "  rouge_1_f.append(measurement[0]['rouge-1']['f'])\n",
        "  rouge_1_p.append(measurement[0]['rouge-1']['p'])\n",
        "  rouge_1_r.append(measurement[0]['rouge-1']['r'])\n",
        "\n",
        "  rouge_2_f.append(measurement[0]['rouge-2']['f'])\n",
        "  rouge_2_p.append(measurement[0]['rouge-2']['p'])\n",
        "  rouge_2_r.append(measurement[0]['rouge-2']['r'])\n",
        "\n",
        "  rouge_l_f.append(measurement[0]['rouge-l']['f'])\n",
        "  rouge_l_p.append(measurement[0]['rouge-l']['p'])\n",
        "  rouge_l_r.append(measurement[0]['rouge-l']['r'])\n",
        "\n",
        "print(f\"\"\"base_lib_rouge_list:   \n",
        "        rouge_1_f {statistics.mean(rouge_1_f):.2f} \n",
        "        rouge_1_p {statistics.mean(rouge_1_p):.2f} \n",
        "        rouge_1_r {statistics.mean(rouge_1_r):.2f} \n",
        "        rouge_2_f {statistics.mean(rouge_2_f):.2f} \n",
        "        rouge_2_p {statistics.mean(rouge_2_p):.2f} \n",
        "        rouge_2_r {statistics.mean(rouge_2_r):.2f} \n",
        "        rouge_l_f {statistics.mean(rouge_l_f):.2f} \n",
        "        rouge_l_p {statistics.mean(rouge_l_p):.2f} \n",
        "        rouge_l_r {statistics.mean(rouge_l_r):.2f}         \n",
        "        \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwTLYb9TlyGy",
        "outputId": "5bf10c24-8cdc-40be-81ab-a66d3e624640"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_lib_rouge_list:   \n",
            "        rouge_1_f 0.28 \n",
            "        rouge_1_p 0.30 \n",
            "        rouge_1_r 0.27 \n",
            "        rouge_2_f 0.08 \n",
            "        rouge_2_p 0.09 \n",
            "        rouge_2_r 0.07 \n",
            "        rouge_l_f 0.24 \n",
            "        rouge_l_p 0.26 \n",
            "        rouge_l_r 0.23         \n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "L2tzPkRynEZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDu-35-JjJob"
      },
      "source": [
        "# Dataset generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCt3s1SsjTyy",
        "outputId": "fe937875-a062-4d97-c9ee-d6caeb6271cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 57.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 62.5 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=4476d2a7e22f8606f79360c4e81ea5a07018dea05dd9c0f708b942d7c48064f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLS-3uTujJod"
      },
      "source": [
        "## NER model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C21UkdU6jJof"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIHPjA-TjJog"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"/content/drive/MyDrive/Diplom/NER/outputs/checkpoint-1372-epoch-7\")\n",
        "\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g16rhxhXjJoi"
      },
      "outputs": [],
      "source": [
        "def generate_decoder_input(example):\n",
        "    output = copy.deepcopy(example)\n",
        "    \n",
        "    for i in range(len(output)):\n",
        "        output[i] = \"<s>\" + output[i]\n",
        "    \n",
        "    return output\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iMY0qbGjJoj"
      },
      "outputs": [],
      "source": [
        "def generate_input(example): #пока не обрабатываю слова составные из двух и более токенов и не маскирую их корректно\n",
        "    ner_results = nlp(example)\n",
        "    output = []\n",
        "    \n",
        "    for i in range(len(ner_results)):\n",
        "      ents = [x['word'] for x in ner_results[i] if x['entity'] != 'LABEL_0' and len(x[\"word\"]) > 3]\n",
        "      words = [x if not [True for y in ents if x.count(y.replace(\"Ġ\", '')) > 0 ] else \"<mask>\" for x in example[0].split()]\n",
        "      output.append(\" \".join(words))\n",
        " \n",
        "    \n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrVnWNhVjJok"
      },
      "outputs": [],
      "source": [
        "def generate_labels(example):\n",
        "    output = copy.deepcopy(example)\n",
        "    for i in range(len(output)):\n",
        "        output[i]+=\"</s>\"\n",
        "    \n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twPFFmrBjJoo"
      },
      "source": [
        "# Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE1fOimJ0G7N",
        "outputId": "5df52509-39d4-4687-98f3-2384edc443a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-05-06 13:19:11--  https://archive.org/download/armancohan-long-summarization-paper-code/pubmed-dataset.zip\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia802905.us.archive.org/4/items/armancohan-long-summarization-paper-code/pubmed-dataset.zip [following]\n",
            "--2022-05-06 13:19:12--  https://ia802905.us.archive.org/4/items/armancohan-long-summarization-paper-code/pubmed-dataset.zip\n",
            "Resolving ia802905.us.archive.org (ia802905.us.archive.org)... 207.241.233.55\n",
            "Connecting to ia802905.us.archive.org (ia802905.us.archive.org)|207.241.233.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 880225504 (839M) [application/zip]\n",
            "Saving to: ‘pubmed-dataset.zip.1’\n",
            "\n",
            "pubmed-dataset.zip.   0%[                    ]       0  --.-KB/s               ^C\n"
          ]
        }
      ],
      "source": [
        "!wget https://archive.org/download/armancohan-long-summarization-paper-code/pubmed-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tQ-PkGZ90EA"
      },
      "outputs": [],
      "source": [
        "!cp /content/pubmed-dataset.zip /content/drive/MyDrive/Diplom/Summarization/pubmed-dataset.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo1tddaLjJoq"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-wDDSTZ6UME",
        "outputId": "f7f648a6-f577-44a0-fa1b-c0e8dd438915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/Diplom/Summarization/pubmed-dataset.zip\n",
            "   creating: pubmed-dataset/\n",
            "  inflating: __MACOSX/._pubmed-dataset  \n",
            "  inflating: pubmed-dataset/train.txt  \n",
            "  inflating: __MACOSX/pubmed-dataset/._train.txt  \n",
            "  inflating: pubmed-dataset/vocab    \n",
            "  inflating: __MACOSX/pubmed-dataset/._vocab  \n",
            "  inflating: pubmed-dataset/test.txt  \n",
            "  inflating: __MACOSX/pubmed-dataset/._test.txt  \n",
            "  inflating: pubmed-dataset/val.txt  \n",
            "  inflating: __MACOSX/pubmed-dataset/._val.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip  /content/drive/MyDrive/Diplom/Summarization/pubmed-dataset.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "qkVX_jmrX8cl",
        "outputId": "9999ba27-4180-48f2-906b-6d542ff0594f"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-cad31a50edd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/pubmed-dataset/test.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/pubmed-dataset/test.txt'"
          ]
        }
      ],
      "source": [
        "with open(\"/content/pubmed-dataset/test.txt\") as file:\n",
        "    example = file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq2mVjLRb_H6",
        "outputId": "e67cd668-b950-4da6-c9f6-469c1081811b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6658"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d0rtSMIjJor"
      },
      "outputs": [],
      "source": [
        "# from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "\n",
        "# tok = BartTokenizer.from_pretrained(\"facebook/bart-large\") #переписать через класс датасет и даталоадер\n",
        "# model = BartForConditionalGeneration(BartConfig())\n",
        "\n",
        "# for i in tqdm(range(15)):\n",
        "#   for iter in range(len(example)):\n",
        "#     batch = [example[iter]]\n",
        "#     input_batch = generate_input(batch)\n",
        "#     decoder_input_batch = generate_decoder_input(batch)\n",
        "#     labels_batch = generate_labels(batch)\n",
        "\n",
        "#     input_batch[0] = input_batch[0][:1024] #bad solution, need to split docs on  words. already did it befpre\n",
        "#     decoder_input_batch[0] = decoder_input_batch[0][:1024]\n",
        "#     labels_batch[0] = labels_batch[0][:1024]\n",
        "\n",
        "#     input_ids = tok.batch_encode_plus(input_batch, add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids\n",
        "#     decoder_input_ids = tok.batch_encode_plus(decoder_input_batch, add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids\n",
        "#     labels = tok.batch_encode_plus(labels_batch, add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids\n",
        "\n",
        "#     try: #also bug to be solved\n",
        "#       loss = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids, labels=labels)[0]\n",
        "      \n",
        "#       print(f\"Epoch {i}, iteration {iter} loss: {loss}\")\n",
        "#     except: \n",
        "#       pass\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00y2_kQ9WLxf"
      },
      "outputs": [],
      "source": [
        "#encoding=utf-8\n",
        "\n",
        "from transformers import (\n",
        "    BartForConditionalGeneration, BartTokenizer, BartForCausalLM,\n",
        "    Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "  )\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "\n",
        "# ## Initiating model and trainer for training\n",
        "from transformers import BartModel, BartConfig\n",
        "from transformers import BartTokenizerFast\n",
        "\n",
        "\n",
        "configuration = BartConfig(\n",
        "    vocab_size=52000,\n",
        "    max_position_embeddings=258,\n",
        "    d_model=256,\n",
        "    encoder_layers=3,\n",
        "    decoder_layers=3,\n",
        "    encoder_attention_heads=4,\n",
        "    decoder_attention_heads=4,\n",
        "    decoder_ffn_dim=1024,\n",
        "    encoder_ffn_dim=1024,\n",
        ")\n",
        "model = BartForCausalLM(configuration)\n",
        "tokenizer = BartTokenizerFast.from_pretrained(\"./dic\", max_len=256, additional_special_tokens=['[CH]', '[OTHER]', '[VAR]', '[NUM]'])\n",
        "\n",
        "\n",
        "# ### HTTP Request DataPreparing & Modeling\n",
        "data = []\n",
        "with open(\"../data/sample.txt\") as f1:\n",
        "    for src in f1:\n",
        "      data.append(\n",
        "          {\n",
        "              \"seq2seq\": {\n",
        "                  \"input\": src.strip()\n",
        "              }\n",
        "          }\n",
        "      )\n",
        "print(f'total size of data is {len(data)}')\n",
        "\n",
        "\n",
        "# splitting dataset into train, validation\n",
        "split = 0.2\n",
        "train_dataset, eval_dataset = random_split(data, lengths=[int((1-split)*len(data))+1, int(split*len(data))])\n",
        "\n",
        "\n",
        "# defining collator functioon for preparing batches on the fly ..\n",
        "def data_collator(features:list):\n",
        "   inputs = [f[\"seq2seq\"][\"input\"] for f in features]\n",
        "   batch = tokenizer.prepare_seq2seq_batch(src_texts=inputs, max_length=256, padding='max_length')\n",
        "   batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
        "   for k in batch:\n",
        "        batch[k] = torch.tensor(batch[k])\n",
        "   return batch\n",
        "\n",
        "\n",
        "batch_out = data_collator(eval_dataset)\n",
        "print(batch_out)\n",
        "print(batch_out['input_ids'].shape,batch_out['labels'].shape,batch_out['attention_mask'].shape)\n",
        "\n",
        "\n",
        "# defining training related arguments\n",
        "args = Seq2SeqTrainingArguments(output_dir=\"clm-checkpoints\",\n",
        "                        do_train=True,\n",
        "                        do_eval=True,\n",
        "                        evaluation_strategy=\"epoch\",\n",
        "                        per_device_train_batch_size=8,\n",
        "                        per_device_eval_batch_size=8,\n",
        "                        learning_rate=5e-5,\n",
        "                        num_train_epochs=1,\n",
        "                        logging_dir=\"./logs\")\n",
        "\n",
        "\n",
        "# defining trainer using 🤗\n",
        "trainer = Seq2SeqTrainer(model=model, \n",
        "                args=args, \n",
        "                data_collator=data_collator, \n",
        "                train_dataset=train_dataset, \n",
        "                eval_dataset=eval_dataset)\n",
        "\n",
        "\n",
        "# ## Training time\n",
        "trainer.train()\n",
        "# It will take hours to train this model on this dataset\n",
        "\n",
        "\n",
        "# lets save model\n",
        "trainer.evaluate(eval_dataset=eval_dataset)\n",
        "trainer.save_model(\"clm-checkpoints\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "ybSf7_rWNv8O",
        "outputId": "c46f2e0d-7328-4c25-c5ea-1951d8c6f612"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3d27167fe07e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
          ]
        }
      ],
      "source": [
        "len(input_ids[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhshaU5Eh41o",
        "outputId": "c1e11ec4-f6c9-4d92-bca8-2095b1c46022"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "223"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(decoder_input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvGHDav9h9Dh",
        "outputId": "60fb017e-40ff-468a-f46c-7c422129ba21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "222"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmWwHHhhjJos"
      },
      "outputs": [],
      "source": [
        "model(input_ids=input_ids.unsqueeze(0), decoder_input_ids=decoder_input_ids.unsqueeze(0), labels=labels.unsqueeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDsKjlDm1hxH"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/Diplom/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQUT1L8SjJos"
      },
      "source": [
        "### dataset handeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUTm4YUBjJoy"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYQKIPYVjJoy",
        "outputId": "5b53469e-8c47-4a8c-d4bd-620b18aa67e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "    BartForConditionalGeneration,\n",
        "    BartTokenizerFast,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    AdamW,\n",
        "    get_scheduler,\n",
        ")\n",
        "\n",
        "from datasets import (\n",
        "    load_metric, \n",
        "    load_dataset,\n",
        "    load_from_disk\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch\n",
        "import nltk\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "nltk.download('punkt');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCxS_ntejJoy"
      },
      "source": [
        "# Preparation of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTm5rr_QjJoy",
        "outputId": "adb6038a-6b3d-4703-e955-e7e4d825cffa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace\r\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTHuSVhkjJoz"
      },
      "outputs": [],
      "source": [
        "# Dataset parameters\n",
        "\n",
        "batch_size = 2\n",
        "max_source_length = 1024\n",
        "max_target_length = 256\n",
        "padding = 'max_length'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2kYZeFKjJoz"
      },
      "outputs": [],
      "source": [
        "# Loading pretrained facebook/bart-base tokenizer\n",
        "\n",
        "tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgrZeFvZjJo0",
        "outputId": "99cc5004-b00b-4e65-c1a9-88d0312d64c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reusing dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 287113\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 13368\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 11490\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading raw CNN/DailyMail dataset\n",
        "\n",
        "raw_dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
        "column_names = raw_dataset['train'].column_names\n",
        "\n",
        "raw_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i2FV-64jJo0"
      },
      "outputs": [],
      "source": [
        "# Function for processing dataset for loading into a model\n",
        "\n",
        "def process_data_to_model_inputs(example):\n",
        "    inputs = example['article']\n",
        "    targets = example['highlights']\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    labels['input_ids'] = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels['input_ids']]\n",
        "\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    \n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvrM5ZHujJo1",
        "outputId": "10d70964-fd0d-434d-aba8-acad42182387",
        "colab": {
          "referenced_widgets": [
            "d01f1b3033284967861320017af47c58",
            "202c81e71ebd4926abece38dfa65dc08",
            "f6202fe1ecd0412ba1844846578915f7"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d01f1b3033284967861320017af47c58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4487 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "202c81e71ebd4926abece38dfa65dc08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/209 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6202fe1ecd0412ba1844846578915f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/180 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['attention_mask', 'input_ids', 'labels'],\n",
              "     num_rows: 287113\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['attention_mask', 'input_ids', 'labels'],\n",
              "     num_rows: 13368\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['attention_mask', 'input_ids', 'labels'],\n",
              "     num_rows: 11490\n",
              " }))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Processing dataset, split on train, eval, test datasets\n",
        "\n",
        "dataset = raw_dataset.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched = True,\n",
        "    batch_size = batch_size,\n",
        "    remove_columns = column_names,\n",
        "    load_from_cache_file = True,\n",
        ")\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "train_dataset, eval_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3Jjo7DTjJo1"
      },
      "outputs": [],
      "source": [
        "# Saving dataset to disk for later reuse\n",
        "\n",
        "dataset.save_to_disk('content/processed_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "817NWdh3jJo2",
        "outputId": "ec75f004-7f50-414b-b8df-41ae02f5f6c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['attention_mask', 'input_ids', 'labels'],\n",
              "     num_rows: 287113\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['attention_mask', 'input_ids', 'labels'],\n",
              "     num_rows: 13368\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['attention_mask', 'input_ids', 'labels'],\n",
              "     num_rows: 11490\n",
              " }))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading dataset from disk for reuse\n",
        "\n",
        "dataset = load_from_disk('content/processed_dataset')\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "train_dataset, eval_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lozrRRvajJo2",
        "outputId": "0c50875a-e5b9-4c27-8531-b180d0046fc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['attention_mask', 'input_ids', 'labels'],\n",
              "     num_rows: 287113\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['attention_mask', 'input_ids', 'labels'],\n",
              "     num_rows: 13368\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['attention_mask', 'input_ids', 'labels'],\n",
              "     num_rows: 13368\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['attention_mask', 'input_ids', 'labels'],\n",
              "     num_rows: 11490\n",
              " }))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Taking a part of the dataset for test training\n",
        "\n",
        "dataset_part = 1\n",
        "\n",
        "train_dataset = dataset['train'].select(range(len(dataset['train']) // dataset_part))\n",
        "eval_dataset = dataset['validation'].select(range(len(dataset['validation']) // dataset_part))\n",
        "test_dataset = dataset['test'].select(range(len(dataset['test']) // dataset_part))\n",
        "\n",
        "train_subset = train_dataset.select(range(len(eval_dataset)))\n",
        "\n",
        "train_dataset, train_subset, eval_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6SIhad3jJo2"
      },
      "source": [
        "# Preparation of model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsYSTz1OjJo3"
      },
      "outputs": [],
      "source": [
        "# Loading pretrained facebook/bart-base model: BartForConditionalGeneration\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9osdBRhjJo3"
      },
      "outputs": [],
      "source": [
        "# DataLoaders for each dataset\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    padding=padding\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "subset_dataloader = DataLoader(\n",
        "    train_subset,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j71THrRDjJo4"
      },
      "outputs": [],
      "source": [
        "# Function for post-processing generated texts\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "    return preds, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nnSClSgjJo4"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpemjV_SjJo4",
        "outputId": "7a7fb9b7-ba05-4f50-a7a9-779abdef8cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Aug 11 17:33:25 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.152.00   Driver Version: 418.152.00   CUDA Version: 11.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |\n",
            "| N/A   52C    P0   237W / 300W |   9959MiB / 32478MiB |     83%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
            "| N/A   47C    P0    53W / 300W |  16326MiB / 32478MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
            "| N/A   47C    P0    51W / 300W |   2733MiB / 32478MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
            "| N/A   47C    P0    50W / 300W |   3007MiB / 32478MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03-hte9fjJo5"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "\n",
        "epoch_num = 10\n",
        "optimizer_steps = 128\n",
        "\n",
        "base_learning_rate = 3e-4\n",
        "betas = (0.9, 0.999)\n",
        "eps = 1e-8\n",
        "weight_decay = 0.01\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "        'weight_decay': 0.01,\n",
        "    },\n",
        "    {\n",
        "        'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "        'weight_decay': 0.0,\n",
        "    },\n",
        "]\n",
        "\n",
        "num_warmup_steps = 256\n",
        "num_training_steps = epoch_num * len(train_dataloader)\n",
        "lr_scheduler_type = 'linear'\n",
        "\n",
        "num_beams = 3\n",
        "\n",
        "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6ptXl2MjJo5"
      },
      "outputs": [],
      "source": [
        "# Optimizer: AdamW\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=base_learning_rate,\n",
        "    betas=betas,\n",
        "    eps=eps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ4CZgbujJo5"
      },
      "outputs": [],
      "source": [
        "# Lr_scheduler\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=lr_scheduler_type,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b1n4x9IjJo6"
      },
      "outputs": [],
      "source": [
        "# Metric\n",
        "\n",
        "metric = load_metric('rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2U37_B3OjJo6",
        "outputId": "4c6449b2-7d1c-4a2f-e2c3-dacd76b292f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transfer model to GPU\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2DQGY3VjJo6"
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "\n",
        "train_losses = []\n",
        "train_losses_epoch = []\n",
        "eval_losses_epoch = []\n",
        "metrics = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T-Dtcs2jJo7"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGMt2KmRjJo8",
        "outputId": "0670cb73-343d-4c73-cc0b-1ee454fc97e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model training\n",
            "opimizers updated, step 1 / 1435570, loss 4.326367378234863\n",
            "opimizers updated, step 129 / 1435570, loss 569.5679888725281\n",
            "opimizers updated, step 257 / 1435570, loss 549.0737895965576\n",
            "opimizers updated, step 385 / 1435570, loss 544.4964938163757\n",
            "opimizers updated, step 513 / 1435570, loss 523.524290561676\n",
            "opimizers updated, step 641 / 1435570, loss 516.6678054332733\n",
            "opimizers updated, step 769 / 1435570, loss 504.1267936229706\n",
            "opimizers updated, step 897 / 1435570, loss 480.6759955883026\n",
            "opimizers updated, step 1025 / 1435570, loss 485.59634804725647\n",
            "opimizers updated, step 1153 / 1435570, loss 475.6121323108673\n",
            "opimizers updated, step 1281 / 1435570, loss 458.60372829437256\n",
            "opimizers updated, step 1409 / 1435570, loss 439.793946146965\n",
            "opimizers updated, step 1537 / 1435570, loss 435.33740639686584\n",
            "opimizers updated, step 1665 / 1435570, loss 415.49433290958405\n",
            "opimizers updated, step 1793 / 1435570, loss 418.0387616157532\n",
            "opimizers updated, step 1921 / 1435570, loss 409.5816035270691\n",
            "opimizers updated, step 2049 / 1435570, loss 402.77449882030487\n",
            "opimizers updated, step 2177 / 1435570, loss 394.28254199028015\n",
            "opimizers updated, step 2305 / 1435570, loss 382.7154850959778\n",
            "opimizers updated, step 2433 / 1435570, loss 383.06559443473816\n",
            "opimizers updated, step 2561 / 1435570, loss 381.28036189079285\n",
            "opimizers updated, step 2689 / 1435570, loss 374.9692550897598\n",
            "opimizers updated, step 2817 / 1435570, loss 369.2015906572342\n",
            "opimizers updated, step 2945 / 1435570, loss 366.148334980011\n",
            "opimizers updated, step 3073 / 1435570, loss 365.29709708690643\n",
            "opimizers updated, step 3201 / 1435570, loss 361.7554929256439\n",
            "opimizers updated, step 3329 / 1435570, loss 354.8096423149109\n",
            "opimizers updated, step 3457 / 1435570, loss 346.62967801094055\n",
            "opimizers updated, step 3585 / 1435570, loss 344.09714794158936\n",
            "opimizers updated, step 3713 / 1435570, loss 341.1741998195648\n",
            "opimizers updated, step 3841 / 1435570, loss 344.5966213941574\n",
            "opimizers updated, step 3969 / 1435570, loss 337.69809913635254\n",
            "opimizers updated, step 4097 / 1435570, loss 324.6456105709076\n",
            "opimizers updated, step 4225 / 1435570, loss 328.4112911224365\n",
            "opimizers updated, step 4353 / 1435570, loss 323.3799525499344\n",
            "opimizers updated, step 4481 / 1435570, loss 320.3372970819473\n",
            "opimizers updated, step 4609 / 1435570, loss 312.1018455028534\n",
            "opimizers updated, step 4737 / 1435570, loss 313.83206367492676\n",
            "opimizers updated, step 4865 / 1435570, loss 307.7608366012573\n",
            "opimizers updated, step 4993 / 1435570, loss 312.33379316329956\n",
            "opimizers updated, step 5121 / 1435570, loss 306.6567908525467\n",
            "opimizers updated, step 5249 / 1435570, loss 305.5007652044296\n",
            "opimizers updated, step 5377 / 1435570, loss 306.0333718061447\n",
            "opimizers updated, step 5505 / 1435570, loss 307.75314980745316\n",
            "opimizers updated, step 5633 / 1435570, loss 311.93526470661163\n",
            "opimizers updated, step 5761 / 1435570, loss 314.1278339624405\n",
            "opimizers updated, step 5889 / 1435570, loss 299.9119622707367\n",
            "opimizers updated, step 6017 / 1435570, loss 295.35016322135925\n",
            "opimizers updated, step 6145 / 1435570, loss 309.22283363342285\n",
            "opimizers updated, step 6273 / 1435570, loss 291.74156391620636\n",
            "opimizers updated, step 6401 / 1435570, loss 297.88037288188934\n",
            "opimizers updated, step 6529 / 1435570, loss 296.5477614402771\n",
            "opimizers updated, step 6657 / 1435570, loss 301.73753798007965\n",
            "opimizers updated, step 6785 / 1435570, loss 289.47751331329346\n",
            "opimizers updated, step 6913 / 1435570, loss 290.84101939201355\n",
            "opimizers updated, step 7041 / 1435570, loss 283.33682692050934\n",
            "opimizers updated, step 7169 / 1435570, loss 291.1494415998459\n",
            "opimizers updated, step 7297 / 1435570, loss 292.4667316675186\n",
            "opimizers updated, step 7425 / 1435570, loss 291.8128016591072\n",
            "opimizers updated, step 7553 / 1435570, loss 295.7669414281845\n",
            "opimizers updated, step 7681 / 1435570, loss 296.6636233329773\n",
            "opimizers updated, step 7809 / 1435570, loss 286.4571968317032\n",
            "opimizers updated, step 7937 / 1435570, loss 288.39797496795654\n",
            "opimizers updated, step 8065 / 1435570, loss 285.2521777153015\n",
            "opimizers updated, step 8193 / 1435570, loss 291.64819252491\n",
            "opimizers updated, step 8321 / 1435570, loss 279.3877388238907\n",
            "opimizers updated, step 8449 / 1435570, loss 291.92416524887085\n",
            "opimizers updated, step 8577 / 1435570, loss 285.86917877197266\n",
            "opimizers updated, step 8705 / 1435570, loss 284.08448219299316\n",
            "opimizers updated, step 8833 / 1435570, loss 286.20844197273254\n",
            "opimizers updated, step 8961 / 1435570, loss 287.43746507167816\n",
            "opimizers updated, step 9089 / 1435570, loss 289.6111904978752\n",
            "opimizers updated, step 9217 / 1435570, loss 282.360458612442\n",
            "opimizers updated, step 9345 / 1435570, loss 279.1490846276283\n",
            "opimizers updated, step 9473 / 1435570, loss 274.98708510398865\n",
            "opimizers updated, step 9601 / 1435570, loss 285.6930792927742\n",
            "opimizers updated, step 9729 / 1435570, loss 281.9749504327774\n",
            "opimizers updated, step 9857 / 1435570, loss 285.59990108013153\n",
            "opimizers updated, step 9985 / 1435570, loss 278.23834335803986\n",
            "opimizers updated, step 10113 / 1435570, loss 281.6553391814232\n",
            "opimizers updated, step 10241 / 1435570, loss 272.5545984506607\n",
            "opimizers updated, step 10369 / 1435570, loss 273.30467212200165\n",
            "opimizers updated, step 10497 / 1435570, loss 278.88436257839203\n",
            "opimizers updated, step 10625 / 1435570, loss 271.48959708213806\n",
            "opimizers updated, step 10753 / 1435570, loss 273.3178491592407\n",
            "opimizers updated, step 10881 / 1435570, loss 276.29723358154297\n",
            "opimizers updated, step 11009 / 1435570, loss 282.117222905159\n",
            "opimizers updated, step 11137 / 1435570, loss 276.5081169605255\n",
            "opimizers updated, step 11265 / 1435570, loss 277.3251795768738\n",
            "opimizers updated, step 11393 / 1435570, loss 273.8026188611984\n",
            "opimizers updated, step 11521 / 1435570, loss 280.3969521522522\n",
            "opimizers updated, step 11649 / 1435570, loss 278.3988403081894\n",
            "opimizers updated, step 11777 / 1435570, loss 275.0667984485626\n",
            "opimizers updated, step 11905 / 1435570, loss 278.1309643983841\n",
            "opimizers updated, step 12033 / 1435570, loss 274.6517856121063\n",
            "opimizers updated, step 12161 / 1435570, loss 277.657289147377\n",
            "opimizers updated, step 12289 / 1435570, loss 279.3511257171631\n",
            "opimizers updated, step 12417 / 1435570, loss 266.3984135389328\n",
            "opimizers updated, step 12545 / 1435570, loss 278.77452278137207\n",
            "opimizers updated, step 12673 / 1435570, loss 277.46578073501587\n",
            "opimizers updated, step 12801 / 1435570, loss 271.91301000118256\n",
            "opimizers updated, step 12929 / 1435570, loss 268.3220158815384\n",
            "opimizers updated, step 13057 / 1435570, loss 269.08871126174927\n",
            "opimizers updated, step 13185 / 1435570, loss 273.37593626976013\n",
            "opimizers updated, step 13313 / 1435570, loss 279.0052295923233\n",
            "opimizers updated, step 13441 / 1435570, loss 270.4537513256073\n",
            "opimizers updated, step 13569 / 1435570, loss 269.02358281612396\n",
            "opimizers updated, step 13697 / 1435570, loss 283.810928940773\n",
            "opimizers updated, step 13825 / 1435570, loss 267.2342674732208\n",
            "opimizers updated, step 13953 / 1435570, loss 270.4993063211441\n",
            "opimizers updated, step 14081 / 1435570, loss 271.98450553417206\n",
            "opimizers updated, step 14209 / 1435570, loss 268.65668284893036\n",
            "opimizers updated, step 14337 / 1435570, loss 272.1294726729393\n",
            "opimizers updated, step 14465 / 1435570, loss 268.41493558883667\n",
            "opimizers updated, step 14593 / 1435570, loss 275.2718359231949\n",
            "opimizers updated, step 14721 / 1435570, loss 279.6187205314636\n",
            "opimizers updated, step 14849 / 1435570, loss 277.83994686603546\n",
            "opimizers updated, step 14977 / 1435570, loss 276.19842052459717\n",
            "opimizers updated, step 15105 / 1435570, loss 280.4442957639694\n",
            "opimizers updated, step 15233 / 1435570, loss 277.25852102041245\n",
            "opimizers updated, step 15361 / 1435570, loss 272.9241632223129\n",
            "opimizers updated, step 15489 / 1435570, loss 269.3675866127014\n",
            "opimizers updated, step 15617 / 1435570, loss 279.07800155878067\n",
            "opimizers updated, step 15745 / 1435570, loss 264.411208987236\n",
            "opimizers updated, step 15873 / 1435570, loss 268.20492446422577\n",
            "opimizers updated, step 16001 / 1435570, loss 259.8013073205948\n",
            "opimizers updated, step 16129 / 1435570, loss 266.9639424085617\n",
            "opimizers updated, step 16257 / 1435570, loss 273.72670888900757\n",
            "opimizers updated, step 16385 / 1435570, loss 263.79712158441544\n",
            "opimizers updated, step 16513 / 1435570, loss 278.4903312921524\n",
            "opimizers updated, step 16641 / 1435570, loss 274.15723264217377\n",
            "opimizers updated, step 16769 / 1435570, loss 270.96385180950165\n",
            "opimizers updated, step 16897 / 1435570, loss 265.1583403944969\n",
            "opimizers updated, step 17025 / 1435570, loss 260.9828271865845\n",
            "opimizers updated, step 17153 / 1435570, loss 271.9037901163101\n",
            "opimizers updated, step 17281 / 1435570, loss 263.1230537891388\n",
            "opimizers updated, step 17409 / 1435570, loss 267.4444054365158\n",
            "opimizers updated, step 17537 / 1435570, loss 270.49348390102386\n",
            "opimizers updated, step 17665 / 1435570, loss 271.8874664902687\n",
            "opimizers updated, step 17793 / 1435570, loss 264.01902961730957\n",
            "opimizers updated, step 17921 / 1435570, loss 256.52360355854034\n",
            "opimizers updated, step 18049 / 1435570, loss 264.8892968893051\n",
            "opimizers updated, step 18177 / 1435570, loss 268.9814334511757\n",
            "opimizers updated, step 18305 / 1435570, loss 273.4779897928238\n",
            "opimizers updated, step 18433 / 1435570, loss 273.75459092855453\n",
            "opimizers updated, step 18561 / 1435570, loss 270.1782020330429\n",
            "opimizers updated, step 18689 / 1435570, loss 269.5665875673294\n",
            "opimizers updated, step 18817 / 1435570, loss 262.2938296198845\n",
            "opimizers updated, step 18945 / 1435570, loss 260.79609298706055\n",
            "opimizers updated, step 19073 / 1435570, loss 265.8775347471237\n",
            "opimizers updated, step 19201 / 1435570, loss 267.03168815374374\n",
            "opimizers updated, step 19329 / 1435570, loss 269.5333893299103\n",
            "opimizers updated, step 19457 / 1435570, loss 264.98604357242584\n",
            "opimizers updated, step 19585 / 1435570, loss 265.51015269756317\n",
            "opimizers updated, step 19713 / 1435570, loss 258.0205252766609\n",
            "opimizers updated, step 19841 / 1435570, loss 264.4417269229889\n",
            "opimizers updated, step 19969 / 1435570, loss 269.84090065956116\n",
            "opimizers updated, step 20097 / 1435570, loss 259.09601974487305\n",
            "opimizers updated, step 20225 / 1435570, loss 267.10619980096817\n",
            "opimizers updated, step 20353 / 1435570, loss 269.21073573827744\n",
            "opimizers updated, step 20481 / 1435570, loss 266.48908615112305\n",
            "opimizers updated, step 20609 / 1435570, loss 265.1155401468277\n",
            "opimizers updated, step 20737 / 1435570, loss 268.5742255449295\n",
            "opimizers updated, step 20865 / 1435570, loss 260.4329845905304\n",
            "opimizers updated, step 20993 / 1435570, loss 274.66324758529663\n",
            "opimizers updated, step 21121 / 1435570, loss 263.2803971171379\n",
            "opimizers updated, step 21249 / 1435570, loss 258.45607525110245\n",
            "opimizers updated, step 21377 / 1435570, loss 262.4846382141113\n",
            "opimizers updated, step 21505 / 1435570, loss 263.2520810365677\n",
            "opimizers updated, step 21633 / 1435570, loss 265.4682973623276\n",
            "opimizers updated, step 21761 / 1435570, loss 278.70940136909485\n",
            "opimizers updated, step 21889 / 1435570, loss 264.7512237429619\n",
            "opimizers updated, step 22017 / 1435570, loss 273.5852960348129\n",
            "opimizers updated, step 22145 / 1435570, loss 259.78077644109726\n",
            "opimizers updated, step 22273 / 1435570, loss 261.1329709291458\n",
            "opimizers updated, step 22401 / 1435570, loss 260.6214975118637\n",
            "opimizers updated, step 22529 / 1435570, loss 272.3233829140663\n",
            "opimizers updated, step 22657 / 1435570, loss 268.6576843261719\n",
            "opimizers updated, step 22785 / 1435570, loss 267.49433171749115\n",
            "opimizers updated, step 22913 / 1435570, loss 270.4259001612663\n",
            "opimizers updated, step 23041 / 1435570, loss 265.37167650461197\n",
            "opimizers updated, step 23169 / 1435570, loss 266.85355961322784\n",
            "opimizers updated, step 23297 / 1435570, loss 270.9147902727127\n",
            "opimizers updated, step 23425 / 1435570, loss 263.2362023591995\n",
            "opimizers updated, step 23553 / 1435570, loss 261.7803971171379\n",
            "opimizers updated, step 23681 / 1435570, loss 268.81545251607895\n",
            "opimizers updated, step 23809 / 1435570, loss 261.14219784736633\n",
            "opimizers updated, step 23937 / 1435570, loss 264.7881190776825\n",
            "opimizers updated, step 24065 / 1435570, loss 261.63462722301483\n",
            "opimizers updated, step 24193 / 1435570, loss 257.5208904147148\n",
            "opimizers updated, step 24321 / 1435570, loss 265.9870421886444\n",
            "opimizers updated, step 24449 / 1435570, loss 263.8135786652565\n",
            "opimizers updated, step 24577 / 1435570, loss 266.2687829732895\n",
            "opimizers updated, step 24705 / 1435570, loss 266.31020736694336\n",
            "opimizers updated, step 24833 / 1435570, loss 262.8351023197174\n",
            "opimizers updated, step 24961 / 1435570, loss 254.53785705566406\n",
            "opimizers updated, step 25089 / 1435570, loss 259.99232375621796\n",
            "opimizers updated, step 25217 / 1435570, loss 263.14244174957275\n",
            "opimizers updated, step 25345 / 1435570, loss 264.26933324337006\n",
            "opimizers updated, step 25473 / 1435570, loss 258.38269823789597\n",
            "opimizers updated, step 25601 / 1435570, loss 257.6615751385689\n",
            "opimizers updated, step 25729 / 1435570, loss 257.08611780405045\n",
            "opimizers updated, step 25857 / 1435570, loss 265.16798174381256\n",
            "opimizers updated, step 25985 / 1435570, loss 271.1221204996109\n",
            "opimizers updated, step 26113 / 1435570, loss 256.3155168890953\n",
            "opimizers updated, step 26241 / 1435570, loss 257.1221049427986\n",
            "opimizers updated, step 26369 / 1435570, loss 260.74475145339966\n",
            "opimizers updated, step 26497 / 1435570, loss 266.6319886445999\n",
            "opimizers updated, step 26625 / 1435570, loss 257.9179736375809\n",
            "opimizers updated, step 26753 / 1435570, loss 264.72927820682526\n",
            "opimizers updated, step 26881 / 1435570, loss 267.2779466509819\n",
            "opimizers updated, step 27009 / 1435570, loss 256.0586887001991\n",
            "opimizers updated, step 27137 / 1435570, loss 263.0340619087219\n",
            "opimizers updated, step 27265 / 1435570, loss 265.8581130504608\n",
            "opimizers updated, step 27393 / 1435570, loss 262.7146458029747\n",
            "opimizers updated, step 27521 / 1435570, loss 265.82966470718384\n",
            "opimizers updated, step 27649 / 1435570, loss 265.69279557466507\n",
            "opimizers updated, step 27777 / 1435570, loss 270.3591034412384\n",
            "opimizers updated, step 27905 / 1435570, loss 260.71305364370346\n",
            "opimizers updated, step 28033 / 1435570, loss 266.4807507991791\n",
            "opimizers updated, step 28161 / 1435570, loss 254.43204843997955\n",
            "opimizers updated, step 28289 / 1435570, loss 267.9782834649086\n",
            "opimizers updated, step 28417 / 1435570, loss 263.38523030281067\n",
            "opimizers updated, step 28545 / 1435570, loss 259.15069013834\n",
            "opimizers updated, step 28673 / 1435570, loss 258.8299633860588\n",
            "opimizers updated, step 28801 / 1435570, loss 268.88245099782944\n",
            "opimizers updated, step 28929 / 1435570, loss 249.86157262325287\n",
            "opimizers updated, step 29057 / 1435570, loss 258.42035657167435\n",
            "opimizers updated, step 29185 / 1435570, loss 257.78345692157745\n",
            "opimizers updated, step 29313 / 1435570, loss 253.31941026449203\n",
            "opimizers updated, step 29441 / 1435570, loss 260.4121845960617\n",
            "opimizers updated, step 29569 / 1435570, loss 262.8066640496254\n",
            "opimizers updated, step 29697 / 1435570, loss 258.9941873550415\n",
            "opimizers updated, step 29825 / 1435570, loss 254.86456871032715\n",
            "opimizers updated, step 29953 / 1435570, loss 264.16139125823975\n",
            "opimizers updated, step 30081 / 1435570, loss 260.94493931531906\n",
            "opimizers updated, step 30209 / 1435570, loss 256.4739991426468\n",
            "opimizers updated, step 30337 / 1435570, loss 258.35936683416367\n",
            "opimizers updated, step 30465 / 1435570, loss 258.9985845685005\n",
            "opimizers updated, step 30593 / 1435570, loss 262.4019426703453\n",
            "opimizers updated, step 30721 / 1435570, loss 257.27689957618713\n",
            "opimizers updated, step 30849 / 1435570, loss 259.1557809114456\n",
            "opimizers updated, step 30977 / 1435570, loss 260.03165566921234\n",
            "opimizers updated, step 31105 / 1435570, loss 261.18221390247345\n",
            "opimizers updated, step 31233 / 1435570, loss 257.26780438423157\n",
            "opimizers updated, step 31361 / 1435570, loss 254.8817493915558\n",
            "opimizers updated, step 31489 / 1435570, loss 265.7639437317848\n",
            "opimizers updated, step 31617 / 1435570, loss 260.64542043209076\n",
            "opimizers updated, step 31745 / 1435570, loss 262.7747107744217\n",
            "opimizers updated, step 31873 / 1435570, loss 256.535708129406\n",
            "opimizers updated, step 32001 / 1435570, loss 256.46428084373474\n",
            "opimizers updated, step 32129 / 1435570, loss 248.64147716760635\n",
            "opimizers updated, step 32257 / 1435570, loss 256.55939543247223\n",
            "opimizers updated, step 32385 / 1435570, loss 264.48769295215607\n",
            "opimizers updated, step 32513 / 1435570, loss 260.4186182022095\n",
            "opimizers updated, step 32641 / 1435570, loss 262.51540011167526\n",
            "opimizers updated, step 32769 / 1435570, loss 262.6047749519348\n",
            "opimizers updated, step 32897 / 1435570, loss 268.42781245708466\n",
            "opimizers updated, step 33025 / 1435570, loss 255.0912522673607\n",
            "opimizers updated, step 33153 / 1435570, loss 267.64858400821686\n",
            "opimizers updated, step 33281 / 1435570, loss 264.1428046822548\n",
            "opimizers updated, step 33409 / 1435570, loss 257.0251727104187\n",
            "opimizers updated, step 33537 / 1435570, loss 260.96047163009644\n",
            "opimizers updated, step 33665 / 1435570, loss 256.055932700634\n",
            "opimizers updated, step 33793 / 1435570, loss 267.8254331946373\n",
            "opimizers updated, step 33921 / 1435570, loss 266.05210268497467\n",
            "opimizers updated, step 34049 / 1435570, loss 257.141864836216\n",
            "opimizers updated, step 34177 / 1435570, loss 259.5252503156662\n",
            "opimizers updated, step 34305 / 1435570, loss 260.74191480875015\n",
            "opimizers updated, step 34433 / 1435570, loss 256.70379000902176\n",
            "opimizers updated, step 34561 / 1435570, loss 256.3343105316162\n",
            "opimizers updated, step 34689 / 1435570, loss 260.9824820160866\n",
            "opimizers updated, step 34817 / 1435570, loss 253.99856287240982\n",
            "opimizers updated, step 34945 / 1435570, loss 265.13826179504395\n",
            "opimizers updated, step 35073 / 1435570, loss 254.74554538726807\n",
            "opimizers updated, step 35201 / 1435570, loss 251.21044075489044\n",
            "opimizers updated, step 35329 / 1435570, loss 257.3572336435318\n",
            "opimizers updated, step 35457 / 1435570, loss 256.19233828783035\n",
            "opimizers updated, step 35585 / 1435570, loss 263.5742329955101\n",
            "opimizers updated, step 35713 / 1435570, loss 256.54247111082077\n",
            "opimizers updated, step 35841 / 1435570, loss 263.04364562034607\n",
            "opimizers updated, step 35969 / 1435570, loss 254.08788245916367\n",
            "opimizers updated, step 36097 / 1435570, loss 256.4669260978699\n",
            "opimizers updated, step 36225 / 1435570, loss 253.70486268401146\n",
            "opimizers updated, step 36353 / 1435570, loss 253.16740065813065\n",
            "opimizers updated, step 36481 / 1435570, loss 263.05204117298126\n",
            "opimizers updated, step 36609 / 1435570, loss 262.53833961486816\n",
            "opimizers updated, step 36737 / 1435570, loss 258.2350164651871\n",
            "opimizers updated, step 36865 / 1435570, loss 251.8274965286255\n",
            "opimizers updated, step 36993 / 1435570, loss 260.18820011615753\n",
            "opimizers updated, step 37121 / 1435570, loss 256.36654901504517\n",
            "opimizers updated, step 37249 / 1435570, loss 264.86972975730896\n",
            "opimizers updated, step 37377 / 1435570, loss 259.1157814860344\n",
            "opimizers updated, step 37505 / 1435570, loss 254.89665108919144\n",
            "opimizers updated, step 37633 / 1435570, loss 259.78257632255554\n",
            "opimizers updated, step 37761 / 1435570, loss 250.1246975660324\n",
            "opimizers updated, step 37889 / 1435570, loss 255.07893866300583\n",
            "opimizers updated, step 38017 / 1435570, loss 257.15923368930817\n",
            "opimizers updated, step 38145 / 1435570, loss 252.42277878522873\n",
            "opimizers updated, step 38273 / 1435570, loss 261.15460580587387\n",
            "opimizers updated, step 38401 / 1435570, loss 259.93111580610275\n",
            "opimizers updated, step 38529 / 1435570, loss 251.3660900592804\n",
            "opimizers updated, step 38657 / 1435570, loss 261.1885269880295\n",
            "opimizers updated, step 38785 / 1435570, loss 257.3899847269058\n",
            "opimizers updated, step 38913 / 1435570, loss 262.4326641559601\n",
            "opimizers updated, step 39041 / 1435570, loss 256.6249117255211\n",
            "opimizers updated, step 39169 / 1435570, loss 246.67816072702408\n",
            "opimizers updated, step 39297 / 1435570, loss 257.36028200387955\n",
            "opimizers updated, step 39425 / 1435570, loss 255.45390617847443\n",
            "opimizers updated, step 39553 / 1435570, loss 250.645853638649\n",
            "opimizers updated, step 39681 / 1435570, loss 260.5438507795334\n",
            "opimizers updated, step 39809 / 1435570, loss 250.1903828382492\n",
            "opimizers updated, step 39937 / 1435570, loss 257.3433116674423\n",
            "opimizers updated, step 40065 / 1435570, loss 261.1711890101433\n",
            "opimizers updated, step 40193 / 1435570, loss 259.62088894844055\n",
            "opimizers updated, step 40321 / 1435570, loss 253.2185959815979\n",
            "opimizers updated, step 40449 / 1435570, loss 265.67864215373993\n",
            "opimizers updated, step 40577 / 1435570, loss 262.2776808142662\n",
            "opimizers updated, step 40705 / 1435570, loss 248.38312953710556\n",
            "opimizers updated, step 40833 / 1435570, loss 249.2921227812767\n",
            "opimizers updated, step 40961 / 1435570, loss 265.48381930589676\n",
            "opimizers updated, step 41089 / 1435570, loss 248.01734602451324\n",
            "opimizers updated, step 41217 / 1435570, loss 253.30824971199036\n",
            "opimizers updated, step 41345 / 1435570, loss 261.71022045612335\n",
            "opimizers updated, step 41473 / 1435570, loss 256.427139043808\n",
            "opimizers updated, step 41601 / 1435570, loss 250.33752143383026\n",
            "opimizers updated, step 41729 / 1435570, loss 254.3104332089424\n",
            "opimizers updated, step 41857 / 1435570, loss 245.8356778025627\n",
            "opimizers updated, step 41985 / 1435570, loss 256.50966012477875\n",
            "opimizers updated, step 42113 / 1435570, loss 260.60998022556305\n",
            "opimizers updated, step 42241 / 1435570, loss 253.91767489910126\n",
            "opimizers updated, step 42369 / 1435570, loss 262.8438394665718\n",
            "opimizers updated, step 42497 / 1435570, loss 250.4668487906456\n",
            "opimizers updated, step 42625 / 1435570, loss 254.1499000787735\n",
            "opimizers updated, step 42753 / 1435570, loss 256.8416247367859\n",
            "opimizers updated, step 42881 / 1435570, loss 261.45099568367004\n",
            "opimizers updated, step 43009 / 1435570, loss 253.0640464425087\n",
            "opimizers updated, step 43137 / 1435570, loss 255.19126969575882\n",
            "opimizers updated, step 43265 / 1435570, loss 252.82458698749542\n",
            "opimizers updated, step 43393 / 1435570, loss 251.760327398777\n",
            "opimizers updated, step 43521 / 1435570, loss 252.08781003952026\n",
            "opimizers updated, step 43649 / 1435570, loss 255.5711830854416\n",
            "opimizers updated, step 43777 / 1435570, loss 263.1238837838173\n",
            "opimizers updated, step 43905 / 1435570, loss 256.66440296173096\n",
            "opimizers updated, step 44033 / 1435570, loss 259.2925986647606\n",
            "opimizers updated, step 44161 / 1435570, loss 263.1366513967514\n",
            "opimizers updated, step 44289 / 1435570, loss 258.78998202085495\n",
            "opimizers updated, step 44417 / 1435570, loss 258.4169384241104\n",
            "opimizers updated, step 44545 / 1435570, loss 250.95971995592117\n",
            "opimizers updated, step 44673 / 1435570, loss 254.7920137643814\n",
            "opimizers updated, step 44801 / 1435570, loss 258.06247770786285\n",
            "opimizers updated, step 44929 / 1435570, loss 248.7523713707924\n",
            "opimizers updated, step 45057 / 1435570, loss 263.34799259901047\n",
            "opimizers updated, step 45185 / 1435570, loss 265.5625972747803\n",
            "opimizers updated, step 45313 / 1435570, loss 255.64812886714935\n",
            "opimizers updated, step 45441 / 1435570, loss 254.01068991422653\n",
            "opimizers updated, step 45569 / 1435570, loss 242.8234804868698\n",
            "opimizers updated, step 45697 / 1435570, loss 261.9319005012512\n",
            "opimizers updated, step 45825 / 1435570, loss 249.31207764148712\n",
            "opimizers updated, step 45953 / 1435570, loss 252.13799703121185\n",
            "opimizers updated, step 46081 / 1435570, loss 250.96330797672272\n",
            "opimizers updated, step 46209 / 1435570, loss 250.6690069437027\n",
            "opimizers updated, step 46337 / 1435570, loss 253.05471378564835\n",
            "opimizers updated, step 46465 / 1435570, loss 252.98733240365982\n",
            "opimizers updated, step 46593 / 1435570, loss 251.368561565876\n",
            "opimizers updated, step 46721 / 1435570, loss 252.58011734485626\n",
            "opimizers updated, step 46849 / 1435570, loss 246.6991360783577\n",
            "opimizers updated, step 46977 / 1435570, loss 258.2184063196182\n",
            "opimizers updated, step 47105 / 1435570, loss 253.19346570968628\n",
            "opimizers updated, step 47233 / 1435570, loss 251.98540657758713\n",
            "opimizers updated, step 47361 / 1435570, loss 245.00585567951202\n",
            "opimizers updated, step 47489 / 1435570, loss 247.72720247507095\n",
            "opimizers updated, step 47617 / 1435570, loss 256.0835156440735\n",
            "opimizers updated, step 47745 / 1435570, loss 260.85695308446884\n",
            "opimizers updated, step 47873 / 1435570, loss 251.7898080945015\n",
            "opimizers updated, step 48001 / 1435570, loss 253.4450542330742\n",
            "opimizers updated, step 48129 / 1435570, loss 248.7845184803009\n",
            "opimizers updated, step 48257 / 1435570, loss 248.2433686852455\n",
            "opimizers updated, step 48385 / 1435570, loss 253.72906005382538\n",
            "opimizers updated, step 48513 / 1435570, loss 250.14497381448746\n",
            "opimizers updated, step 48641 / 1435570, loss 248.36500388383865\n",
            "opimizers updated, step 48769 / 1435570, loss 258.6566017270088\n",
            "opimizers updated, step 48897 / 1435570, loss 244.77760475873947\n",
            "opimizers updated, step 49025 / 1435570, loss 243.8234776854515\n",
            "opimizers updated, step 49153 / 1435570, loss 258.0282884836197\n",
            "opimizers updated, step 49281 / 1435570, loss 251.2576858997345\n",
            "opimizers updated, step 49409 / 1435570, loss 258.2504963874817\n",
            "opimizers updated, step 49537 / 1435570, loss 259.8857194185257\n",
            "opimizers updated, step 49665 / 1435570, loss 246.51896411180496\n",
            "opimizers updated, step 49793 / 1435570, loss 254.86508536338806\n",
            "opimizers updated, step 49921 / 1435570, loss 252.55429422855377\n",
            "opimizers updated, step 50049 / 1435570, loss 260.0285520553589\n",
            "opimizers updated, step 50177 / 1435570, loss 260.0904445052147\n",
            "opimizers updated, step 50305 / 1435570, loss 245.2421623468399\n",
            "opimizers updated, step 50433 / 1435570, loss 252.97119390964508\n",
            "opimizers updated, step 50561 / 1435570, loss 252.93154138326645\n",
            "opimizers updated, step 50689 / 1435570, loss 263.18287909030914\n",
            "opimizers updated, step 50817 / 1435570, loss 262.7990902662277\n",
            "opimizers updated, step 50945 / 1435570, loss 258.79027116298676\n",
            "opimizers updated, step 51073 / 1435570, loss 245.8358090519905\n",
            "opimizers updated, step 51201 / 1435570, loss 254.22812056541443\n",
            "opimizers updated, step 51329 / 1435570, loss 264.3845644593239\n",
            "opimizers updated, step 51457 / 1435570, loss 247.19852685928345\n",
            "opimizers updated, step 51585 / 1435570, loss 247.39987033605576\n",
            "opimizers updated, step 51713 / 1435570, loss 264.72408854961395\n",
            "opimizers updated, step 51841 / 1435570, loss 257.39593827724457\n",
            "opimizers updated, step 51969 / 1435570, loss 253.52738118171692\n",
            "opimizers updated, step 52097 / 1435570, loss 260.1534643173218\n",
            "opimizers updated, step 52225 / 1435570, loss 254.02074468135834\n",
            "opimizers updated, step 52353 / 1435570, loss 249.88506907224655\n",
            "opimizers updated, step 52481 / 1435570, loss 249.29211163520813\n",
            "opimizers updated, step 52609 / 1435570, loss 266.237716794014\n",
            "opimizers updated, step 52737 / 1435570, loss 251.23530846834183\n",
            "opimizers updated, step 52865 / 1435570, loss 258.12739300727844\n",
            "opimizers updated, step 52993 / 1435570, loss 256.2248129248619\n",
            "opimizers updated, step 53121 / 1435570, loss 252.182215154171\n",
            "opimizers updated, step 53249 / 1435570, loss 254.11969715356827\n",
            "opimizers updated, step 53377 / 1435570, loss 255.4454219341278\n",
            "opimizers updated, step 53505 / 1435570, loss 261.8577898144722\n",
            "opimizers updated, step 53633 / 1435570, loss 257.6599555015564\n",
            "opimizers updated, step 53761 / 1435570, loss 252.59629482030869\n",
            "opimizers updated, step 53889 / 1435570, loss 256.15893936157227\n",
            "opimizers updated, step 54017 / 1435570, loss 252.96879506111145\n",
            "opimizers updated, step 54145 / 1435570, loss 256.25021332502365\n",
            "opimizers updated, step 54273 / 1435570, loss 255.7310101389885\n",
            "opimizers updated, step 54401 / 1435570, loss 258.5171146392822\n",
            "opimizers updated, step 54529 / 1435570, loss 252.35357296466827\n",
            "opimizers updated, step 54657 / 1435570, loss 243.50596964359283\n",
            "opimizers updated, step 54785 / 1435570, loss 252.8598654270172\n",
            "opimizers updated, step 54913 / 1435570, loss 260.0671660900116\n",
            "opimizers updated, step 55041 / 1435570, loss 254.05826711654663\n",
            "opimizers updated, step 55169 / 1435570, loss 250.6057626605034\n",
            "opimizers updated, step 55297 / 1435570, loss 259.1722931265831\n",
            "opimizers updated, step 55425 / 1435570, loss 244.6373987197876\n",
            "opimizers updated, step 55553 / 1435570, loss 245.62317037582397\n",
            "opimizers updated, step 55681 / 1435570, loss 250.47354781627655\n",
            "opimizers updated, step 55809 / 1435570, loss 246.6135379076004\n",
            "opimizers updated, step 55937 / 1435570, loss 254.0536447763443\n",
            "opimizers updated, step 56065 / 1435570, loss 242.73899495601654\n",
            "opimizers updated, step 56193 / 1435570, loss 251.32798290252686\n",
            "opimizers updated, step 56321 / 1435570, loss 254.3754130601883\n",
            "opimizers updated, step 56449 / 1435570, loss 251.57364827394485\n",
            "opimizers updated, step 56577 / 1435570, loss 244.51224690675735\n",
            "opimizers updated, step 56705 / 1435570, loss 253.6628818511963\n",
            "opimizers updated, step 56833 / 1435570, loss 253.37444067001343\n",
            "opimizers updated, step 56961 / 1435570, loss 252.47051864862442\n",
            "opimizers updated, step 57089 / 1435570, loss 257.94826662540436\n",
            "opimizers updated, step 57217 / 1435570, loss 256.91301131248474\n",
            "opimizers updated, step 57345 / 1435570, loss 250.31470572948456\n",
            "opimizers updated, step 57473 / 1435570, loss 267.0030806660652\n",
            "opimizers updated, step 57601 / 1435570, loss 251.66970324516296\n",
            "opimizers updated, step 57729 / 1435570, loss 248.93891197443008\n",
            "opimizers updated, step 57857 / 1435570, loss 253.22646579146385\n",
            "opimizers updated, step 57985 / 1435570, loss 246.34666007757187\n",
            "opimizers updated, step 58113 / 1435570, loss 251.87286031246185\n",
            "opimizers updated, step 58241 / 1435570, loss 254.43009847402573\n",
            "opimizers updated, step 58369 / 1435570, loss 249.62975758314133\n",
            "opimizers updated, step 58497 / 1435570, loss 258.71549862623215\n",
            "opimizers updated, step 58625 / 1435570, loss 240.42824804782867\n",
            "opimizers updated, step 58753 / 1435570, loss 247.08786886930466\n",
            "opimizers updated, step 58881 / 1435570, loss 245.44138151407242\n",
            "opimizers updated, step 59009 / 1435570, loss 244.15169990062714\n",
            "opimizers updated, step 59137 / 1435570, loss 254.8560715317726\n",
            "opimizers updated, step 59265 / 1435570, loss 245.67689126729965\n",
            "opimizers updated, step 59393 / 1435570, loss 244.84929084777832\n",
            "opimizers updated, step 59521 / 1435570, loss 252.93180012702942\n",
            "opimizers updated, step 59649 / 1435570, loss 248.3527317047119\n",
            "opimizers updated, step 59777 / 1435570, loss 245.43441861867905\n",
            "opimizers updated, step 59905 / 1435570, loss 255.59387707710266\n",
            "opimizers updated, step 60033 / 1435570, loss 253.5181811451912\n",
            "opimizers updated, step 60161 / 1435570, loss 248.60385930538177\n",
            "opimizers updated, step 60289 / 1435570, loss 249.69126325845718\n",
            "opimizers updated, step 60417 / 1435570, loss 254.78572219610214\n",
            "opimizers updated, step 60545 / 1435570, loss 245.52997201681137\n",
            "opimizers updated, step 60673 / 1435570, loss 249.75990343093872\n",
            "opimizers updated, step 60801 / 1435570, loss 252.0479701757431\n",
            "opimizers updated, step 60929 / 1435570, loss 249.91090232133865\n",
            "opimizers updated, step 61057 / 1435570, loss 253.39102935791016\n",
            "opimizers updated, step 61185 / 1435570, loss 248.3645870089531\n",
            "opimizers updated, step 61313 / 1435570, loss 250.16648292541504\n",
            "opimizers updated, step 61441 / 1435570, loss 244.18060529232025\n",
            "opimizers updated, step 61569 / 1435570, loss 241.26750993728638\n",
            "opimizers updated, step 61697 / 1435570, loss 254.46100234985352\n",
            "opimizers updated, step 61825 / 1435570, loss 259.7794796824455\n",
            "opimizers updated, step 61953 / 1435570, loss 240.09619319438934\n",
            "opimizers updated, step 62081 / 1435570, loss 245.06213915348053\n",
            "opimizers updated, step 62209 / 1435570, loss 250.26550233364105\n",
            "opimizers updated, step 62337 / 1435570, loss 244.0626438856125\n",
            "opimizers updated, step 62465 / 1435570, loss 261.82344526052475\n",
            "opimizers updated, step 62593 / 1435570, loss 243.35847091674805\n",
            "opimizers updated, step 62721 / 1435570, loss 253.1798220872879\n",
            "opimizers updated, step 62849 / 1435570, loss 257.8310688138008\n",
            "opimizers updated, step 62977 / 1435570, loss 251.3334811925888\n",
            "opimizers updated, step 63105 / 1435570, loss 249.9425396323204\n",
            "opimizers updated, step 63233 / 1435570, loss 249.084088742733\n",
            "opimizers updated, step 63361 / 1435570, loss 252.19929778575897\n",
            "opimizers updated, step 63489 / 1435570, loss 256.68096059560776\n",
            "opimizers updated, step 63617 / 1435570, loss 252.64743584394455\n",
            "opimizers updated, step 63745 / 1435570, loss 248.63439065217972\n",
            "opimizers updated, step 63873 / 1435570, loss 254.67988973855972\n",
            "opimizers updated, step 64001 / 1435570, loss 252.9620418548584\n",
            "opimizers updated, step 64129 / 1435570, loss 248.43290543556213\n",
            "opimizers updated, step 64257 / 1435570, loss 258.377785384655\n",
            "opimizers updated, step 64385 / 1435570, loss 247.74099388718605\n",
            "opimizers updated, step 64513 / 1435570, loss 248.76825326681137\n",
            "opimizers updated, step 64641 / 1435570, loss 251.8603568971157\n",
            "opimizers updated, step 64769 / 1435570, loss 243.10101762413979\n",
            "opimizers updated, step 64897 / 1435570, loss 238.63006526231766\n",
            "opimizers updated, step 65025 / 1435570, loss 247.0194782614708\n",
            "opimizers updated, step 65153 / 1435570, loss 250.58122205734253\n",
            "opimizers updated, step 65281 / 1435570, loss 244.28182643651962\n",
            "opimizers updated, step 65409 / 1435570, loss 257.35401207208633\n",
            "opimizers updated, step 65537 / 1435570, loss 247.05082374811172\n",
            "opimizers updated, step 65665 / 1435570, loss 242.86418640613556\n",
            "opimizers updated, step 65793 / 1435570, loss 250.2489576935768\n",
            "opimizers updated, step 65921 / 1435570, loss 244.7521654367447\n",
            "opimizers updated, step 66049 / 1435570, loss 258.05976873636246\n",
            "opimizers updated, step 66177 / 1435570, loss 250.6017256975174\n",
            "opimizers updated, step 66305 / 1435570, loss 249.2214344739914\n",
            "opimizers updated, step 66433 / 1435570, loss 245.9599226117134\n",
            "opimizers updated, step 66561 / 1435570, loss 246.55443066358566\n",
            "opimizers updated, step 66689 / 1435570, loss 250.6606389284134\n",
            "opimizers updated, step 66817 / 1435570, loss 249.93223410844803\n",
            "opimizers updated, step 66945 / 1435570, loss 248.02974086999893\n",
            "opimizers updated, step 67073 / 1435570, loss 253.8069036602974\n",
            "opimizers updated, step 67201 / 1435570, loss 251.84231519699097\n",
            "opimizers updated, step 67329 / 1435570, loss 250.63936311006546\n",
            "opimizers updated, step 67457 / 1435570, loss 251.91868752241135\n",
            "opimizers updated, step 67585 / 1435570, loss 258.8863590955734\n",
            "opimizers updated, step 67713 / 1435570, loss 253.40384656190872\n",
            "opimizers updated, step 67841 / 1435570, loss 245.48666369915009\n",
            "opimizers updated, step 67969 / 1435570, loss 247.3494564294815\n",
            "opimizers updated, step 68097 / 1435570, loss 250.2826428413391\n",
            "opimizers updated, step 68225 / 1435570, loss 241.3713060617447\n",
            "opimizers updated, step 68353 / 1435570, loss 256.15314650535583\n",
            "opimizers updated, step 68481 / 1435570, loss 247.93210643529892\n",
            "opimizers updated, step 68609 / 1435570, loss 244.82380264997482\n",
            "opimizers updated, step 68737 / 1435570, loss 242.25718915462494\n",
            "opimizers updated, step 68865 / 1435570, loss 246.3178921341896\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "\n",
        "completed_steps = 0\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "    train_loss_sum = 0\n",
        "    eval_loss_sum = 0\n",
        "    loss_buf = 0\n",
        "\n",
        "    print('model training')\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        completed_steps += 1\n",
        "\n",
        "        batch = batch.to(device)\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        \n",
        "        loss_buf += loss.item()\n",
        "        train_losses.append(loss.item())\n",
        "        loss.backward()\n",
        "\n",
        "        if step % optimizer_steps == 0 or step == len(train_dataset) - 1:\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            print(f'opimizers updated, step {completed_steps} / {epoch_num * len(train_dataloader)}, loss {loss_buf}')\n",
        "            loss_buf = 0\n",
        "\n",
        "    print('model evaluating')\n",
        "    model.eval()\n",
        "\n",
        "    gen_kwargs = {\n",
        "        'max_length': 256,\n",
        "        'num_beams': 3\n",
        "    }\n",
        "\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        batch = batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            \n",
        "            eval_loss_sum += loss.item()\n",
        "\n",
        "            generated_tokens = model.generate(\n",
        "                batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                **gen_kwargs,\n",
        "            )\n",
        "\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            labels = labels.cpu().numpy()\n",
        "            generated_tokens = generated_tokens.cpu().numpy()\n",
        "\n",
        "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "            metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
        "    \n",
        "    eval_result = metric.compute(use_stemmer=True)\n",
        "    metrics.append(eval_result)\n",
        "    result = {key: round(value.mid.fmeasure * 100, 4) for key, value in eval_result.items()}\n",
        "\n",
        "    for step, batch in enumerate(subset_dataloader):\n",
        "        batch = batch.to(device)\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        train_loss_sum += loss.item()\n",
        "\n",
        "    train_losses_epoch.append(train_loss_sum)\n",
        "    eval_losses_epoch.append(eval_loss_sum)\n",
        "\n",
        "    print(f'evaluating metrics: {result}')\n",
        "    print(f'epoch {epoch + 1} / {epoch_num} completed, train_loss: {train_loss_sum}, eval loss: {eval_loss_sum}')\n",
        "\n",
        "print('training completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzGKE1f6jJo8"
      },
      "outputs": [],
      "source": [
        "# Saving model to disk for later reuse\n",
        "\n",
        "torch.save(model, 'content/model/model.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjZWh_a8jJo9"
      },
      "outputs": [],
      "source": [
        "# Loading model from disk for reuse\n",
        "\n",
        "model = torch.load('content/model/model.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6ZTl2lBjJo9",
        "outputId": "ded9317e-46fb-4d7f-b27a-1fc44c9329e4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1224x432 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Training and evaluation losses by epoch\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(17,6))\n",
        "\n",
        "ax[0].plot(train_losses_epoch)\n",
        "ax[0].set_title('train losses')\n",
        "ax[1].plot(eval_losses_epoch)\n",
        "ax[1].set_title('eval losses');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4aKILvBjJo-",
        "outputId": "510b6e7c-f0c1-4b66-b918-60deb6f47b1d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Training loss graphic\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(train_losses)\n",
        "plt.legend(['training'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQlgoyX6jJo-",
        "outputId": "afb32fb5-91e9-4246-c685-601509dcb1fe"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x1080 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Metric graphics\n",
        "\n",
        "rouge1 = []\n",
        "rouge2 = []\n",
        "rougeL = []\n",
        "rougeLsum = []\n",
        "\n",
        "for metric_i in metrics:\n",
        "    metric = {key: value.mid.fmeasure for key, value in metric_i.items()}\n",
        "    rouge1.append(metric['rouge1'])\n",
        "    rouge2.append(metric['rouge2'])\n",
        "    rougeL.append(metric['rougeL'])\n",
        "    rougeLsum.append(metric['rougeLsum'])\n",
        "\n",
        "fig, ax = plt.subplots(2, 2, figsize=(15,15))\n",
        "\n",
        "ax[0][0].plot(rouge1)\n",
        "ax[0][0].set_title('rouge1')\n",
        "\n",
        "ax[0][1].plot(rouge2)\n",
        "ax[0][1].set_title('rouge2')\n",
        "\n",
        "ax[1][0].plot(rougeL)\n",
        "ax[1][0].set_title('rougeL')\n",
        "\n",
        "ax[1][1].plot(rougeLsum)\n",
        "ax[1][1].set_title('rougeLsum');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvw-KHExjJo_"
      },
      "outputs": [],
      "source": [
        "some_summaries = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MYjILNvjJo_",
        "outputId": "ecaeda6c-188d-41f1-a2a0-ed9f51d39b8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "testing model\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'add_batch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-fee3390dca98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msome_summaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoded_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoded_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'step {step + 1} / {len(test_dataloader)} completed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'add_batch'"
          ]
        }
      ],
      "source": [
        "print('testing model')\n",
        "\n",
        "gen_kwargs = {\n",
        "    'max_length': 256,\n",
        "    'num_beams': 3\n",
        "}\n",
        "\n",
        "for step, batch in enumerate(test_dataloader):\n",
        "    batch = batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        generated_tokens = model.generate(\n",
        "            batch['input_ids'],\n",
        "            attention_mask = batch['attention_mask'],\n",
        "            **gen_kwargs                         \n",
        "        )\n",
        "\n",
        "        labels = batch['labels']\n",
        "\n",
        "        labels = labels.cpu().numpy()\n",
        "        generated_tokens = generated_tokens.cpu().numpy()\n",
        "\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "        if step == 0 or step == 50 or step == 60 or step == 74:\n",
        "            some_summaries.append((decoded_preds, decoded_labels))\n",
        "            \n",
        "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "        print(f'step {step + 1} / {len(test_dataloader)} completed')\n",
        "\n",
        "test_result = metric.compute(use_stemmer=True)\n",
        "result = {key: round(value.mid.fmeasure * 100, 4) for key, value in test_result.items()}\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrD196IKjJo_",
        "outputId": "78f454cf-4c6d-42c6-d6e1-d979c9a30120"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'rouge1': 42.4506, 'rouge2': 19.9627, 'rougeL': 29.8358, 'rougeLsum': 39.7059}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr-XNwhHjJpA",
        "outputId": "ee4f644b-799c-4ab7-c05d-e64b50f9162c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pred\n",
            "James Best was best known for his role on \"The Dukes of Hazzard\"\n",
            "He died in hospice in Hickory, North Carolina, of complications from pneumonia.\n",
            "label\n",
            "James Best, who played the sheriff on \"The Dukes of Hazzard,\" died Monday at 88.\n",
            "\"Hazzard\" ran from 1979 to 1985 and was among the most popular shows on TV.\n",
            "\n",
            "pred\n",
            "Dr. Anthony Moschetto, 54, pleads not guilty to all charges against him.\n",
            "He is charged in what authorities say was a failed scheme to have another doctor hurt or killed.\n",
            "Two other men are accused of being accomplices in the plot.\n",
            "label\n",
            "A lawyer for Dr. Anthony Moschetto says the charges against him are baseless.\n",
            "Moschetto, 54, was arrested for selling drugs and weapons, prosecutors say.\n",
            "Authorities allege Moschetto hired accomplices to burn down the practice of former associate.\n",
            "\n",
            "pred\n",
            "James Best was best known for his portrayal of bumbling sheriff Rosco P. Coltrane on \"The Dukes of Hazzard\"\n",
            "Best died Monday after a brief illness.\n",
            "label\n",
            "James Best, who played the sheriff on \"The Dukes of Hazzard,\" died Monday at 88.\n",
            "\"Hazzard\" ran from 1979 to 1985 and was among the most popular shows on TV.\n",
            "\n",
            "pred\n",
            "Dr. Anthony Moschetto, 54, pleads not guilty to all charges.\n",
            "He is accused of conspiracy, conspiracy, burglary, arson, criminal prescription sale and weapons.\n",
            "label\n",
            "A lawyer for Dr. Anthony Moschetto says the charges against him are baseless.\n",
            "Moschetto, 54, was arrested for selling drugs and weapons, prosecutors say.\n",
            "Authorities allege Moschetto hired accomplices to burn down the practice of former associate.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for preds, labels in some_summaries:\n",
        "    for pred, label in zip(preds, labels):\n",
        "        print('pred')\n",
        "        print(pred)\n",
        "        print('label')\n",
        "        print(label)\n",
        "        print()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhldxBbwjJpA",
        "outputId": "b975d339-ac9c-4015-8745-f41fd042861e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(['James Best was best known for his role on \"The Dukes of Hazzard\"\\nHe died in hospice in Hickory, North Carolina, of complications from pneumonia.',\n",
              "   'Dr. Anthony Moschetto, 54, pleads not guilty to all charges against him.\\nHe is charged in what authorities say was a failed scheme to have another doctor hurt or killed.\\nTwo other men are accused of being accomplices in the plot.'],\n",
              "  ['James Best, who played the sheriff on \"The Dukes of Hazzard,\" died Monday at 88.\\n\"Hazzard\" ran from 1979 to 1985 and was among the most popular shows on TV.',\n",
              "   'A lawyer for Dr. Anthony Moschetto says the charges against him are baseless.\\nMoschetto, 54, was arrested for selling drugs and weapons, prosecutors say.\\nAuthorities allege Moschetto hired accomplices to burn down the practice of former associate.'])]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "some_summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4HtIcvkjJpA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "VVs871VXSI9n",
        "0loGPq7kxXdT",
        "CDu-35-JjJob",
        "twPFFmrBjJoo",
        "TUTm4YUBjJoy",
        "xCxS_ntejJoy",
        "O6SIhad3jJo2",
        "8T-Dtcs2jJo7"
      ],
      "machine_shape": "hm",
      "name": "bart_pretrain.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}